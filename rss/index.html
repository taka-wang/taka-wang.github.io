<?xml version="1.0" encoding="UTF-8"?><rss xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" version="2.0" xmlns:media="http://search.yahoo.com/mrss/"><channel><title><![CDATA[Liberation Notes]]></title><description><![CDATA[This is me thinking about life and software, discovering old lessons and learning new stuff.]]></description><link>https://blog.cmwang.net/</link><image><url>https://blog.cmwang.net/favicon.png</url><title>Liberation Notes</title><link>https://blog.cmwang.net/</link></image><generator>Ghost 5.78</generator><lastBuildDate>Wed, 31 Jan 2024 18:06:07 GMT</lastBuildDate><atom:link href="https://blog.cmwang.net/rss/" rel="self" type="application/rss+xml"/><ttl>60</ttl><item><title><![CDATA[迷途 (Astray) - AI (Lee Ji Eun)]]></title><description><![CDATA[Disclaimer: Songs generated by this AI are for entertainment purposes only and strictly prohibited for commercial use. Not liable for legal consequences.]]></description><link>https://blog.cmwang.net/ai-iu-astray/</link><guid isPermaLink="false">65ba5e4dd5373f88eb1a0a5d</guid><category><![CDATA[AI]]></category><category><![CDATA[IU]]></category><dc:creator><![CDATA[Jamie Wang]]></dc:creator><pubDate>Tue, 14 Nov 2023 07:51:00 GMT</pubDate><media:content url="https://blog.cmwang.net/content/images/2024/01/1616593-1.jpg" medium="image"/><content:encoded><![CDATA[<figure class="kg-card kg-embed-card"><iframe width="200" height="113" src="https://www.youtube.com/embed/_1oCs-Tx5Jc?feature=oembed" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen title="&#x8FF7;&#x9014; - AI IU"></iframe></figure><img src="https://blog.cmwang.net/content/images/2024/01/1616593-1.jpg" alt="&#x8FF7;&#x9014; (Astray) - AI (Lee Ji Eun)"><p>&#x8A5E;&#xFF1A;&#x6797;&#x963F;&#x8207; &#x66F2;&#xFF1A;&#x912D;&#x51B0;&#x51B0;</p>
<p>&#x9ED1;&#x591C;&#x64A5;&#x52D5;&#x6642;&#x91DD;<br>
&#x55A7;&#x56C2;&#x9010;&#x6F38;&#x4EE3;&#x66FF;&#x7121;&#x8072;<br>
&#x6211;&#x5011;&#x6F14;&#x8457;&#x76F8;&#x9047;&#x5287;&#x672C;<br>
&#x9472;&#x5D4C;&#x591C;&#x7684;&#x5F69;&#x8679;<br>
&#x9084;&#x6709;&#x7A7F;&#x904E;&#x6C99;&#x6F20;&#x7684;&#x98A8;<br>
&#x90FD;&#x66FE;&#x7D93;&#x662F;&#x6211;&#x5011;&#x7684;&#x898B;&#x8B49;</p>
<p>&#x8AB0;&#x4E0D;&#x662F;&#x832B;&#x832B;&#x4E2D;&#x8FF7;&#x9014;&#x7684;&#x4EBA;<br>
&#x627E;&#x5C0B; &#x5104;&#x842C;&#x5206;&#x4E4B;&#x4E00;&#x7684;&#x53EF;&#x80FD;<br>
&#x56F0;&#x5728;&#x9019; &#x540D;&#x70BA;&#x547D;&#x904B;&#x7684;&#x57CE;<br>
&#x537B;&#x4F9D;&#x820A;&#x8FFD;&#x9010;&#x8457;&#x661F;&#x8FB0;<br>
&#x5225;&#x64D4;&#x5FC3; &#x6211;&#x611B;&#x8457;&#x7684;&#x8FF7;&#x9014;&#x7684;&#x4EBA;<br>
&#x57F7;&#x676F;&#x9152; &#x6EAB;&#x6696;&#x5B64;&#x7368;&#x9748;&#x9B42;<br>
&#x8B93;&#x6211;&#x64C1;&#x62B1; &#x4F60;&#x7684;&#x88C2;&#x75D5;<br>
&#x4F60;&#x5982;&#x661F;&#x71E6;&#x721B; &#x9583;&#x720D;&#x5728;&#x843D;&#x5BDE;&#x6642;&#x5206;</p>
<p>&#x9ED1;&#x591C;&#x64A5;&#x52D5;&#x6642;&#x91DD;<br>
&#x55A7;&#x56C2;&#x9010;&#x6F38;&#x4EE3;&#x66FF;&#x7121;&#x8072;<br>
&#x6211;&#x5011;&#x6F14;&#x8457;&#x76F8;&#x9047;&#x5287;&#x672C;<br>
&#x9472;&#x5D4C;&#x591C;&#x7684;&#x5F69;&#x8679;<br>
&#x9084;&#x6709;&#x7A7F;&#x904E;&#x6C99;&#x6F20;&#x7684;&#x98A8;<br>
&#x90FD;&#x66FE;&#x7D93;&#x662F;&#x6211;&#x5011;&#x7684;&#x898B;&#x8B49;</p>
<p>&#x8AB0;&#x4E0D;&#x662F;&#x832B;&#x832B;&#x4E2D;&#x8FF7;&#x9014;&#x7684;&#x4EBA;<br>
&#x627E;&#x5C0B; &#x5104;&#x842C;&#x5206;&#x4E4B;&#x4E00;&#x7684;&#x53EF;&#x80FD;<br>
&#x56F0;&#x5728;&#x9019; &#x540D;&#x70BA;&#x547D;&#x904B;&#x7684;&#x57CE;<br>
&#x537B;&#x4F9D;&#x820A;&#x8FFD;&#x9010;&#x8457;&#x661F;&#x8FB0;<br>
&#x5225;&#x64D4;&#x5FC3; &#x6211;&#x611B;&#x8457;&#x7684;&#x8FF7;&#x9014;&#x7684;&#x4EBA;<br>
&#x57F7;&#x676F;&#x9152; &#x6EAB;&#x6696;&#x5B64;&#x7368;&#x9748;&#x9B42;<br>
&#x8B93;&#x6211;&#x64C1;&#x62B1; &#x4F60;&#x7684;&#x88C2;&#x75D5;<br>
&#x4F60;&#x5982;&#x661F;&#x71E6;&#x721B; &#x9583;&#x720D;&#x5728;&#x843D;&#x5BDE;&#x6642;&#x5206;</p>
<p>&#x8AB0;&#x4E0D;&#x662F;&#x832B;&#x832B;&#x4E2D;&#x8FF7;&#x9014;&#x7684;&#x4EBA;<br>
&#x627E;&#x5C0B; &#x5104;&#x842C;&#x5206;&#x4E4B;&#x4E00;&#x7684;&#x53EF;&#x80FD;<br>
&#x56F0;&#x5728;&#x9019; &#x540D;&#x70BA;&#x547D;&#x904B;&#x7684;&#x57CE;<br>
&#x537B;&#x4F9D;&#x820A;&#x8FFD;&#x9010;&#x8457;&#x661F;&#x8FB0;<br>
&#x5225;&#x64D4;&#x5FC3; &#x6211;&#x611B;&#x8457;&#x7684;&#x8FF7;&#x9014;&#x7684;&#x4EBA;<br>
&#x57F7;&#x676F;&#x9152; &#x6EAB;&#x6696;&#x5B64;&#x7368;&#x9748;&#x9B42;<br>
&#x8B93;&#x6211;&#x64C1;&#x62B1; &#x4F60;&#x7684;&#x88C2;&#x75D5;<br>
&#x4F60;&#x5982;&#x661F;&#x71E6;&#x721B; &#x9583;&#x720D;&#x5728;&#x843D;&#x5BDE;&#x6642;&#x5206;</p>
]]></content:encoded></item><item><title><![CDATA[重啟人生 | BRUSH UP LIFE]]></title><description><![CDATA[《重啟人生》（ブラッシュアップライフ）是今年度最熱門的日劇之一，它深深觸動了許多人的心靈。]]></description><link>https://blog.cmwang.net/brush-up-life/</link><guid isPermaLink="false">65ba7277b102c4db988584e2</guid><category><![CDATA[ブラッシュアップライフ]]></category><category><![CDATA[日劇]]></category><category><![CDATA[Japanese Drama]]></category><category><![CDATA[Emotions]]></category><dc:creator><![CDATA[Taka Wang]]></dc:creator><pubDate>Sun, 23 Apr 2023 10:36:21 GMT</pubDate><media:content url="https://blog.cmwang.net/content/images/max/800/1-qmnohe88z8o1ibeoayglua.png" medium="image"/><content:encoded><![CDATA[<img src="https://blog.cmwang.net/content/images/max/800/1-qmnohe88z8o1ibeoayglua.png" alt="&#x91CD;&#x555F;&#x4EBA;&#x751F; | BRUSH UP LIFE"><p>&#x300A;&#x91CD;&#x555F;&#x4EBA;&#x751F;&#x300B;&#xFF08;&#x30D6;&#x30E9;&#x30C3;&#x30B7;&#x30E5;&#x30A2;&#x30C3;&#x30D7;&#x30E9;&#x30A4;&#x30D5;&#xFF09;&#x662F;&#x4ECA;&#x5E74;&#x5EA6;&#x6700;&#x71B1;&#x9580;&#x7684;&#x65E5;&#x5287;&#x4E4B;&#x4E00;&#xFF0C;&#x5B83;&#x6DF1;&#x6DF1;&#x89F8;&#x52D5;&#x4E86;&#x8A31;&#x591A;&#x4EBA;&#x7684;&#x5FC3;&#x9748;&#x3002;&#x5287;&#x4E2D;&#x878D;&#x5408;&#x4E86;&#x8A31;&#x591A;&#x5E73;&#x6210;&#x6642;&#x4EE3;&#x7684;&#x7D93;&#x5178;&#x5143;&#x7D20;&#xFF0C;&#x9084;&#x6709;&#x4E0D;&#x5C11;&#x6642;&#x4EE3;&#x7684;&#x773C;&#x6DDA;&#xFF0C;&#x5982;BB Call&#x3001;&#x305F;&#x307E;&#x3054;&#x3063;&#x3061;&#x3001;Game Boy Advance&#x7B49;&#x7B49;&#xFF0C;&#x52FE;&#x8D77;&#x4E86;&#x4EBA;&#x5011;&#x5C0D;&#x65BC;&#x5F80;&#x65E5;&#x6642;&#x5149;&#x7684;&#x7F8E;&#x597D;&#x56DE;&#x61B6;&#x3002;&#x6B64;&#x5916;&#xFF0C;&#x9019;&#x90E8;&#x5287;&#x7684;&#x97F3;&#x6A02;&#x4E5F;&#x5341;&#x5206;&#x8B1B;&#x7A76;&#xFF0C;&#x5176;&#x4E2D;&#x7684;&#x61F7;&#x820A;&#x65E5;&#x6587;&#x6B4C;&#x66F2;&#x66F4;&#x662F;&#x8B93;&#x4EBA;&#x56DE;&#x60F3;&#x8D77;&#x90A3;&#x4E9B;&#x5E74;&#x8F15;&#x6642;&#x7684;&#x7A2E;&#x7A2E;&#x3002;&#x6545;&#x4E8B;&#x60C5;&#x7BC0;&#x63CF;&#x5BEB;&#x4E86;&#x4E00;&#x751F;&#x646F;&#x53CB;&#x4E4B;&#x9593;&#x7684;&#x771F;&#x646F;&#x60C5;&#x8ABC;&#x8207;&#x65E5;&#x5E38;&#x9593;&#x7684;&#x9592;&#x804A;&#xFF0C;&#x6545;&#x4E8B;&#x76F8;&#x7576;&#x4EE4;&#x4EBA;&#x611F;&#x52D5;&#x3002;</p><!--kg-card-begin: html--><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://www.kkbox.com/hk/tc/playlist/LZFv22FCYzN14uoKXl?ref=localhost"><div class="kg-bookmark-content"><div class="kg-bookmark-title">&#x91CD;&#x555F;&#x4EBA;&#x751F;_&#x6B4C;&#x55AE;</div><div class="kg-bookmark-description"></div><div class="kg-bookmark-metadata"></div></div><div class="kg-bookmark-thumbnail"><img src="https://blog.cmwang.net/content/images/fit/c/160/160/0-op9c4ycwtorfx7pm.jpg" alt="&#x91CD;&#x555F;&#x4EBA;&#x751F; | BRUSH UP LIFE"></div></a></figure><!--kg-card-end: html--><p>&#x9019;&#x90E8;&#x5287;&#x7684;&#x7DE8;&#x5287; &#x30D0;&#x30AB;&#x30EA;&#x30BA;&#x30E0; &#x904E;&#x5F80;&#x66FE;&#x7D93;&#x5275;&#x4F5C;&#x51FA;&#x985E;&#x4F3C;&#x4E3B;&#x984C;&#x7684;&#x4F5C;&#x54C1;&#xFF0C;&#x4F8B;&#x5982;&#x300A;&#x7D20;&#x6575;&#x306A;&#x9078;TAXI&#x300B;&#xFF0C;&#x500B;&#x4EBA;&#x4E5F;&#x662F;&#x5F88;&#x63A8;&#x85A6;&#x9019;&#x500B;&#x8F15;&#x9B06;&#x5C0F;&#x54C1;&#x3002;&#x6211;&#x5728;&#x89C0;&#x5F71;&#x7684;&#x904E;&#x7A0B;&#x4E2D;&#xFF0C;&#x4E5F;&#x4E0D;&#x514D;&#x806F;&#x60F3;&#x5230;&#x53E6;&#x5916;&#x5169;&#x90E8;&#x76F8;&#x95DC;&#x984C;&#x6750;&#x7684;&#x4F5C;&#x54C1;&#xFF0C;&#x5206;&#x5225;&#x662F;&#x5E74;&#x4EE3;&#x6709;&#x9EDE;&#x4E45;&#x9060;&#x7684;&#x65E5;&#x5287;&#x300A;Loss:Time:Life&#x300B;&#xFF08;&#x30ED;&#x30B9;&#xFF1A;&#x30BF;&#x30A4;&#x30E0;&#xFF1A;&#x30E9;&#x30A4;&#x30D5;&#xFF09; &#x548C;&#x8FD1;&#x671F;&#x7684;&#x97D3;&#x5287;&#x300A;&#x8A8D;&#x8B58;&#x7684;&#x59BB;&#x5B50;&#x300B;&#xFF08;&#xC544;&#xB294;&#xC640;&#xC774;&#xD504;&#xFF09;&#xFF0C;&#x90FD;&#x662F;&#x767C;&#x4EBA;&#x7701;&#x601D;&#x4E5F;&#x5F88;&#x6709;&#x610F;&#x601D;&#x7684;&#x5F71;&#x96C6;&#x3002;&#x9019;&#x4E9B;&#x7A7F;&#x8D8A;&#x6216;&#x8005;&#x5E73;&#x884C;&#x5B87;&#x5B99;&#x7684;&#x5F71;&#x96C6;&#x96D6;&#x7136;&#x8457;&#x91CD;&#x7684;&#x5167;&#x5BB9;&#x4E0D;&#x540C;&#xFF0C;&#x4F46;&#x300A;&#x91CD;&#x555F;&#x4EBA;&#x751F;&#x300B;&#x537B;&#x4E0D;&#x540C;&#x65BC;&#x5176;&#x4ED6;&#x985E;&#x4F3C;&#x4F5C;&#x54C1;&#xFF0C;&#x5B83;&#x4E26;&#x4E0D;&#x8457;&#x91CD;&#x65BC;&#x4EBA;&#x751F;&#x7684;&#x61CA;&#x6094;&#x8207;&#x88DC;&#x6551;&#xFF0C;&#x53CD;&#x800C;&#x50CF;&#x662F;&#x6389;&#x5165;&#x4E0D;&#x540C;&#x5E73;&#x884C;&#x5B87;&#x5B99;&#x7684;&#x6642;&#x7A7A;&#x65C5;&#x8005;&#xFF0C;&#x91CD;&#x65B0;&#x518D;&#x904E;&#x4E00;&#x6B21;&#x4E0D;&#x540C;&#x7684;&#x4EBA;&#x751F;&#x3002;&#x96D6;&#x7136;&#x5728;&#x4E16;&#x754C;&#x89C0;&#x7684;&#x8A2D;&#x5B9A;&#x4E0A;&#x53EF;&#x80FD;&#x6703;&#x8B93;&#x4EBA;&#x6709;&#x9EDE;&#x56F0;&#x60D1;&#xFF0C;&#x611F;&#x89BA;&#x597D;&#x50CF;&#x6709;&#x4E00;&#x4E9B; Bug&#xFF0C;&#x4F46;&#x8ACB;&#x5E36;&#x8457;&#x6109;&#x6085;&#x7684;&#x5FC3;&#x60C5;&#xFF0C;&#x76E1;&#x60C5;&#x4EAB;&#x53D7;&#x9019;&#x90E8;&#x6EFF;&#x6EFF;&#x61F7;&#x820A;&#x98A8;&#x65E5;&#x5287;&#x60F3;&#x50B3;&#x9054;&#x7684;&#x6545;&#x4E8B;&#x5373;&#x53EF;&#x3002;</p><!--kg-card-begin: html--><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://www.amazon.co.jp/%E3%83%AD%E3%82%B9-%E3%82%BF%E3%82%A4%E3%83%A0-%E3%83%A9%E3%82%A4%E3%83%95/dp/B0B8P9K1TS?ref=localhost"><div class="kg-bookmark-content"><div class="kg-bookmark-title">&#x30ED;&#x30B9;:&#x30BF;&#x30A4;&#x30E0;:&#x30E9;&#x30A4;&#x30D5;</div><div class="kg-bookmark-description"></div><div class="kg-bookmark-metadata"></div></div><div class="kg-bookmark-thumbnail"><img src="https://blog.cmwang.net/content/images/fit/c/160/160/0-pne8z2_fdgsa0yx1.jpg" alt="&#x91CD;&#x555F;&#x4EBA;&#x751F; | BRUSH UP LIFE"></div></a></figure><!--kg-card-end: html--><!--kg-card-begin: html--><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://www.viki.com/tv/35862c-familiar-wife?ref=localhost"><div class="kg-bookmark-content"><div class="kg-bookmark-title">Familiar Wife | Korea | Drama | Watch with English Subtitles &amp; More &#x2714;&#xFE0F;</div><div class="kg-bookmark-description">One unexpected choice can change everything about your life. Cha Joo Hyuk (Ji Sung) works at a bank and has been&#x2026;</div><div class="kg-bookmark-metadata"></div></div><div class="kg-bookmark-thumbnail"><img src="https://blog.cmwang.net/content/images/fit/c/160/160/0-ybzomdgs0pke-u_8.jpg" alt="&#x91CD;&#x555F;&#x4EBA;&#x751F; | BRUSH UP LIFE"></div></a></figure><!--kg-card-end: html--><p>&#x7576;&#x5F71;&#x7247;&#x4E2D;&#x97FF;&#x8D77; ZARD &#x7684;&#x300A;&#x8CA0;&#x3051;&#x306A;&#x3044;&#x3067;&#x300B;&#x6642;&#xFF0C;&#x6211;&#x7684;&#x5167;&#x5FC3;&#x4E0D;&#x7981;&#x611F;&#x5230;&#x6FC0;&#x52D5;&#x548C;&#x611F;&#x50B7;&#x3002;&#x5742;&#x4E95;&#x6CC9;&#x6C34;&#x7684;&#x97F3;&#x6A02;&#x66FE;&#x7D93;&#x966A;&#x4F34;&#x6211;&#x5EA6;&#x904E;&#x9752;&#x6F80;&#x5B64;&#x55AE;&#x7684;&#x5B78;&#x751F;&#x6642;&#x5149;&#xFF0C;&#x61F7;&#x820A;&#x97F3;&#x6A02;&#x771F;&#x7684;&#x70BA;&#x672C;&#x5287;&#x52A0;&#x5206;&#x4E0D;&#x5C11;&#xFF0C;&#x6BCF;&#x6BCF;&#x90FD;&#x5C07;&#x4EBA;&#x5E36;&#x56DE;&#x90A3;&#x500B;&#x55AE;&#x7D14;&#x7684;&#x5E74;&#x4EE3;&#x3002;</p><p>&#x4E3B;&#x4EBA;&#x516C;&#x5728;&#x5287;&#x4E2D;&#x6BCF;&#x4E00;&#x6B21;&#x7684;&#x91CD;&#x751F;&#x90FD;&#x662F;&#x70BA;&#x4E86;&#x80FD;&#x5920;&#x91CD;&#x65B0;&#x6295;&#x80CE;&#x6210;&#x70BA;&#x4EBA;&#x985E;&#xFF0C;&#x56E0;&#x6B64;&#xFF0C;&#x4ED6;&#x91CD;&#x65B0;&#x52AA;&#x529B;&#x7A4D;&#x798F;&#x505A;&#x597D;&#x4E8B;&#xFF0C;&#x5F9E;&#x4E00;&#x958B;&#x59CB;&#x7684;&#x5C0F;&#x611B;&#xFF08;&#x70BA;&#x81EA;&#x5DF1;&#xFF09;&#x8F49;&#x8B8A;&#x6210;&#x5927;&#x611B;&#xFF08;&#x70BA;&#x5225;&#x4EBA;&#xFF09;&#xFF0C;&#x9019;&#x7A2E;&#x4E0D;&#x65B7;&#x91CD;&#x751F;&#x4E26;&#x81EA;&#x6211;&#x512A;&#x5316;&#x7684;&#x904E;&#x7A0B;&#xFF0C;&#x96D6;&#x7136;&#x6709;&#x9EDE;&#x8352;&#x8A95;&#x641E;&#x7B11;&#xFF0C;&#x537B;&#x662F;&#x7B11;&#x4E2D;&#x5E36;&#x6DDA;&#x7684;&#x89F8;&#x52D5;&#x4EBA;&#x5FC3;&#x3002;&#x9019;&#x4E5F;&#x8B93;&#x6211;&#x5011;&#x53CD;&#x601D;&#xFF0C;&#x70BA;&#x4EC0;&#x9EBC;&#x91CD;&#x65B0;&#x6295;&#x80CE;&#x6210;&#x70BA;&#x4EBA;&#x985E;&#x5C31;&#x662F;&#x6700;&#x597D;&#x7684;&#x9078;&#x64C7;&#x5462;&#xFF1F;&#x90A3;&#x662F;&#x4E0D;&#x662F;&#x56E0;&#x70BA;&#x6211;&#x5011;&#x81EA;&#x5DF1;&#x5E36;&#x8457;&#x4EBA;&#x985E;&#x4E2D;&#x5FC3;&#x4E3B;&#x7FA9;&#x7684;&#x60F3;&#x6CD5;&#xFF0C;&#x4EA6;&#x6216;&#x8005;&#x662F;&#x56E0;&#x70BA;&#x4EBA;&#x985E;&#x672C;&#x8EAB;&#x7684;&#x512A;&#x8D8A;&#x611F;&#x4F7F;&#x7136;&#xFF1F;&#x9019;&#x4E5F;&#x5F71;&#x97FF;&#x8457;&#x6211;&#x5011;&#x8A72;&#x5982;&#x4F55;&#x597D;&#x597D;&#x5EA6;&#x904E;&#x6B64;&#x751F;&#x3002;</p><figure class="kg-card kg-embed-card kg-card-hascaption"><iframe src="https://www.youtube.com/embed/_4DJkOUU648?feature=oembed" width="700" height="393" frameborder="0" scrolling="no"></iframe><figcaption>ZARD&#x200A;&#x2014;&#x200A;&#x8CA0;&#x3051;&#x306A;&#x3044;&#x3067;</figcaption></figure><p>&#x300A;&#x91CD;&#x555F;&#x4EBA;&#x751F;&#x300B;&#x7D66;&#x89C0;&#x773E;&#x5E36;&#x4F86;&#x4E86;&#x5F88;&#x591A;&#x7684;&#x601D;&#x8003;&#x548C;&#x611F;&#x52D5;&#x3002;&#x5B83;&#x8B93;&#x4EBA;&#x5011;&#x91CD;&#x65B0;&#x601D;&#x8003;&#x81EA;&#x5DF1;&#x7684;&#x751F;&#x547D;&#x610F;&#x7FA9;&#x548C;&#x50F9;&#x503C;&#xFF0C;&#x8B93;&#x4EBA;&#x5011;&#x610F;&#x8B58;&#x5230;&#x751F;&#x547D;&#x7684;&#x73CD;&#x8CB4;&#x8207;&#x6709;&#x9650;&#x3002;&#x9019;&#x90E8;&#x5F71;&#x96C6;&#x8B93;&#x6211;&#x5011;&#x91CD;&#x65B0;&#x56DE;&#x9867;&#x904E;&#x53BB;&#x7684;&#x6B72;&#x6708;&#xFF0C;&#x56DE;&#x61B6;&#x90A3;&#x4E9B;&#x7F8E;&#x597D;&#x7684;&#x6642;&#x5149;&#xFF0C;&#x540C;&#x6642;&#x4E5F;&#x8B93;&#x4EBA;&#x5011;&#x91CD;&#x65B0;&#x601D;&#x8003;&#x672A;&#x4F86;&#x7684;&#x65B9;&#x5411;&#xFF0C;&#x662F;&#x503C;&#x5F97;&#x4E00;&#x770B;&#x7684;&#x611F;&#x4EBA;&#x4F5C;&#x54C1;&#x3002;</p><figure class="kg-card kg-embed-card kg-card-hascaption"><iframe src="https://www.youtube.com/embed/B2fPYlGKdXM?feature=oembed" width="700" height="393" frameborder="0" scrolling="no"></iframe><figcaption>AI&#x200A;&#x2014;&#x200A;Story</figcaption></figure>]]></content:encoded></item><item><title><![CDATA[製作脆梅的時節 / Make Crispy Plum]]></title><description><![CDATA[製作脆梅一直是我們家重要的儀式，每年我們都會在年初與信義鄉的小農預訂青梅，並以此製作脆梅。]]></description><link>https://blog.cmwang.net/e8-a3-bd-e4-bd-9c-e8-84-86-e6-a2-85-e7-9a-84-e6-99-82-e7-af-80-make-crispy-plum/</link><guid isPermaLink="false">65ba7277b102c4db988584f0</guid><category><![CDATA[Plum]]></category><category><![CDATA[脆梅]]></category><category><![CDATA[Ceremony]]></category><category><![CDATA[Life]]></category><category><![CDATA[Family]]></category><dc:creator><![CDATA[Taka Wang]]></dc:creator><pubDate>Sun, 16 Apr 2023 15:16:37 GMT</pubDate><media:content url="https://blog.cmwang.net/content/images/max/800/1-1kj9n9ovro9pacr9jkftsw.jpg" medium="image"/><content:encoded><![CDATA[<h3 id="-make-crispy-plum">&#x88FD;&#x4F5C;&#x8106;&#x6885;&#x7684;&#x6642;&#x7BC0; | Make Crispy Plum</h3><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://blog.cmwang.net/content/images/max/800/1-8ya9rsc0l5ob3ll72cnmnw.jpg" class="kg-image" alt="&#x88FD;&#x4F5C;&#x8106;&#x6885;&#x7684;&#x6642;&#x7BC0; / Make Crispy Plum" loading="lazy" width="1440" height="1440" srcset="https://blog.cmwang.net/content/images/size/w600/max/800/1-8ya9rsc0l5ob3ll72cnmnw.jpg 600w, https://blog.cmwang.net/content/images/size/w1000/max/800/1-8ya9rsc0l5ob3ll72cnmnw.jpg 1000w, https://blog.cmwang.net/content/images/max/800/1-8ya9rsc0l5ob3ll72cnmnw.jpg 1440w" sizes="(min-width: 720px) 720px"><figcaption>&#x9752;&#x6885; | green&#xA0;plums</figcaption></figure><img src="https://blog.cmwang.net/content/images/max/800/1-1kj9n9ovro9pacr9jkftsw.jpg" alt="&#x88FD;&#x4F5C;&#x8106;&#x6885;&#x7684;&#x6642;&#x7BC0; / Make Crispy Plum"><p>&#x8FD1;&#x5E74;&#x4F86;&#xFF0C;&#x6E05;&#x660E;&#x7BC0;&#x524D;&#x5F8C;&#x4E00;&#x76F4;&#x662F;&#x6211;&#x5011;&#x5BB6;&#x91CD;&#x8981;&#x7684;&#x5100;&#x5F0F;&#xFF0C;&#x6211;&#x5011;&#x6703;&#x5728;&#x5E74;&#x521D;&#x8207;&#x4FE1;&#x7FA9;&#x9109;&#x7684;&#x5C0F;&#x8FB2;&#x9810;&#x8A02;12&#x516C;&#x65A4;&#x9752;&#x6885;&#xFF0C;&#x4E26;&#x4EE5;&#x6B64;&#x88FD;&#x4F5C;&#x8106;&#x6885;&#x3002;&#x7136;&#x800C;&#xFF0C;&#x4ECA;&#x5E74;&#x7531;&#x65BC;&#x96E8;&#x91CF;&#x4E0D;&#x8DB3;&#xFF0C;&#x679C;&#x5BE6;&#x76F8;&#x8F03;&#x65BC;&#x5F80;&#x5E74;&#x7A0D;&#x5ACC;&#x4E0D;&#x76E1;&#x4EBA;&#x610F;&#x3002;</p><p>&#x88FD;&#x4F5C;&#x8106;&#x6885;&#x7684;&#x65B9;&#x5F0F;&#x76F8;&#x7576;&#x7C21;&#x55AE;&#xFF0C;&#x4E0D;&#x9700;&#x8981;&#x8907;&#x96DC;&#x7684;&#x5668;&#x6750;&#xFF0C;&#x53EA;&#x9700;&#x82B1;&#x8CBB;&#x4E00;&#x4E9B;&#x6642;&#x9593;&#x8207;&#x8010;&#x5FC3;&#x3002;&#x914D;&#x65B9;&#x975E;&#x5E38;&#x7C21;&#x55AE;&#xFF0C;&#x4EE5;&#x4E00;&#x516C;&#x65A4;&#x9752;&#x6885;&#x3001;100&#x514B;&#x7C97;&#x9E7D;&#x548C;600&#x514B;&#x7802;&#x7CD6;&#x7684;&#x6BD4;&#x4F8B;&#x9032;&#x884C;&#x88FD;&#x4F5C;&#x3002;&#x63A5;&#x4E0B;&#x4F86;&#xFF0C;&#x8ACB;&#x4F9D;&#x7167;&#x4EE5;&#x4E0B;&#x6B65;&#x9A5F;&#x9032;&#x884C;&#xFF1A;</p><ul><li>&#x6E05;&#x6D17;&#xFF1A;&#x53BB;&#x6389;&#x6885;&#x5B50;&#x7684;&#x8482;&#x982D;&#xFF0C;&#x4E00;&#x9846;&#x9846;&#x6311;&#x9664;&#x3002;</li><li>&#x6BBA;&#x9752;&#xFF1A;&#x4F7F;&#x7528;&#x7C97;&#x9E7D;&#x5E6B;&#x52A9;&#x6885;&#x5B50;SPA&#xFF0C;&#x6413;&#x63C9;&#x81F3;&#x5C11;15&#x5206;&#x9418;&#xFF0C;&#x76F4;&#x5230;&#x8868;&#x9762;&#x6FD5;&#x6F64;&#x8B8A;&#x8272;&#x3002;</li><li>&#x6572;&#x6885;&#x5B50;&#xFF1A;&#x7528;&#x69CC;&#x5B50;&#x4E00;&#x9846;&#x9846;&#x6572;&#x6253;&#x81F3;&#x5FAE;&#x88C2;&#xFF0C;&#x5343;&#x842C;&#x5225;&#x6572;&#x788E;&#x4E86;&#x3002;</li><li>&#x6CE1;&#x9E7D;&#x6C34;8&#x5C0F;&#x6642;&#xFF1A;&#x52D9;&#x5FC5;&#x78BA;&#x4FDD;&#x9E7D;&#x6C34;&#x5B8C;&#x5168;&#x8986;&#x84CB;&#x4F4F;&#x6885;&#x5B50;&#x3002;</li><li>&#x63DB;&#x6C34;8&#x5C0F;&#x6642;&#xFF1A;&#x6BCF;&#x4E8C;&#x5230;&#x4E09;&#x5C0F;&#x6642;&#x66F4;&#x63DB;&#x4E00;&#x6B21;&#x6C34;&#x3002;</li><li>&#x52A0;&#x7CD6;&#x6C34;&#xFF1A;&#x6BCF;&#x5929;&#x66F4;&#x63DB;&#x65B0;&#x7684;&#x7CD6;&#x6C34;&#xFF0C;&#x8986;&#x84CB;&#x4F4F;&#x6885;&#x5B50;&#xFF0C;&#x91CD;&#x8907;&#x4E09;&#x5929;&#x3002;&#x63DB;&#x7CD6;&#x6C34;&#x7684;&#x904E;&#x7A0B;&#x9817;&#x70BA;&#x91CD;&#x8981;&#xFF0C;&#x8981;&#x5C0F;&#x5FC3;&#x8B39;&#x614E;&#xFF0C;&#x4EE5;&#x514D;&#x5F71;&#x97FF;&#x53E3;&#x611F;&#x3002;</li><li>&#x53BB;&#x6389;&#x82E6;&#x6F80;&#x7CD6;&#x6C34;&#xFF1A;&#x5C07;&#x6240;&#x6709;&#x7684;&#x82E6;&#x6F80;&#x7CD6;&#x6C34;&#x5012;&#x6389;&#xFF0C;&#x53D6;&#x4EE3;&#x70BA;&#x6700;&#x5F8C;&#x4E00;&#x6B21;&#x7684;&#x7CD6;&#x6C34;&#xFF0C;&#x518D;&#x7B49;&#x5F85;&#x4E09;&#x5929;&#xFF0C;&#x5373;&#x53EF;&#x5B8C;&#x6210;&#x88FD;&#x4F5C;&#x3002;</li></ul><p>&#x6211;&#x611F;&#x6FC0;&#x5BB6;&#x4EBA;&#x70BA;&#x751F;&#x6D3B;&#x5E36;&#x4F86;&#x7684;&#x5100;&#x5F0F;&#x611F;&#xFF0C;&#x9019;&#x7A2E;&#x611F;&#x89BA;&#x8B93;&#x6211;&#x611F;&#x53D7;&#x5230;&#x751F;&#x547D;&#x7684;&#x7F8E;&#x597D;&#x3002;</p><hr><p>The time leading up to and following the Tomb Sweeping Festival has become a significant rite for my family in recent years. We purchase green plums in advance from local farmers in Xinyi Township and use them to create preserved plums. However, due to insufficient rainfall this year, the fruit was slightly less than satisfactory compared to previous years.</p><p>Making preserved plums is a straightforward technique that doesn&#x2019;t call for specialized equipment; rather, it just takes some time and perseverance. One kilogram of green plums, 100 grams of coarse salt, and 600 grams of sugar make up the recipe, which is quite straightforward. Please carry out the actions listed below:</p><ul><li>Wash the plums and remove the stems one by one.</li><li>Blanch the plums by rubbing them with coarse salt for at least 15 minutes until the surface becomes moist and changes color.</li><li>Tap the plums with a hammer one by one until they crack slightly, but do not smash them.</li><li>Soak the plums in saltwater for 8 hours, making sure that the water covers them completely.</li><li>Pour out the syrup and replace it with fresh syrup every 8 hours.</li><li>Repeat step 5 for 3 days until the plums are fully infused with the sugar syrup.</li><li>Drain the plums of any remaining bitter syrup, replace it with the final batch of sugar syrup, and let it sit for another 3 days to complete the process.</li></ul><p>I am grateful to my family for bringing a sense of ceremony into our daily lives. This ritual makes me appreciate the beauty of life.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://blog.cmwang.net/content/images/max/800/1-_ervpwwjvx3arz_sv_rqcw.jpg" class="kg-image" alt="&#x88FD;&#x4F5C;&#x8106;&#x6885;&#x7684;&#x6642;&#x7BC0; / Make Crispy Plum" loading="lazy" width="1440" height="1440" srcset="https://blog.cmwang.net/content/images/size/w600/max/800/1-_ervpwwjvx3arz_sv_rqcw.jpg 600w, https://blog.cmwang.net/content/images/size/w1000/max/800/1-_ervpwwjvx3arz_sv_rqcw.jpg 1000w, https://blog.cmwang.net/content/images/max/800/1-_ervpwwjvx3arz_sv_rqcw.jpg 1440w" sizes="(min-width: 720px) 720px"><figcaption>&#x8106;&#x6885;&#x6210;&#x54C1; | Crispy&#xA0;Plum</figcaption></figure>]]></content:encoded></item><item><title><![CDATA[今天你會有好事發生]]></title><description><![CDATA[Today you will have a good day. Jesus spoke to them at once, “Don’t be afraid,” he said. “Take courage! I am here!” (Mark 6:50)]]></description><link>https://blog.cmwang.net/e4-bb-8a-e5-a4-a9-e4-bd-a0-e6-9c-83-e6-9c-89-e5-a5-bd-e4-ba-8b-e7-99-bc-e7-94-9f/</link><guid isPermaLink="false">65ba7277b102c4db988584ea</guid><category><![CDATA[我的出走日記]]></category><category><![CDATA[Myliberationnotes]]></category><category><![CDATA[Feelings]]></category><category><![CDATA[Courage]]></category><dc:creator><![CDATA[Taka Wang]]></dc:creator><pubDate>Thu, 06 Apr 2023 12:29:10 GMT</pubDate><media:content url="https://blog.cmwang.net/content/images/max/800/1-dts8vwjc-loeagf4bnsunw.jpg" medium="image"/><content:encoded><![CDATA[<img src="https://blog.cmwang.net/content/images/max/800/1-dts8vwjc-loeagf4bnsunw.jpg" alt="&#x4ECA;&#x5929;&#x4F60;&#x6703;&#x6709;&#x597D;&#x4E8B;&#x767C;&#x751F;"><p>Today you will have a good day. Jesus spoke to them at once, &#x201C;Don&#x2019;t be afraid,&#x201D; he said. &#x201C;Take courage! I am here!&#x201D; (Mark 6:50)</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://blog.cmwang.net/content/images/max/800/1-tmx6ku_lgl2oxumfvarlpw.png" class="kg-image" alt="&#x4ECA;&#x5929;&#x4F60;&#x6703;&#x6709;&#x597D;&#x4E8B;&#x767C;&#x751F;" loading="lazy" width="2000" height="1333" srcset="https://blog.cmwang.net/content/images/size/w600/max/800/1-tmx6ku_lgl2oxumfvarlpw.png 600w, https://blog.cmwang.net/content/images/size/w1000/max/800/1-tmx6ku_lgl2oxumfvarlpw.png 1000w, https://blog.cmwang.net/content/images/size/w1600/max/800/1-tmx6ku_lgl2oxumfvarlpw.png 1600w, https://blog.cmwang.net/content/images/max/800/1-tmx6ku_lgl2oxumfvarlpw.png 2000w" sizes="(min-width: 720px) 720px"><figcaption>My Liberation Notes</figcaption></figure>]]></content:encoded></item><item><title><![CDATA[Reflections on Five Years in the Company]]></title><description><![CDATA[As I reflect on my five years in this company, I realize how much has happened and how much I have grown…]]></description><link>https://blog.cmwang.net/reflections-on-five-years-in-the-company/</link><guid isPermaLink="false">65ba7277b102c4db988584f1</guid><category><![CDATA[Reflections]]></category><dc:creator><![CDATA[Jamie Wang]]></dc:creator><pubDate>Mon, 27 Mar 2023 14:46:01 GMT</pubDate><media:content url="https://blog.cmwang.net/content/images/max/800/1-soyyinqnadusp3aofsthqg-2x.jpg" medium="image"/><content:encoded><![CDATA[<img src="https://blog.cmwang.net/content/images/max/800/1-soyyinqnadusp3aofsthqg-2x.jpg" alt="Reflections on Five Years in the Company"><p>As I reflect on my five years in this company, I realize how much has happened and how much I have grown. From having a supervisor to being a lone ranger, to forming a small team and eventually founding the Nilvana, time has flown by quickly, and I am left wondering where tomorrow will take me.</p><p>It seems like just yesterday that I started my journey in this company. I remember the excitement of meeting new colleagues, learning new skills, and finding my place in the organization. Over time, I became more confident and began taking on more responsibilities. I was proud of my accomplishments and the impact that I was making.</p><p>However, the road was not always easy. There were times when I faced challenges that made me question my abilities and my decisions. I learned that setbacks and failures are a part of the journey, and they can be a source of growth and resilience.</p><p>One of the most significant moments in my journey was the creation of the Nilvana brand. It was a leap of faith, but I knew that I had a vision that could change the game. With the help of a few dedicated team members, we worked tirelessly to bring the brand to life. Seeing it take off and gain recognition was one of the proudest moments of my career.</p><p>Looking back on my journey, I realize that time truly flies. It seems like just yesterday that I started, but five years have gone by in a blink of an eye. As I prepare to take on new challenges, I am grateful for the experiences and the lessons that I have learned.</p><hr><figure class="kg-card kg-embed-card"><iframe src="https://www.youtube.com/embed/1t8kAbUg4t4?feature=oembed" width="700" height="393" frameborder="0" scrolling="no"></iframe></figure>]]></content:encoded></item><item><title><![CDATA[Backing Up a Stellar Data Engineer: Open for New Opportunities in Taipei]]></title><description><![CDATA[I am writing this post to recommend my colleague Jill Hsu for any relevant job opportunities in Taipei.]]></description><link>https://blog.cmwang.net/backing-up-a-stellar-data-engineer-open-for-new-opportunities-in-taipei/</link><guid isPermaLink="false">65ba7277b102c4db988584ef</guid><category><![CDATA[Jobsearch]]></category><category><![CDATA[Taipei]]></category><category><![CDATA[Backend Development]]></category><category><![CDATA[Data Science]]></category><category><![CDATA[Tech Jobs]]></category><dc:creator><![CDATA[Taka Wang]]></dc:creator><pubDate>Thu, 23 Mar 2023 11:08:10 GMT</pubDate><media:content url="https://blog.cmwang.net/content/images/max/800/1-h3-aa9-_smfa7fsgn8kavw.png" medium="image"/><content:encoded><![CDATA[<h3 id="backing-up-a-stellar-engineer-open-for-new-opportunities-in-taipei">Backing Up a Stellar Engineer: Open for New Opportunities in Taipei</h3><img src="https://blog.cmwang.net/content/images/max/800/1-h3-aa9-_smfa7fsgn8kavw.png" alt="Backing Up a Stellar Data Engineer: Open for New Opportunities in Taipei"><p>I am writing this post to recommend my colleague <a href="https://medium.com/u/e76bffbfaa3d?ref=localhost">Jill Hsu</a> for any relevant job opportunities in Taipei. As her teammate in the backend and machine learning data engineering team, I can attest to her outstanding work ethics, attention to detail, and passion for delivering high-quality results.</p><p><a href="https://medium.com/u/e76bffbfaa3d?ref=localhost">Jill Hsu</a> is a highly skilled data engineer with extensive experience in both backend development and machine learning. Her technical knowledge and expertise in backend development, including database management, API design, and server architecture, have been invaluable to our team&#x2019;s success. She has consistently demonstrated her ability to handle complex backend development projects and deliver results in a timely and efficient manner.</p><p>Moreover, <a href="https://medium.com/u/e76bffbfaa3d?ref=localhost">Jill Hsu</a> is a great team player and collaborator. She communicates effectively with her teammates and stakeholders, always willing to share her knowledge and insights to help others achieve their goals. Her positive attitude, proactive approach, and strong work ethic make her an asset to any team.</p><p>As <a href="https://medium.com/u/e76bffbfaa3d?ref=localhost">Jill Hsu</a> is now looking for new job opportunities in Taipei, I highly recommend her for any data engineering or related positions. She is a reliable and dedicated professional who can bring significant value to any organization. I am confident that her skills and experience will make her a valuable addition to any team.</p><p>I believe that she has the potential to excel in any role she takes on and wish her all the best in her career journey.</p><!--kg-card-begin: html--><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://takawang.medium.com/the-release-of-vision-studio-2-0-is-scheduled-for-november-a347d4bae86c?ref=localhost"><div class="kg-bookmark-content"><div class="kg-bookmark-title">The release of Vision Studio 2.0.0 is scheduled for November</div><div class="kg-bookmark-description">Support for multiple GPU training is one of the most commonly feature requested, we will incorporate this option in the&#x2026;</div><div class="kg-bookmark-metadata"></div></div><div class="kg-bookmark-thumbnail"><img src="https://blog.cmwang.net/content/images/fit/c/160/160/1-zatxvvtmagb_5hvah3tepq.jpg" alt="Backing Up a Stellar Data Engineer: Open for New Opportunities in Taipei"></div></a></figure><!--kg-card-end: html-->]]></content:encoded></item><item><title><![CDATA[Configuring USB-FS for USB3 Vision Camera]]></title><description><![CDATA[This post explains how to increase the buffer memory for USB-FS devices on Linux systems.]]></description><link>https://blog.cmwang.net/configuring-usb-fs-for-usb3-vision-camera/</link><guid isPermaLink="false">65ba7277b102c4db988584e4</guid><category><![CDATA[Industrial Camera Systems]]></category><category><![CDATA[Embedded Systems]]></category><category><![CDATA[U3v]]></category><category><![CDATA[Computer Vision]]></category><category><![CDATA[Machine Vision System]]></category><dc:creator><![CDATA[Taka Wang]]></dc:creator><pubDate>Thu, 29 Dec 2022 13:14:24 GMT</pubDate><media:content url="https://blog.cmwang.net/content/images/max/800/1-uoy9jp33j7xif2emebtdcg.jpg" medium="image"/><content:encoded><![CDATA[<img src="https://blog.cmwang.net/content/images/max/800/1-uoy9jp33j7xif2emebtdcg.jpg" alt="Configuring USB-FS for USB3 Vision Camera"><p>This post explains how to increase the buffer memory for USB-FS devices on Linux systems in order to make full use of the imaging hardware&#x2019;s capabilities. By default, USB-FS on Linux systems only allows 16 MB of buffer memory for all USB devices, which may not be sufficient for high-resolution cameras or multiple-camera set ups, resulting in image acquisition issues. To configure USB-FS and increase the buffer memory limit, the following steps should be taken:</p><p>Note that GRUB is for desktop PC architecture. ARM embedded systems use a different bootloader, as GRUB requires a system with a BIOS, and embedded systems do not have one.</p><ol><li>Create the file <code>/etc/rc.local</code> with the command <code>sudo touch /etc/rc.local</code>. This will create the file, allowing it to be edited.</li><li>Change the permissions of the file with the command <code>sudo chmod 744 /etc/rc.local</code>. This will ensure that the file has the correct permissions to be edited.</li><li>Change the buffer memory limit with the command <code>echo 1000 &gt; /sys/module/usbcore/parameters/usbfs_memory_mb</code>. This command will set the memory limit to 1000 MB, which should be enough to prevent image acquisition issues.</li></ol><pre><code class="language-bash">## /etc/rc.local file contents example 
 
#!/bin/sh -e 
echo 1000 &gt; /sys/module/usbcore/parameters/usbfs_memory_mb 
exit 0</code></pre><p>After changing the memory limit, it is important to confirm the changes have been made correctly. This can be done by running the command <code>cat /sys/module/usbcore/parameters/usbfs_memory_mb</code>, which will display the current memory limit. If the limit is still 16 MB, then the changes will need to be made again. Additionally, further information about USB-FS on Linux can be found in the following sources:</p><ul><li><a href="https://www.flir.asia/support-center/iis/machine-vision/application-note/understanding-usbfs-on-linux/?ref=localhost" rel="noopener">Understanding USBFS on Linux</a></li><li><a href="https://importgeek.wordpress.com/2017/02/26/increase-usbfs-memory-limit-in-ubuntu/?ref=localhost" rel="noopener">Increase USBFS Memory Limit in Ubuntu</a></li><li><a href="https://forums.developer.nvidia.com/t/change-kernel-cmdline-by-edit-etc-default-grub-failed/159831/2?ref=localhost" rel="noopener">Change Kernel Cmdline by Edit /etc/default/grub Failed</a></li></ul>]]></content:encoded></item><item><title><![CDATA[How to install People Counting Toolkit on your Jetson devices]]></title><description><![CDATA[The installation procedure is quite simple — bash <(curl -fsSL https://links.nilvana.ai/get-pc)]]></description><link>https://blog.cmwang.net/how-to-install-people-counting-toolkit-on-your-jetson-devices/</link><guid isPermaLink="false">65ba7277b102c4db988584eb</guid><category><![CDATA[Computer Vision]]></category><category><![CDATA[Artificial Intelligence]]></category><category><![CDATA[Deep Learning]]></category><category><![CDATA[AI]]></category><category><![CDATA[Jetson Nano]]></category><dc:creator><![CDATA[Taka Wang]]></dc:creator><pubDate>Wed, 28 Sep 2022 01:13:22 GMT</pubDate><media:content url="https://blog.cmwang.net/content/images/max/800/1-2zavqgypzo5yujkr_uzflq.jpg" medium="image"/><content:encoded><![CDATA[<img src="https://blog.cmwang.net/content/images/max/800/1-2zavqgypzo5yujkr_uzflq.jpg" alt="How to install People Counting Toolkit on your Jetson devices"><p><strong>People Counting Toolkit</strong> is a commercial product of Nilvana&#xAE; AI. The installation procedure is quite simple. You must first request a software activation code from us, then ensure that your device satisfies the requirements listed below:</p><ul><li>Jetpack 4.6.x installed</li><li>At least 15GB free space left</li></ul><!--kg-card-begin: html--><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://shop.nilvana.ai/products/people-counting-toolkit?ref=localhost"><div class="kg-bookmark-content"><div class="kg-bookmark-title">People Counting Toolkit</div><div class="kg-bookmark-description">Why you need people counting system? Successful site selection is one of the core competencies of a brand. Real-time&#x2026;</div><div class="kg-bookmark-metadata"></div></div><div class="kg-bookmark-thumbnail"><img src="https://blog.cmwang.net/content/images/fit/c/160/160/0-yx8vmnhogjwmvgpa.jpg" alt="How to install People Counting Toolkit on your Jetson devices"></div></a></figure><!--kg-card-end: html--><!--kg-card-begin: html--><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://shop.nilvana.ai/products/atom-nx-industrial?ref=localhost"><div class="kg-bookmark-content"><div class="kg-bookmark-title">Atom NX Industrial</div><div class="kg-bookmark-description">Rapid and stable field inference with fast integration. Equipped with NVIDIA&#xAE; Jetson Xavier&#x2122; NX. All-in-One hardware&#x2026;</div><div class="kg-bookmark-metadata"></div></div><div class="kg-bookmark-thumbnail"><img src="https://blog.cmwang.net/content/images/fit/c/160/160/0-ckzdtl2pgf-s7nj8.jpg" alt="How to install People Counting Toolkit on your Jetson devices"></div></a></figure><!--kg-card-end: html--><p>Open the terminal and enter the following command:</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://blog.cmwang.net/content/images/max/800/1-hqzoxv_3vnv9ek_glmus5w.png" class="kg-image" alt="How to install People Counting Toolkit on your Jetson devices" loading="lazy" width="1114" height="390" srcset="https://blog.cmwang.net/content/images/size/w600/max/800/1-hqzoxv_3vnv9ek_glmus5w.png 600w, https://blog.cmwang.net/content/images/size/w1000/max/800/1-hqzoxv_3vnv9ek_glmus5w.png 1000w, https://blog.cmwang.net/content/images/max/800/1-hqzoxv_3vnv9ek_glmus5w.png 1114w" sizes="(min-width: 720px) 720px"><figcaption>bash &lt;(curl -fsSL <a href="https://links.nilvana.ai/get-pc%29?ref=localhost" data-href="https://links.nilvana.ai/get-pc)" class="markup--anchor markup--figure-anchor" rel="noopener" target="_blank">https://links.nilvana.ai/get-pc)</a></figcaption></figure><p>If your system doesn&#x2019;t have curl, you can install it like this:</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://blog.cmwang.net/content/images/max/800/1-odmq_raf3jb083mw4rmxeq.png" class="kg-image" alt="How to install People Counting Toolkit on your Jetson devices" loading="lazy" width="1130" height="390" srcset="https://blog.cmwang.net/content/images/size/w600/max/800/1-odmq_raf3jb083mw4rmxeq.png 600w, https://blog.cmwang.net/content/images/size/w1000/max/800/1-odmq_raf3jb083mw4rmxeq.png 1000w, https://blog.cmwang.net/content/images/max/800/1-odmq_raf3jb083mw4rmxeq.png 1130w" sizes="(min-width: 720px) 720px"><figcaption>sudo apt-get update &amp;&amp; sudo apt-get install -y&#xA0;curl</figcaption></figure><p>This toolkit is implemented as a container-based micro-services, so you don&#x2019;t need to be an expert in Docker to utilize it&#x200A;&#x2014;&#x200A;we&#x2019;ll take care of the rest.</p><p>We don&#x2019;t currently provide support for Jetpack 5.x, but if you have this or other platform needs, do not hesitate to contact us.</p><!--kg-card-begin: html--><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://github.com/nilvana-ai/people-counting-installer?ref=localhost"><div class="kg-bookmark-content"><div class="kg-bookmark-title">GitHub - nilvana-ai/people-counting-installer: People counting toolkit installer</div><div class="kg-bookmark-description">People Counting Toolkit is a commercial product of Nilvana&#x2122; AI, please refer to the official website for more about&#x2026;</div><div class="kg-bookmark-metadata"></div></div><div class="kg-bookmark-thumbnail"><img src="https://blog.cmwang.net/content/images/fit/c/160/160/0-if_vsqtz28zh7zyo.png" alt="How to install People Counting Toolkit on your Jetson devices"></div></a></figure><!--kg-card-end: html--><hr><p>Nilvana&#xAE; has developed <strong>Vision Studio</strong> and <strong>Vision Inference Runtime</strong> toolkits, which enable collaborative data annotation, model creation, and rapid inference without AI expertise.</p><p>Nilvana&#xAE; provides a <a href="https://shop.nilvana.ai/pages/our-services?ref=localhost" rel="noopener ugc nofollow noopener noopener">wide range of services</a> from consulting and model creation to the development of AI-based applications. Feel free to <a href="https://shop.nilvana.ai/pages/contact-us?ref=localhost" rel="noopener ugc nofollow noopener noopener">contact us</a> for any inquiries.</p><!--kg-card-begin: html--><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://nilvana.ai/?ref=localhost"><div class="kg-bookmark-content"><div class="kg-bookmark-title">Hello nilvana&#x2122;, Hello Tomorrow</div><div class="kg-bookmark-description">AI Workstation, Jetson Nano, Xavier, Nvidia, Edge Computing, IoT, Edge Computing, &#x4EBA;&#x5DE5;&#x667A;&#x6167;&#xFF0C;&#x5DE5;&#x4F5C;&#x7AD9;&#xFF0C;&#x7269;&#x806F;&#x7DB2;&#xFF0C;&#x908A;&#x7DE3;&#x904B;&#x7B97;&#xFF0C;&#x908A;&#x7DE3;&#x88DD;&#x7F6E;&#x3002;</div><div class="kg-bookmark-metadata"></div></div><div class="kg-bookmark-thumbnail"><img src="https://blog.cmwang.net/content/images/fit/c/160/160/0-pwhpl116x_cl6sk5.jpg" alt="How to install People Counting Toolkit on your Jetson devices"></div></a></figure><!--kg-card-end: html-->]]></content:encoded></item><item><title><![CDATA[Make your own pig counter]]></title><description><![CDATA[In this article, I’ll show you how to create a basic pig counter using vision studio and inference runtime.]]></description><link>https://blog.cmwang.net/make-your-own-pig-counter/</link><guid isPermaLink="false">65ba7277b102c4db988584df</guid><category><![CDATA[Object Detection]]></category><category><![CDATA[Object Tracking]]></category><category><![CDATA[Computer Vision]]></category><category><![CDATA[Artificial Intelligence]]></category><category><![CDATA[Deep Learning]]></category><dc:creator><![CDATA[Taka Wang]]></dc:creator><pubDate>Thu, 15 Sep 2022 08:01:11 GMT</pubDate><media:content url="https://blog.cmwang.net/content/images/max/800/1-g2dy2xyioybolnrapwaxvg.jpg" medium="image"/><content:encoded><![CDATA[<h3 id="make-your-own-pig-counter-">Make your own pig counter &#x1F437;</h3><img src="https://blog.cmwang.net/content/images/max/800/1-g2dy2xyioybolnrapwaxvg.jpg" alt="Make your own pig counter"><p>&#x201C;Pork meat is a good source of proteins since their quality is high biological value and has all required amino-acids to promote simple absorption by the organism,&#x201D; says nutritionist Magnolia Escobar. Its meat has a high degree of or rate of digestibility, which can reach 92%, and is more similar to white meat than red meat. But how does artificial intelligence relate to the pig <strong>breeding industry</strong>? Can artificial intelligence and computer vision boost the breeding industry&#x2019;s output? We&#x2019;ll put on a quick demonstration in this post. No offense to vegans, but this is only an illustration. We all love Peppa Pig &#x1F43D;.</p><p>In this article, I&#x2019;ll show you how to create a basic pig counter using <a href="https://shop.nilvana.ai/products/nilvana-vision-studio?ref=localhost" rel="noopener">vision studio</a> and <a href="https://shop.nilvana.ai/products/nilvana-vision-inference-runtime?ref=localhost" rel="noopener">inference runtime</a>. You must first realize that <strong>object detection</strong> and <strong>object tracking</strong> are two distinct ideas. Finding the class and coordinates of objects in a given frame is the goal of object detection, whereas associating objects in a series of frames is the goal of object tracking. Our toolkits can assist you in solving the object detection issue, and the OpenCV library has at least seven pre-made object tracking methods that you can employ as necessary.</p><!--kg-card-begin: html--><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://docs.opencv.org/4.6.0/d2/d0a/tutorial_introduction_to_tracker.html?ref=localhost"><div class="kg-bookmark-content"><div class="kg-bookmark-title">OpenCV: Introduction to OpenCV Tracker</div><div class="kg-bookmark-description">In this tutorial you will learn how to Create a tracker object. Use the roi Selector function to select a ROI from a&#x2026;</div><div class="kg-bookmark-metadata"></div></div><div class="kg-bookmark-thumbnail"><img src="https://blog.cmwang.net/content/images/fit/c/160/160/0-rcm5nzhymkic4psy.png" alt="Make your own pig counter"></div></a></figure><!--kg-card-end: html--><p>To build a pig detection model, we must first collect photos of pigs for labeling. We can find the labeled photos of pigs from this paper&#x200A;&#x2014;&#x200A;<a href="https://arxiv.org/abs/2111.10971?ref=localhost" rel="noopener">Tracking Grow-Finish Pigs Across Large Pens Using Multiple Cameras</a>. Sincere thanks to The <a href="https://ansc.illinois.edu/about/facilities/imported-swine-research-laboratory?ref=localhost" rel="noopener">Imported Swine Research Laboratory</a> (ISRL). Then we can import the dataset into vision studio for model creation.</p><!--kg-card-begin: html--><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://drive.google.com/drive/folders/1E2wW2aRENgy_TqlzfICn58ahbTHVIaK6?usp=sharing&amp;ref=localhost"><div class="kg-bookmark-content"><div class="kg-bookmark-title">ISRL Multi-Camera Tracking Dataset - Google Drive</div><div class="kg-bookmark-description">Edit description</div><div class="kg-bookmark-metadata"></div></div><div class="kg-bookmark-thumbnail"><img src="https://blog.cmwang.net/content/images/fit/c/160/160/0-akzefkxu7-8d39iz.png" alt="Make your own pig counter"></div></a></figure><!--kg-card-end: html--><hr><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://blog.cmwang.net/content/images/max/800/1-dpurpw5awev7ab7gzs4-ta.png" class="kg-image" alt="Make your own pig counter" loading="lazy" width="1427" height="808" srcset="https://blog.cmwang.net/content/images/size/w600/max/800/1-dpurpw5awev7ab7gzs4-ta.png 600w, https://blog.cmwang.net/content/images/size/w1000/max/800/1-dpurpw5awev7ab7gzs4-ta.png 1000w, https://blog.cmwang.net/content/images/max/800/1-dpurpw5awev7ab7gzs4-ta.png 1427w" sizes="(min-width: 720px) 720px"><figcaption>ISRL datasets in vision&#xA0;studio.</figcaption></figure><p>You may use the <a href="https://github.com/nilvana-ai/nvir-mask-demo/blob/main/video.py?ref=localhost" rel="noopener"><strong>nvir-mask-demo</strong> repository</a> we previously provided to infer the video once you have finished training the model and establishing the inference endpoint. To incorporate object tracking into your pipeline, you simply need to make a minor modification. In this case, I opt for Kernelized Correlation Filters (KCF), which can consider both speed and accuracy. You can use different algorithms depending on your particular circumstances. The list of pseudo-code is shown below.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://blog.cmwang.net/content/images/max/800/1-_35nc1s54zmdws9znulioq.png" class="kg-image" alt="Make your own pig counter" loading="lazy" width="1285" height="754" srcset="https://blog.cmwang.net/content/images/size/w600/max/800/1-_35nc1s54zmdws9znulioq.png 600w, https://blog.cmwang.net/content/images/size/w1000/max/800/1-_35nc1s54zmdws9znulioq.png 1000w, https://blog.cmwang.net/content/images/max/800/1-_35nc1s54zmdws9znulioq.png 1285w" sizes="(min-width: 720px) 720px"><figcaption>Pseudo code of object&#xA0;tracking</figcaption></figure><p>With this pipeline, you only need to count the pigs that cross a <strong>ROI line</strong> in order to calculate the number. Here is a short demonstration video we created. The original author is the rightful owner of the video. I hope you enjoy this demo.</p><figure class="kg-card kg-embed-card"><iframe src="https://www.youtube.com/embed/v0O3NPUery4?feature=oembed" width="700" height="393" frameborder="0" scrolling="no"></iframe></figure><blockquote>Pigs are highly intelligent, social animals!<br><br>The study &#x201C;<a href="https://arxiv.org/abs/2111.10971?ref=localhost" rel="noopener">Tracking Grow-Finish Pigs Across Large Pens Using Multiple Cameras</a>&#x201D; indicated how socially active pigs are. Quick social contacts lead to <strong>identity shifts</strong>. These are the tracking challenges. Difficulties in detection: Pigs gather to stay warm, which might lead to <strong>occlusion</strong>.</blockquote><p>Therefore, today&#x2019;s demonstration is only for educational purposes; further thought and experimental planning are required to create a reliable pig counter. Please don&#x2019;t hesitate to <a href="https://shop.nilvana.ai/pages/contact-us?ref=localhost" rel="noopener">get in touch with us</a> so we can assist you in building your own solution.</p><hr><p>Nilvana<strong>&#x2122;</strong> has developed <strong>Vision Studio</strong> and <strong>Vision Inference Runtime</strong> toolkits, which enable collaborative data annotation, model creation, and rapid inference without AI expertise.</p><p>Nilvana<strong>&#x2122;</strong> provides a <a href="https://shop.nilvana.ai/pages/our-services?ref=localhost" rel="noopener ugc nofollow noopener">wide range of services</a> from consulting and model creation to the development of AI-based applications. Feel free to <a href="https://shop.nilvana.ai/pages/contact-us?ref=localhost" rel="noopener ugc nofollow noopener">contact us</a> for any inquiries.</p><!--kg-card-begin: html--><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://nilvana.ai/?ref=localhost"><div class="kg-bookmark-content"><div class="kg-bookmark-title">Hello nilvana&#x2122;, Hello Tomorrow</div><div class="kg-bookmark-description">AI Workstation, Jetson Nano, Xavier, Nvidia, Edge Computing, IoT, Edge Computing, &#x4EBA;&#x5DE5;&#x667A;&#x6167;&#xFF0C;&#x5DE5;&#x4F5C;&#x7AD9;&#xFF0C;&#x7269;&#x806F;&#x7DB2;&#xFF0C;&#x908A;&#x7DE3;&#x904B;&#x7B97;&#xFF0C;&#x908A;&#x7DE3;&#x88DD;&#x7F6E;&#x3002;</div><div class="kg-bookmark-metadata"></div></div><div class="kg-bookmark-thumbnail"><img src="https://blog.cmwang.net/content/images/fit/c/160/160/0-vcyzxuvuo4tg_4_m.jpg" alt="Make your own pig counter"></div></a></figure><!--kg-card-end: html--><!--kg-card-begin: html--><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://shop.nilvana.ai/products/nilvana-vision-studio?ref=localhost"><div class="kg-bookmark-content"><div class="kg-bookmark-title">Nilvana&#x2122; Vision Studio</div><div class="kg-bookmark-description">AI Workstation, Jetson Nano, Xavier, Nvidia, Edge Computing, IoT, Edge Computing, &#x4EBA;&#x5DE5;&#x667A;&#x6167;&#xFF0C;&#x5DE5;&#x4F5C;&#x7AD9;&#xFF0C;&#x7269;&#x806F;&#x7DB2;&#xFF0C;&#x908A;&#x7DE3;&#x904B;&#x7B97;&#xFF0C;&#x908A;&#x7DE3;&#x88DD;&#x7F6E;&#x3002;</div><div class="kg-bookmark-metadata"></div></div><div class="kg-bookmark-thumbnail"><img src="https://blog.cmwang.net/content/images/fit/c/160/160/0-yekycoi5fyp_lsjs.png" alt="Make your own pig counter"></div></a></figure><!--kg-card-end: html--><!--kg-card-begin: html--><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://shop.nilvana.ai/products/nilvana-vision-inference-runtime?ref=localhost"><div class="kg-bookmark-content"><div class="kg-bookmark-title">Nilvana&#x2122; Vision Inference Runtime</div><div class="kg-bookmark-description">Aside from light-weighting the model and reducing resource requirements on the edge devices, it is also optimized for&#x2026;</div><div class="kg-bookmark-metadata"></div></div><div class="kg-bookmark-thumbnail"><img src="https://blog.cmwang.net/content/images/fit/c/160/160/0-vvnuailsg9aqw2xr.jpg" alt="Make your own pig counter"></div></a></figure><!--kg-card-end: html-->]]></content:encoded></item><item><title><![CDATA[Object Detection from the Video Taken by Drone]]></title><description><![CDATA[Nilvana™ Vision Studio has an intuitive web interface that makes it easy to merge annotation data or categories from different datasets.]]></description><link>https://blog.cmwang.net/object-detection-from-the-video-taken-by-drone/</link><guid isPermaLink="false">65ba7277b102c4db988584e1</guid><category><![CDATA[Drones]]></category><category><![CDATA[Object Detection]]></category><category><![CDATA[Computer Vision]]></category><category><![CDATA[Deep Learning]]></category><category><![CDATA[Artificial Intelligence]]></category><dc:creator><![CDATA[Taka Wang]]></dc:creator><pubDate>Tue, 13 Sep 2022 01:46:48 GMT</pubDate><media:content url="https://blog.cmwang.net/content/images/max/800/1-4hcxr7y6b8vdvqo31il38g.jpg" medium="image"/><content:encoded><![CDATA[<img src="https://blog.cmwang.net/content/images/max/800/1-4hcxr7y6b8vdvqo31il38g.jpg" alt="Object Detection from the Video Taken by Drone"><p>In the <a href="https://link.medium.com/0pZ5J3tCftb?ref=localhost" rel="noopener"><strong>last article</strong></a> from <a href="https://medium.com/hello-nilvana?ref=localhost">hello nilvana</a>, we learned how to import the <a href="http://aiskyeye.com/?ref=localhost" rel="noopener">VisDrone dataset</a> into Nilvana<strong>&#x2122;</strong> Vision Studio. It&#x2019;s not that difficult, just follow the instructions or ask us for help. If you have not read it yet, you definitely have to!</p><!--kg-card-begin: html--><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://link.medium.com/0pZ5J3tCftb?ref=localhost"><div class="kg-bookmark-content"><div class="kg-bookmark-title">VisDrone Datasets - Nilavna Vision Studio</div><div class="kg-bookmark-description">&#x4ECA;&#x65E5;&#x8981;&#x5206;&#x4EAB;&#x7684;&#x662F;&#xFF0C;&#x5982;&#x4F55;&#x5C07;&#x76EE;&#x524D; VisDrone &#x7684;&#x7167;&#x7247;&#x8CC7;&#x6599;&#x96C6;&#x900F;&#x904E; Nilvana&#x2122; Vision Studio &#x9032;&#x884C;&#x532F;&#x5165;&#xFF0C;&#x65B9;&#x4FBF;&#x65E5;&#x5F8C;&#x9032;&#x884C;&#x6A21;&#x578B;&#x7684;&#x8A13;&#x7DF4;&#x3002;</div><div class="kg-bookmark-metadata"></div></div><div class="kg-bookmark-thumbnail"><img src="https://blog.cmwang.net/content/images/fit/c/160/160/0-worej9bweqrri1bg.png" alt="Object Detection from the Video Taken by Drone"></div></a></figure><!--kg-card-end: html--><p>As you can see in the Vision Studio screenshots below, you can clearly see the 10 categories annotated in the images. Some time ago I found a beautiful video&#x200A;&#x2014;&#x200A;<a href="https://youtu.be/yXEb0fWLJIY?ref=localhost" rel="noopener"><strong>World&#x2019;s longest drone fpv one shot</strong></a> made by <a href="https://newstepmedia.net/?ref=localhost" rel="noopener">New Step Media</a> on Youtube. I found that there are a lot of indoor scenes that are very suitable for adding the COCO dataset based on the visdrone dataset, so I used this combined dataset to train a object detector. In case you didn&#x2019;t already know, Nilvana<strong>&#x2122;</strong> Vision Studio has an intuitive web interface that makes it easy to <strong>merge annotation data</strong> or <strong>categories</strong> from different datasets with a few mouse clicks.</p><figure class="kg-card kg-gallery-card kg-width-wide kg-card-hascaption"><div class="kg-gallery-container"><div class="kg-gallery-row"><div class="kg-gallery-image"><img src="https://blog.cmwang.net/content/images/max/1000/1-hsvqcqt8rvoqfztapf_yra.png" width="1756" height="938" loading="lazy" alt="Object Detection from the Video Taken by Drone" srcset="https://blog.cmwang.net/content/images/size/w600/max/1000/1-hsvqcqt8rvoqfztapf_yra.png 600w, https://blog.cmwang.net/content/images/size/w1000/max/1000/1-hsvqcqt8rvoqfztapf_yra.png 1000w, https://blog.cmwang.net/content/images/size/w1600/max/1000/1-hsvqcqt8rvoqfztapf_yra.png 1600w, https://blog.cmwang.net/content/images/max/1000/1-hsvqcqt8rvoqfztapf_yra.png 1756w" sizes="(min-width: 720px) 720px"></div><div class="kg-gallery-image"><img src="https://blog.cmwang.net/content/images/max/400/1-qhif2slonafquakcfivdwg.png" width="619" height="907" loading="lazy" alt="Object Detection from the Video Taken by Drone" srcset="https://blog.cmwang.net/content/images/size/w600/max/400/1-qhif2slonafquakcfivdwg.png 600w, https://blog.cmwang.net/content/images/max/400/1-qhif2slonafquakcfivdwg.png 619w"></div></div></div><figcaption>Visdrone dataset on Nilvana<strong class="markup--strong markup--figure-strong">&#x2122;</strong> Vision&#xA0;Studio</figcaption></figure><p>You can watch the finished video below. Kudos to <a href="https://newstepmedia.net/?ref=localhost" rel="noopener">New Step Media</a> for creating such a stunning aerial video. Ownership of the original video belongs to <a href="https://newstepmedia.net/?ref=localhost" rel="noopener">New Step Media</a>.</p><figure class="kg-card kg-embed-card kg-card-hascaption"><iframe src="https://www.youtube.com/embed/-sPxNKfXasU?feature=oembed" width="700" height="393" frameborder="0" scrolling="no"></iframe><figcaption>Drone video object detection</figcaption></figure><hr><p>Nilvana<strong>&#x2122;</strong> has developed <strong>Vision Studio</strong> and <strong>Vision Inference Runtime</strong> toolkits, which enable collaborative data annotation, model creation, and rapid inference without AI expertise.</p><p>Nilvana<strong>&#x2122;</strong> provides a <a href="https://shop.nilvana.ai/pages/our-services?ref=localhost" rel="noopener ugc nofollow noopener">wide range of services</a> from consulting and model creation to the development of AI-based applications. Feel free to <a href="https://shop.nilvana.ai/pages/contact-us?ref=localhost" rel="noopener ugc nofollow noopener">contact us</a> for any inquiries.</p><!--kg-card-begin: html--><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://shop.nilvana.ai/products/nilvana-vision-studio?ref=localhost"><div class="kg-bookmark-content"><div class="kg-bookmark-title">Nilvana&#x2122; Vision Studio</div><div class="kg-bookmark-description">AI Workstation, Jetson Nano, Xavier, Nvidia, Edge Computing, IoT, Edge Computing, &#x4EBA;&#x5DE5;&#x667A;&#x6167;&#xFF0C;&#x5DE5;&#x4F5C;&#x7AD9;&#xFF0C;&#x7269;&#x806F;&#x7DB2;&#xFF0C;&#x908A;&#x7DE3;&#x904B;&#x7B97;&#xFF0C;&#x908A;&#x7DE3;&#x88DD;&#x7F6E;&#x3002;</div><div class="kg-bookmark-metadata"></div></div><div class="kg-bookmark-thumbnail"><img src="https://blog.cmwang.net/content/images/fit/c/160/160/0-t1p4i956ywoiisku.png" alt="Object Detection from the Video Taken by Drone"></div></a></figure><!--kg-card-end: html--><!--kg-card-begin: html--><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://nilvana.ai/?ref=localhost"><div class="kg-bookmark-content"><div class="kg-bookmark-title">Hello nilvana&#x2122;, Hello Tomorrow</div><div class="kg-bookmark-description">AI Workstation, Jetson Nano, Xavier, Nvidia, Edge Computing, IoT, Edge Computing, &#x4EBA;&#x5DE5;&#x667A;&#x6167;&#xFF0C;&#x5DE5;&#x4F5C;&#x7AD9;&#xFF0C;&#x7269;&#x806F;&#x7DB2;&#xFF0C;&#x908A;&#x7DE3;&#x904B;&#x7B97;&#xFF0C;&#x908A;&#x7DE3;&#x88DD;&#x7F6E;&#x3002;</div><div class="kg-bookmark-metadata"></div></div><div class="kg-bookmark-thumbnail"><img src="https://blog.cmwang.net/content/images/fit/c/160/160/0-axl713cxhhq1guco.jpg" alt="Object Detection from the Video Taken by Drone"></div></a></figure><!--kg-card-end: html-->]]></content:encoded></item><item><title><![CDATA[Object detection on a 360 degree camera]]></title><description><![CDATA[As long as you use the same camera to collect photos and perform model inference, you don’t need to care about the type of cameras.]]></description><link>https://blog.cmwang.net/object-detection-on-a-360-degree-camera/</link><guid isPermaLink="false">65ba7277b102c4db988584e0</guid><category><![CDATA[Object Tracking]]></category><category><![CDATA[Object Detection]]></category><category><![CDATA[Computer Vision]]></category><category><![CDATA[Deep Learning]]></category><category><![CDATA[Artificial Intelligence]]></category><dc:creator><![CDATA[Taka Wang]]></dc:creator><pubDate>Wed, 07 Sep 2022 02:52:06 GMT</pubDate><media:content url="https://blog.cmwang.net/content/images/max/800/1-akdt9gxneokjpcdchodnoq.jpg" medium="image"/><content:encoded><![CDATA[<h3 id="object-detection-on-a-360-degree-camera">Object Detection on a 360 degree camera</h3><img src="https://blog.cmwang.net/content/images/max/800/1-akdt9gxneokjpcdchodnoq.jpg" alt="Object detection on a 360 degree camera"><p>We get this question a lot from customers: &#x201C;What kind of cameras should I acquire?&#x201D; Is it possible to detect objects while using a GoPro or other 360-degree camera with <strong>Nilvana&#x2122; vision studio</strong>? The simple answer is that as long as you use the same camera to collect photos and the same device to perform model inference, you don&#x2019;t need to care about the type of cameras you use.</p><p>Due to the necessity of traveling throughout Taiwan, our colleague Sam Li just bought the <a href="https://www.insta360.com/fr/product/insta360-oners/1inch-360?ref=localhost" rel="noopener">Insta360 One RS 1-inch</a>, the company&#x2019;s first 360 camera with two 1-inch CMOS sensors. I think that will result in extremely good image quality performance, we wish him a safe journey.</p><figure class="kg-card kg-image-card"><img src="https://blog.cmwang.net/content/images/max/800/1-fuokuvzhf7ec5jt0ajqjna.gif" class="kg-image" alt="Object detection on a 360 degree camera" loading="lazy" width="638" height="385" srcset="https://blog.cmwang.net/content/images/size/w600/max/800/1-fuokuvzhf7ec5jt0ajqjna.gif 600w, https://blog.cmwang.net/content/images/max/800/1-fuokuvzhf7ec5jt0ajqjna.gif 638w"></figure><p>Sam really used this camera to gather a lot of real road vehicle condition videos for the vehicle counting toolkit that will be released next month, so keep an eye out for that.</p><figure class="kg-card kg-gallery-card kg-width-wide kg-card-hascaption"><div class="kg-gallery-container"><div class="kg-gallery-row"><div class="kg-gallery-image"><img src="https://blog.cmwang.net/content/images/max/600/1-mhtfv3zohrxoddkrw19rdq.jpg" width="2000" height="1365" loading="lazy" alt="Object detection on a 360 degree camera" srcset="https://blog.cmwang.net/content/images/size/w600/max/600/1-mhtfv3zohrxoddkrw19rdq.jpg 600w, https://blog.cmwang.net/content/images/size/w1000/max/600/1-mhtfv3zohrxoddkrw19rdq.jpg 1000w, https://blog.cmwang.net/content/images/size/w1600/max/600/1-mhtfv3zohrxoddkrw19rdq.jpg 1600w, https://blog.cmwang.net/content/images/size/w2400/max/600/1-mhtfv3zohrxoddkrw19rdq.jpg 2400w" sizes="(min-width: 720px) 720px"></div><div class="kg-gallery-image"><img src="https://blog.cmwang.net/content/images/max/800/1-ker9jficq_gl5ylppcfx0q.png" width="1446" height="813" loading="lazy" alt="Object detection on a 360 degree camera" srcset="https://blog.cmwang.net/content/images/size/w600/max/800/1-ker9jficq_gl5ylppcfx0q.png 600w, https://blog.cmwang.net/content/images/size/w1000/max/800/1-ker9jficq_gl5ylppcfx0q.png 1000w, https://blog.cmwang.net/content/images/max/800/1-ker9jficq_gl5ylppcfx0q.png 1446w" sizes="(min-width: 720px) 720px"></div></div></div><figcaption>Vehicle Counting Toolkit is coming&#xA0;soon!!</figcaption></figure><p>Finally, a video of the actual inference of the insta360 video is presented for your reference. It would be great to see more compelling application cases on a 360-degree camera.</p><figure class="kg-card kg-embed-card kg-card-hascaption"><iframe src="https://www.youtube.com/embed/wi_HJ9aobJQ?feature=oembed" width="700" height="393" frameborder="0" scrolling="no"></iframe><figcaption>Object detection in action &#x1F680;</figcaption></figure><hr><p>Nilvana<strong>&#x2122;</strong> has developed <strong>Vision Studio</strong> and <strong>Vision Inference Runtime</strong> toolkits, which enable collaborative data annotation, model creation, and rapid inference without AI expertise.</p><p>Nilvana<strong>&#x2122;</strong> provides a <a href="https://shop.nilvana.ai/pages/our-services?ref=localhost" rel="noopener ugc nofollow noopener">wide range of services</a> from consulting and model creation to the development of AI-based applications. Feel free to <a href="https://shop.nilvana.ai/pages/contact-us?ref=localhost" rel="noopener ugc nofollow noopener">contact us</a> for any inquiries.</p><!--kg-card-begin: html--><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://nilvana.ai/?ref=localhost"><div class="kg-bookmark-content"><div class="kg-bookmark-title">Hello nilvana&#x2122;, Hello Tomorrow</div><div class="kg-bookmark-description">AI Workstation, Jetson Nano, Xavier, Nvidia, Edge Computing, IoT, Edge Computing, &#x4EBA;&#x5DE5;&#x667A;&#x6167;&#xFF0C;&#x5DE5;&#x4F5C;&#x7AD9;&#xFF0C;&#x7269;&#x806F;&#x7DB2;&#xFF0C;&#x908A;&#x7DE3;&#x904B;&#x7B97;&#xFF0C;&#x908A;&#x7DE3;&#x88DD;&#x7F6E;&#x3002;</div><div class="kg-bookmark-metadata"></div></div><div class="kg-bookmark-thumbnail"><img src="https://blog.cmwang.net/content/images/fit/c/160/160/0-flo4dgj3qprtljo6.jpg" alt="Object detection on a 360 degree camera"></div></a></figure><!--kg-card-end: html-->]]></content:encoded></item><item><title><![CDATA[Execute YOLOv7 model on iOS devices]]></title><description><![CDATA[Real-time object detection with CoreML is trickier than you think.]]></description><link>https://blog.cmwang.net/execute-yolov7-model-on-ios-devices/</link><guid isPermaLink="false">65ba7277b102c4db988584e3</guid><category><![CDATA[Coreml]]></category><category><![CDATA[iOS]]></category><category><![CDATA[Yolov7]]></category><category><![CDATA[Object Detection]]></category><category><![CDATA[Artificial Intelligence]]></category><dc:creator><![CDATA[Taka Wang]]></dc:creator><pubDate>Sat, 06 Aug 2022 11:04:15 GMT</pubDate><media:content url="https://blog.cmwang.net/content/images/max/800/1-0nm5zh4imj2n42wibaybtq.jpg" medium="image"/><content:encoded><![CDATA[<h3 id="run-yolov7-model-on-ios-devices">Run YOLOv7 model on iOS devices</h3><img src="https://blog.cmwang.net/content/images/max/800/1-0nm5zh4imj2n42wibaybtq.jpg" alt="Execute YOLOv7 model on iOS devices"><p>Usually, you have two choices to build a machine learning app for your mobile device, <strong>inference</strong> can happen either directly on-device or on cloud-based servers. It all depends on your usage scenario, there is no one-size fit all solution. In this article, we will only focus on on-device inference, if your are interested in remote inference solution, maybe you can take a look at <a href="https://shop.nilvana.ai/products/nilvana-vision-inference-runtime?ref=localhost" rel="noopener">Nilvana&#x2122; Vision Inference Runtime</a>.</p><p>At WWDC 2017 Apple released first Core ML. Core ML is Apple&#x2019;s machine learning framework for doing on-device inference. Core ML is not the only way to do on-device inference, there are tens of libraries and frameworks that are compatible with iOS, but that&#x2019;s beyond the scope of this article. From the <a href="https://github.com/WongKinYiu/yolov7?ref=localhost" rel="noopener">YOLOv7 official repository</a>, we can get the <a href="https://github.com/WongKinYiu/yolov7/blob/main/export.py?ref=localhost" rel="noopener">export script</a> to convert trained PyTorch model to Core ML format effortlessly. However, keep one thing in mind, YOLOv7 is a popular open source project, new changes and updates are added very quickly. I&#x2019;m also very glad to send a <a href="https://github.com/WongKinYiu/yolov7/pull/434/commits?ref=localhost" rel="noopener">PR</a> to improve the export script last night due to this writing &#x1F603;.</p><hr><p>After you got the exported Core ML models, no kidding, you have tons of things in your todo list. <a href="https://twitter.com/mhollemans?ref=localhost" rel="noopener">Matthijs Hollemans</a> has already written an insightful <a href="https://machinethink.net/blog/bounding-boxes/?ref=localhost" rel="noopener">article</a> in his blog, be sure to checkout and <a href="https://leanpub.com/coreml-survival-guide?ref=localhost" rel="noopener">support his efforts</a>! Here is my short list:</p><ul><li>Configure your Core ML model in a particular way. You can either append NMS to your model or write a lot of additional Swift code. IMHO, this is the most difficult part if you know nothing about the object detection model.</li><li>Specify camera resolution, don&#x2019;t simply select the highest resolution available if your app doesn&#x2019;t require it.</li><li>Resize or crop your input image to fit network input dimension, it depends on your application.</li><li>Feed modified images to your model in a correct orientation.</li><li>Fix Vision&#x2019;s weird orin.</li><li>Convert bounding boxes coordinate system for display. This is also a trickier part, you need some iOS development experiences and a pencil for calculation &#x1F60E;.</li></ul><p>According to Hollemans&#x2019;s article, there are at least 5 different coordinate systems you need to take care, not to mention how to handle real-time capturing correctly and efficiently is also non-trivial. You can follow these two articles to learn how to create a custom camera view.</p><!--kg-card-begin: html--><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://developer.apple.com/documentation/vision/recognizing_objects_in_live_capture?ref=localhost"><div class="kg-bookmark-content"><div class="kg-bookmark-title">Apple Developer Documentation | Recognizing Objects in Live Capture</div><div class="kg-bookmark-description"></div><div class="kg-bookmark-metadata"></div></div></a></figure><!--kg-card-end: html--><!--kg-card-begin: html--><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://guides.codepath.com/ios/Creating-a-Custom-Camera-View?ref=localhost"><div class="kg-bookmark-content"><div class="kg-bookmark-title">Creating a Custom Camera View | CodePath iOS Cliffnotes</div><div class="kg-bookmark-description">Accessing the Built in Image Picker Controller is a quick and easy way to get image and video capture into your app&#x2026;</div><div class="kg-bookmark-metadata"></div></div></a></figure><!--kg-card-end: html--><hr><p>At the latest <a href="https://developer.apple.com/videos/play/wwdc2022/10027/?ref=localhost" rel="noopener">WWDC 2022</a>, Apple introduced even more performance tools to its CoreML toolchain, now you can check your model&#x2019;s metadata via <strong>performance reports</strong> and <strong>Core ML Instrument </strong>without writing any code. You can also use <code>computeUnits = .cpuAndNeuralEngine</code> if you don&#x2019;t want to use the GPU but always force the model to run on the CPU and ANE if available.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://blog.cmwang.net/content/images/max/800/1-pmnazzvfrotjl62lsin-cq.png" class="kg-image" alt="Execute YOLOv7 model on iOS devices" loading="lazy" width="2000" height="784" srcset="https://blog.cmwang.net/content/images/size/w600/max/800/1-pmnazzvfrotjl62lsin-cq.png 600w, https://blog.cmwang.net/content/images/size/w1000/max/800/1-pmnazzvfrotjl62lsin-cq.png 1000w, https://blog.cmwang.net/content/images/size/w1600/max/800/1-pmnazzvfrotjl62lsin-cq.png 1600w, https://blog.cmwang.net/content/images/size/w2400/max/800/1-pmnazzvfrotjl62lsin-cq.png 2400w" sizes="(min-width: 720px) 720px"><figcaption>Prefer CPU and ANE instead of&#xA0;GPU.</figcaption></figure><p>You can learn more about ANE from the following repository, thank you again Hollemans.</p><!--kg-card-begin: html--><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://github.com/hollance/neural-engine?ref=localhost"><div class="kg-bookmark-content"><div class="kg-bookmark-title">GitHub - hollance/neural-engine: Everything we actually know about the Apple Neural Engine (ANE)</div><div class="kg-bookmark-description">Most new iPhones and iPads have a Neural Engine, a special processor that makes machine learning models really fast&#x2026;</div><div class="kg-bookmark-metadata"></div></div><div class="kg-bookmark-thumbnail"><img src="https://blog.cmwang.net/content/images/fit/c/160/160/0-bim0fuyf26vxrxj3.png" alt="Execute YOLOv7 model on iOS devices"></div></a></figure><!--kg-card-end: html--><p>Here are snapshots from my model&#x2019;s performance reports.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://blog.cmwang.net/content/images/max/800/1-chrnkwsmronwdmxy5gzk5g.png" class="kg-image" alt="Execute YOLOv7 model on iOS devices" loading="lazy" width="999" height="924" srcset="https://blog.cmwang.net/content/images/size/w600/max/800/1-chrnkwsmronwdmxy5gzk5g.png 600w, https://blog.cmwang.net/content/images/max/800/1-chrnkwsmronwdmxy5gzk5g.png 999w" sizes="(min-width: 720px) 720px"><figcaption>You can evaluate your model via drag-and-drop image&#xA0;files.</figcaption></figure><p>There is no significant inference speed differences among quantization models, but the model size only about half the size. It&#x2019;s a good thing for your mobile applications.</p><figure class="kg-card kg-gallery-card kg-width-wide kg-card-hascaption"><div class="kg-gallery-container"><div class="kg-gallery-row"><div class="kg-gallery-image"><img src="https://blog.cmwang.net/content/images/max/600/1-8nh4sx9qus2s6vjpdl8kdw.png" width="761" height="376" loading="lazy" alt="Execute YOLOv7 model on iOS devices" srcset="https://blog.cmwang.net/content/images/size/w600/max/600/1-8nh4sx9qus2s6vjpdl8kdw.png 600w, https://blog.cmwang.net/content/images/max/600/1-8nh4sx9qus2s6vjpdl8kdw.png 761w" sizes="(min-width: 720px) 720px"></div><div class="kg-gallery-image"><img src="https://blog.cmwang.net/content/images/max/600/1-b1dmp5fanjnpxlmdx6ovfg.png" width="757" height="374" loading="lazy" alt="Execute YOLOv7 model on iOS devices" srcset="https://blog.cmwang.net/content/images/size/w600/max/600/1-b1dmp5fanjnpxlmdx6ovfg.png 600w, https://blog.cmwang.net/content/images/max/600/1-b1dmp5fanjnpxlmdx6ovfg.png 757w" sizes="(min-width: 720px) 720px"></div></div></div><figcaption>No inference speed improved. (Left is FP32, right is&#xA0;FP16)</figcaption></figure><figure class="kg-card kg-gallery-card kg-width-wide kg-card-hascaption"><div class="kg-gallery-container"><div class="kg-gallery-row"><div class="kg-gallery-image"><img src="https://blog.cmwang.net/content/images/max/600/1-t-t3lxffhjluzrfmcjyrcq.png" width="927" height="724" loading="lazy" alt="Execute YOLOv7 model on iOS devices" srcset="https://blog.cmwang.net/content/images/size/w600/max/600/1-t-t3lxffhjluzrfmcjyrcq.png 600w, https://blog.cmwang.net/content/images/max/600/1-t-t3lxffhjluzrfmcjyrcq.png 927w" sizes="(min-width: 720px) 720px"></div><div class="kg-gallery-image"><img src="https://blog.cmwang.net/content/images/max/600/1-xqsp3itb9mi-ozpyndna9a.png" width="929" height="718" loading="lazy" alt="Execute YOLOv7 model on iOS devices" srcset="https://blog.cmwang.net/content/images/size/w600/max/600/1-xqsp3itb9mi-ozpyndna9a.png 600w, https://blog.cmwang.net/content/images/max/600/1-xqsp3itb9mi-ozpyndna9a.png 929w" sizes="(min-width: 720px) 720px"></div></div></div><figcaption>Half the size of the FP32&#xA0;model.</figcaption></figure><p>Finally, you have a working YOLOv7 Core ML model on the iOS devices, be careful of the heat&#x1F525;. Happy coding!</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://blog.cmwang.net/content/images/max/800/1-f23rqz-9nanzq4n65ymkda.gif" class="kg-image" alt="Execute YOLOv7 model on iOS devices" loading="lazy" width="800" height="449" srcset="https://blog.cmwang.net/content/images/size/w600/max/800/1-f23rqz-9nanzq4n65ymkda.gif 600w, https://blog.cmwang.net/content/images/max/800/1-f23rqz-9nanzq4n65ymkda.gif 800w" sizes="(min-width: 720px) 720px"><figcaption><strong class="markup--strong markup--figure-strong">Yolov7-tiny on iPad Mini 6.</strong> Copyrights of <a href="https://youtu.be/hngml3y2Rq8?ref=localhost" data-href="https://youtu.be/hngml3y2Rq8" class="markup--anchor markup--figure-anchor" rel="noopener" target="_blank">BBIBBI Dance Practice</a> belongs Kakao Entertainment.</figcaption></figure><hr><p>Any opinions expressed are solely my own and do not express the views or opinions of my employer.</p><h3 id="references">References</h3><ul><li><a href="https://developer.apple.com/documentation/vision/recognizing_objects_in_live_capture?ref=localhost" rel="noopener">Recognizing Objects in Live Capture</a></li><li><a href="https://machinethink.net/blog/bounding-boxes/?ref=localhost" rel="noopener">How to display Vision bounding boxes</a></li><li><a href="https://guides.codepath.com/ios/Creating-a-Custom-Camera-View?ref=localhost" rel="noopener">Creating a Custom Camera View</a></li><li><a href="https://github.com/hollance/neural-engine?ref=localhost" rel="noopener">The Neural Engine&#x200A;&#x2014;&#x200A;what do we know about it?</a></li><li><a href="https://github.com/WongKinYiu/yolov7?ref=localhost" rel="noopener">WongKinYiu/yolov7</a></li><li><a href="https://developer.apple.com/videos/play/wwdc2022/10027/?ref=localhost" rel="noopener">WWDC2022&#x200A;&#x2014;&#x200A;Optimize your Core ML usage</a></li><li><a href="https://machinethink.net/blog/mobilenet-ssdlite-coreml/?ref=localhost" rel="noopener">MobileNetV2 + SSDLite with Core ML</a></li><li><a href="https://github.com/dbsystel/yolov5-coreml-tools?ref=localhost" rel="noopener">yolov5&#x200A;&#x2014;&#x200A;CoreML Tools</a></li></ul>]]></content:encoded></item><item><title><![CDATA[Performance Benchmarking of YOLOv7 TensorRT from Cloud GPUs to Edge GPUs]]></title><description><![CDATA[We will conduct a series performance test of YOLOv7 variants models from cloud GPUs to the latest powerhouse AGX Orin in this post.]]></description><link>https://blog.cmwang.net/performance-benchmarking-of-yolov7-tensorrt-from-cloud-gpus-to-edge-gpus/</link><guid isPermaLink="false">65ba7277b102c4db988584dc</guid><category><![CDATA[Agx Orin]]></category><category><![CDATA[Yolov7]]></category><category><![CDATA[Computer Vision]]></category><category><![CDATA[Object Detection]]></category><category><![CDATA[Artificial Intelligence]]></category><dc:creator><![CDATA[Taka Wang]]></dc:creator><pubDate>Mon, 25 Jul 2022 10:30:00 GMT</pubDate><media:content url="https://blog.cmwang.net/content/images/max/800/1-uilai7xjc-5scp4rozd8-w.gif" medium="image"/><content:encoded><![CDATA[<img src="https://blog.cmwang.net/content/images/max/800/1-uilai7xjc-5scp4rozd8-w.gif" alt="Performance Benchmarking of YOLOv7 TensorRT from Cloud GPUs to Edge GPUs"><p>Object detection is one of the fundamental problems of computer vision. Instead of region detection and object classification separately in two stage detectors, object classification and bounding-box regression are done directly without using pre-generated region proposals in one stage detectors. YOLO (You Only Look Once) is one of the representative models of one-stage architecture. The YOLO family has continued to evolve since 2016, this summer we&#x2019;ve got its latest update to version 7.</p><!--kg-card-begin: html--><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://github.com/WongKinYiu/yolov7?ref=localhost"><div class="kg-bookmark-content"><div class="kg-bookmark-title">GitHub - WongKinYiu/yolov7: Implementation of paper - YOLOv7: Trainable bag-of-freebies sets new&#x2026;</div><div class="kg-bookmark-description">Implementation of paper - YOLOv7: Trainable bag-of-freebies sets new state-of-the-art for real-time object detectors MS&#x2026;</div><div class="kg-bookmark-metadata"></div></div><div class="kg-bookmark-thumbnail"><img src="https://blog.cmwang.net/content/images/fit/c/160/160/0-pex7bashe7svci5r.png" alt="Performance Benchmarking of YOLOv7 TensorRT from Cloud GPUs to Edge GPUs"></div></a></figure><!--kg-card-end: html--><p>If you are trying to learn how to train your model on a custom dataset from the beginning, there are already many tutorials, notebooks and videos available online. In <a href="https://nilvana.ai/?ref=localhost" rel="noopener">Nilvana</a>, we really care about its real-world performance on the embedded devices, especially Nvidia Jetson family devices. So we conducted a series performance testing of YOLOv7 variants models on different devices, from cloud GPUs A100 to the latest tiny powerhouse<a href="https://developer.nvidia.com/embedded/jetson-orin?ref=localhost" rel="noopener"> AGX Orin</a>.</p><!--kg-card-begin: html--><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://www.seeedstudio.com/NVIDIA-Jetson-AGX-Orin-Developer-Kit-p-5314.html?ref=localhost"><div class="kg-bookmark-content"><div class="kg-bookmark-title">NVIDIA&#xAE; Jetson AGX Orin&#x2122; Developer Kit: smallest and most powerful AI edge computer</div><div class="kg-bookmark-description">NVIDIA&#xAE; Jetson AGX Orin&#x2122; Developer Kit, provides a giant leap forward for Robotics and Edge AI. With up to 275 TOPS of&#x2026;</div><div class="kg-bookmark-metadata"></div></div><div class="kg-bookmark-thumbnail"><img src="https://blog.cmwang.net/content/images/fit/c/160/160/0-ze7j0-x469op2ttq.jpg" alt="Performance Benchmarking of YOLOv7 TensorRT from Cloud GPUs to Edge GPUs"></div></a></figure><!--kg-card-end: html--><blockquote>The main reason YOLOv7 is more accurate, compare to other models with similar AP, YOLOv7 has only about half computational cost.&#x200A;&#x2014;&#x200A;<a href="https://github.com/WongKinYiu/yolov7/issues/207?ref=localhost#issuecomment-1186934294" rel="noopener">WongKinYiu</a></blockquote><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://blog.cmwang.net/content/images/max/800/1-egbwzwax-kjvas4om68ypa.png" class="kg-image" alt="Performance Benchmarking of YOLOv7 TensorRT from Cloud GPUs to Edge GPUs" loading="lazy" width="488" height="197"><figcaption>Input and Output shape of YOLOv7 (80&#xA0;class)</figcaption></figure><p>According to the results table, Xavier NX can run YOLOv7-tiny model pretty well. AGX Orin can even run YOLOv7x model more than 30 FPS, it&#x2019;s amazing!</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://blog.cmwang.net/content/images/max/800/1-qkotq0kxs9ewjblqz2h9ga.png" class="kg-image" alt="Performance Benchmarking of YOLOv7 TensorRT from Cloud GPUs to Edge GPUs" loading="lazy" width="755" height="279" srcset="https://blog.cmwang.net/content/images/size/w600/max/800/1-qkotq0kxs9ewjblqz2h9ga.png 600w, https://blog.cmwang.net/content/images/max/800/1-qkotq0kxs9ewjblqz2h9ga.png 755w" sizes="(min-width: 720px) 720px"><figcaption>End-to-End Performance on 1080P video, Batch&#xA0;Size=1</figcaption></figure><figure class="kg-card kg-embed-card kg-card-hascaption"><iframe src="https://www.youtube.com/embed/videoseries?list=PL3ZyjLA-y4kHCeKiUT9HsYNheLKsuVuqj" width="700" height="394" frameborder="0" scrolling="no"></iframe><figcaption>Performance Benchmarking Playlist</figcaption></figure><!--kg-card-begin: html--><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://nilvana.ai/?ref=localhost"><div class="kg-bookmark-content"><div class="kg-bookmark-title">Hello nilvana&#x2122;, Hello Tomorrow</div><div class="kg-bookmark-description">AI Workstation, Jetson Nano, Xavier, Nvidia, Edge Computing, IoT, Edge Computing, &#x4EBA;&#x5DE5;&#x667A;&#x6167;&#xFF0C;&#x5DE5;&#x4F5C;&#x7AD9;&#xFF0C;&#x7269;&#x806F;&#x7DB2;&#xFF0C;&#x908A;&#x7DE3;&#x904B;&#x7B97;&#xFF0C;&#x908A;&#x7DE3;&#x88DD;&#x7F6E;&#x3002;</div><div class="kg-bookmark-metadata"></div></div><div class="kg-bookmark-thumbnail"><img src="https://blog.cmwang.net/content/images/fit/c/160/160/0-jdybrwmwowhvmnps.jpg" alt="Performance Benchmarking of YOLOv7 TensorRT from Cloud GPUs to Edge GPUs"></div></a></figure><!--kg-card-end: html-->]]></content:encoded></item><item><title><![CDATA[Building an Object Detection App quickly with Streamlit]]></title><description><![CDATA[With Streamlit, you can quickly integrate Nilvana Vision Inference Runtime to develop your own object detection Web App]]></description><link>https://blog.cmwang.net/e9-80-8f-e9-81-8e-streamlit-e5-bf-ab-e9-80-9f-e5-bb-ba-e7-ab-8b-e7-89-a9-e9-ab-94-e5-81-b5-e6-b8-ac-web-app/</link><guid isPermaLink="false">65ba7277b102c4db988584ed</guid><category><![CDATA[Computer Vision]]></category><category><![CDATA[Artificial Intelligence]]></category><category><![CDATA[AI]]></category><dc:creator><![CDATA[Taka Wang]]></dc:creator><pubDate>Mon, 25 Apr 2022 15:25:22 GMT</pubDate><media:content url="https://blog.cmwang.net/content/images/max/800/1-_uc9oywq_buu-1kpjnmvlw.png" medium="image"/><content:encoded><![CDATA[<h3 id="building-an-object-detection-web-app-quickly-with-streamlit">Building an Object Detection Web App quickly with Streamlit</h3><img src="https://blog.cmwang.net/content/images/max/800/1-_uc9oywq_buu-1kpjnmvlw.png" alt="Building an Object Detection App quickly with Streamlit"><p>Sometimes, you may want to quickly create a graphical interface for others to test your model, but you don&#x2019;t want to deal with front-end technologies. This is where <a href="https://streamlit.io/?ref=localhost" rel="noopener">Streamlit</a> comes in, allowing you to quickly create a web application using Python syntax.</p><p>In this article, I will show you how to use <a href="https://streamlit.io/?ref=localhost" rel="noopener">Streamlit</a> and <a href="https://nilvana.tw/products/nilvana-vision-inference-runtime?ref=localhost" rel="noopener">Nilvana Vision Inference Runtime</a> to quickly deploy a web application that can detect masks. If you are not familiar with <a href="https://nilvana.tw/products/nilvana-vision-inference-runtime?ref=localhost" rel="noopener">Nilvana Vision Inference Runtime</a>, it is a software package we developed for node-code deployment of AI models, which you can refer to in our previous articles.</p><!--kg-card-begin: html--><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://medium.com/hello-nilvana/%E9%80%8F%E9%81%8E-nilvana-vision-inference-runtime-%E9%83%A8%E7%BD%B2%E6%A8%A1%E5%9E%8B-a52b05a63b74?ref=localhost"><div class="kg-bookmark-content"><div class="kg-bookmark-title">Deploying models through Nilvana Vision Inference Runtime</div><div class="kg-bookmark-description">Using Nilvana Vision Inference Runtime (inference engine), you can easily deploy your models to x86 or Jetson&#x2026;</div><div class="kg-bookmark-metadata"></div></div><div class="kg-bookmark-thumbnail"><img src="https://blog.cmwang.net/content/images/fit/c/160/160/1-u6xc7wr939zohyws2_7okg.png" alt="Building an Object Detection App quickly with Streamlit"></div></a></figure><!--kg-card-end: html--><p>The completed result is shown in the following figure:</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://blog.cmwang.net/content/images/max/800/1-fk7w7zaaph74duaps6_kla.gif" class="kg-image" alt="Building an Object Detection App quickly with Streamlit" loading="lazy" width="800" height="453" srcset="https://blog.cmwang.net/content/images/size/w600/max/800/1-fk7w7zaaph74duaps6_kla.gif 600w, https://blog.cmwang.net/content/images/max/800/1-fk7w7zaaph74duaps6_kla.gif 800w" sizes="(min-width: 720px) 720px"><figcaption>Video by cottonbro from&#xA0;Pexels:</figcaption></figure><p>You only need to install Streamlit by running <code>pip install streamlit</code>. Then, you can use the <code>st</code> object to generate the sidebar, and the entire process takes less than three lines of code.import streamlit as sturl = st.sidebar.text_input(&apos;NVIR Endpoint URL&apos;, &apos;http://192.168.41.200:52010/v1/infer/111111111&apos;)st.sidebar.write(&apos;#### Select a video to upload.&apos;)uploaded_video = st.sidebar.file_uploader(&quot;Choose video&quot;, type=[&quot;mp4&quot;])</p><p>The main screen is also quite simple. Declare two containers created by <code><strong>st.empty() </strong></code>to hold the video and JSON output, respectively. This step fixes the <strong>frame</strong> and <strong>JSON output</strong> positions. Open the video file through the main loop of OpenCV and obtain the inference result using the Restful API provided by <a href="https://nilvana.tw/products/nilvana-vision-inference-runtime?ref=localhost" rel="noopener">Nilvana Vision Inference Runtime</a>. Then, overlay the inference result on the frame. This part is similar to the content of the previous article. In other words, you can easily convert OpenCV applications into Streamlit web applications.frame_holder = st.empty()<br>placeholder = st.empty()# ... opencv partsframe_holder.image(frame) # put your frame herewith placeholder.container():<br> &#xA0;# Display the JSON in main window.<br> &#xA0;st.write(&apos;### JSON Output&apos;)<br> &#xA0;st.write(result)</p><p>The entire application can be completed in less than 60 lines of code, and we highly recommend that Python developers further explore this. The complete example code is also available on our GitHub page at the end of the article. Happy Deployment!</p><!--kg-card-begin: html--><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://docs.streamlit.io/?ref=localhost"><div class="kg-bookmark-content"><div class="kg-bookmark-title">Streamlit Docs</div><div class="kg-bookmark-description">Streamlit is more than just a way to make data apps, it&apos;s also a community of creators that share their apps and ideas&#x2026;</div><div class="kg-bookmark-metadata"></div></div><div class="kg-bookmark-thumbnail"><img src="https://blog.cmwang.net/content/images/fit/c/160/160/0-od3ubed0y3nqqemw.jpg" alt="Building an Object Detection App quickly with Streamlit"></div></a></figure><!--kg-card-end: html--><!--kg-card-begin: html--><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://github.com/nilvana-ai/nvir-streamlit-demo?ref=localhost"><div class="kg-bookmark-content"><div class="kg-bookmark-title">GitHub - nilvana-ai/nvir-streamlit-demo: This is a demo repo to demonstrate nilvana vision&#x2026;</div><div class="kg-bookmark-description">This is a demo repo to demonstrate nilvana vision inference runtime with streamlit. </div><div class="kg-bookmark-metadata"></div></div><div class="kg-bookmark-thumbnail"><img src="https://blog.cmwang.net/content/images/fit/c/160/160/0-8tnv3cnpvuvsfnkf.png" alt="Building an Object Detection App quickly with Streamlit"></div></a></figure><!--kg-card-end: html--><!--kg-card-begin: html--><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://nilvana.ai/?ref=localhost"><div class="kg-bookmark-content"><div class="kg-bookmark-title">Hello nilvana&#x2122;, Hello Tomorrow</div><div class="kg-bookmark-description">AI Workstation, Jetson Nano, Xavier, Nvidia, Edge Computing, IoT, Edge Computing, &#x4EBA;&#x5DE5;&#x667A;&#x6167;&#xFF0C;&#x5DE5;&#x4F5C;&#x7AD9;&#xFF0C;&#x7269;&#x806F;&#x7DB2;&#xFF0C;&#x908A;&#x7DE3;&#x904B;&#x7B97;&#xFF0C;&#x908A;&#x7DE3;&#x88DD;&#x7F6E;&#x3002;</div><div class="kg-bookmark-metadata"></div></div><div class="kg-bookmark-thumbnail"><img src="https://blog.cmwang.net/content/images/fit/c/160/160/0-gegawc126g4zni92.jpg" alt="Building an Object Detection App quickly with Streamlit"></div></a></figure><!--kg-card-end: html-->]]></content:encoded></item><item><title><![CDATA[Deploying models through Nilvana Vision Inference Runtime]]></title><description><![CDATA[Using Nilvana Vision Inference Runtime (inference engine), you can easily deploy your models to x86 or Jetson platforms.]]></description><link>https://blog.cmwang.net/e9-80-8f-e9-81-8e-nilvana-vision-inference-runtime-e9-83-a8-e7-bd-b2-e6-a8-a1-e5-9e-8b/</link><guid isPermaLink="false">65ba7277b102c4db988584ee</guid><category><![CDATA[AI]]></category><category><![CDATA[Artificail Intelligence]]></category><category><![CDATA[Computer Vision]]></category><dc:creator><![CDATA[Taka Wang]]></dc:creator><pubDate>Mon, 25 Apr 2022 10:59:01 GMT</pubDate><media:content url="https://blog.cmwang.net/content/images/max/800/1-u6xc7wr939zohyws2_7okg.png" medium="image"/><content:encoded><![CDATA[<h3 id="deploying-models-through-nilvana-vision-inference-runtime">Deploying Models through Nilvana Vision Inference Runtime</h3><img src="https://blog.cmwang.net/content/images/max/800/1-u6xc7wr939zohyws2_7okg.png" alt="Deploying models through Nilvana Vision Inference Runtime"><p><a href="https://nilvana.tw/products/nilvana-vision-inference-runtime?ref=localhost" rel="noopener">Nilvana Vision Inference Runtime</a> is a software toolkit we have developed for deploying models on different platforms. You can easily deploy models produced by <a href="https://nilvana.tw/products/nilvana%E2%84%A2-vision-studio?ref=localhost" rel="noopener">Nilvana Vision Studio</a> to x86 or Nvidia Jetson platforms through this toolkit. Below, I will demonstrate how to easily create an Endpoint and complete a toy example using less than 10 lines of Python code.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://blog.cmwang.net/content/images/max/800/1-asbi29m4wxwozhpgd1329w.png" class="kg-image" alt="Deploying models through Nilvana Vision Inference Runtime" loading="lazy" width="2000" height="1103" srcset="https://blog.cmwang.net/content/images/size/w600/max/800/1-asbi29m4wxwozhpgd1329w.png 600w, https://blog.cmwang.net/content/images/size/w1000/max/800/1-asbi29m4wxwozhpgd1329w.png 1000w, https://blog.cmwang.net/content/images/size/w1600/max/800/1-asbi29m4wxwozhpgd1329w.png 1600w, https://blog.cmwang.net/content/images/size/w2400/max/800/1-asbi29m4wxwozhpgd1329w.png 2400w" sizes="(min-width: 720px) 720px"><figcaption>After registering for an account, this is the login screen you will see when you open the&#xA0;package.</figcaption></figure><p>After logging in with your account and password, you can quickly create an Endpoint through the &#x201C;Quick Operation&#x201D; option.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://blog.cmwang.net/content/images/max/800/1-9_7sn3hmf7ptuurufxymua.png" class="kg-image" alt="Deploying models through Nilvana Vision Inference Runtime" loading="lazy" width="2000" height="1441" srcset="https://blog.cmwang.net/content/images/size/w600/max/800/1-9_7sn3hmf7ptuurufxymua.png 600w, https://blog.cmwang.net/content/images/size/w1000/max/800/1-9_7sn3hmf7ptuurufxymua.png 1000w, https://blog.cmwang.net/content/images/size/w1600/max/800/1-9_7sn3hmf7ptuurufxymua.png 1600w, https://blog.cmwang.net/content/images/max/800/1-9_7sn3hmf7ptuurufxymua.png 2082w" sizes="(min-width: 720px) 720px"><figcaption>Select &#x201C;Create Endpoint&#x201D;</figcaption></figure><p>At this point, name your model and upload the compressed ONNX file exported from Nilvana Vision Studio. Currently, we support three types of problems: Object Detection, Image Classification and Segmentation.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://blog.cmwang.net/content/images/max/800/1-xvz4aywqam29h4abud36-q.png" class="kg-image" alt="Deploying models through Nilvana Vision Inference Runtime" loading="lazy" width="2000" height="1157" srcset="https://blog.cmwang.net/content/images/size/w600/max/800/1-xvz4aywqam29h4abud36-q.png 600w, https://blog.cmwang.net/content/images/size/w1000/max/800/1-xvz4aywqam29h4abud36-q.png 1000w, https://blog.cmwang.net/content/images/size/w1600/max/800/1-xvz4aywqam29h4abud36-q.png 1600w, https://blog.cmwang.net/content/images/max/800/1-xvz4aywqam29h4abud36-q.png 2316w" sizes="(min-width: 720px) 720px"><figcaption>Name the model and upload the compressed model&#xA0;file.</figcaption></figure><p>Next, you can choose the minimum threshold and IoU (Intersection over Union) expected for the Endpoint. These values can be adjusted later. If your model has multiple classes, you can also choose to predict only some of them.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://blog.cmwang.net/content/images/max/800/1-j8doonsb3w60gjdecaebfw.png" class="kg-image" alt="Deploying models through Nilvana Vision Inference Runtime" loading="lazy" width="2000" height="999" srcset="https://blog.cmwang.net/content/images/size/w600/max/800/1-j8doonsb3w60gjdecaebfw.png 600w, https://blog.cmwang.net/content/images/size/w1000/max/800/1-j8doonsb3w60gjdecaebfw.png 1000w, https://blog.cmwang.net/content/images/size/w1600/max/800/1-j8doonsb3w60gjdecaebfw.png 1600w, https://blog.cmwang.net/content/images/size/w2400/max/800/1-j8doonsb3w60gjdecaebfw.png 2400w" sizes="(min-width: 720px) 720px"><figcaption>Set the threshold and IoU threshold values, which can be fine-tuned later.</figcaption></figure><p>Finally, you can customize a unique identification code with a length of 9 digits, or have the system generate one for you. <strong>Each independent Endpoint uses a unique token</strong>.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://blog.cmwang.net/content/images/max/800/1-i51t_a0tvo54mssjc7avla.png" class="kg-image" alt="Deploying models through Nilvana Vision Inference Runtime" loading="lazy" width="2000" height="1041" srcset="https://blog.cmwang.net/content/images/size/w600/max/800/1-i51t_a0tvo54mssjc7avla.png 600w, https://blog.cmwang.net/content/images/size/w1000/max/800/1-i51t_a0tvo54mssjc7avla.png 1000w, https://blog.cmwang.net/content/images/size/w1600/max/800/1-i51t_a0tvo54mssjc7avla.png 1600w, https://blog.cmwang.net/content/images/size/w2400/max/800/1-i51t_a0tvo54mssjc7avla.png 2400w" sizes="(min-width: 720px) 720px"><figcaption>You can define the token yourself or have the system generate it. The token cannot be changed&#xA0;later.</figcaption></figure><p>At this point, the system has begun optimizing the model. We perform different optimization actions for different systems. <strong>You only need to wait a few minutes for the Endpoint to become available</strong>.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://blog.cmwang.net/content/images/max/800/1-jqmgxaebesqgoewoboyz9q.png" class="kg-image" alt="Deploying models through Nilvana Vision Inference Runtime" loading="lazy" width="1270" height="1028" srcset="https://blog.cmwang.net/content/images/size/w600/max/800/1-jqmgxaebesqgoewoboyz9q.png 600w, https://blog.cmwang.net/content/images/size/w1000/max/800/1-jqmgxaebesqgoewoboyz9q.png 1000w, https://blog.cmwang.net/content/images/max/800/1-jqmgxaebesqgoewoboyz9q.png 1270w" sizes="(min-width: 720px) 720px"><figcaption>The optimization time varies from 5 to 20 minutes depending on the hardware.</figcaption></figure><p>When the model optimization is complete, you need to activate this Endpoint. <strong>If you plan to fine-tune the threshold of the Endpoint, you need to deactivate it first</strong>. We also provide you with real-time inference screens and example code below.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://blog.cmwang.net/content/images/max/800/1--fr9uckkys35zd4j4svptq.png" class="kg-image" alt="Deploying models through Nilvana Vision Inference Runtime" loading="lazy" width="1062" height="900" srcset="https://blog.cmwang.net/content/images/size/w600/max/800/1--fr9uckkys35zd4j4svptq.png 600w, https://blog.cmwang.net/content/images/size/w1000/max/800/1--fr9uckkys35zd4j4svptq.png 1000w, https://blog.cmwang.net/content/images/max/800/1--fr9uckkys35zd4j4svptq.png 1062w" sizes="(min-width: 720px) 720px"><figcaption>Endpoint needs to be activated before it can operate properly.</figcaption></figure><p>At this point, the Endpoint is ready and you can send an HTTP Post Request with your familiar development language. The Endpoint will return the results in JSON format. Below is an example return with two possible objects.{<br> &apos;result&apos;: [{<br> &#xA0;&apos;xmin&apos;: 656.40967,<br> &#xA0;&apos;ymin&apos;: 427.88263,<br> &#xA0;&apos;width&apos;: 172,<br> &#xA0;&apos;height&apos;: 214,<br> &#xA0;&apos;label&apos;: &apos;mask&apos;,<br> &#xA0;&apos;confidence&apos;: 0.99844<br> }, {<br> &#xA0;&apos;xmin&apos;: 390.02213,<br> &#xA0;&apos;ymin&apos;: 724.0925,<br> &#xA0;&apos;width&apos;: 187,<br> &#xA0;&apos;height&apos;: 224,<br> &#xA0;&apos;label&apos;: &apos;face&apos;,<br> &#xA0;&apos;confidence&apos;: 0.99813914<br> }]<br>}</p><p>With the Python <a href="https://docs.python-requests.org/en/latest/?ref=localhost" rel="noopener">Request</a>s package, you can easily perform secondary development without writing complicated code, as shown in the following example:import requests<br>import jsonurl = &apos;http://192.168.41.200:52010/v1/infer/111111111&apos; # change me# inference<br>files = {&apos;<strong>image</strong>&apos;: open(&apos;assets/demo.jpg&apos;, &apos;rb&apos;)}<br>response = requests.post(url, files=files)<br>result = json.loads(response.text)<br>print(result)</p><p>Using the OpenCV package, you can also easily overlay the inference result on the image. I will provide a complete demonstration at the end of the article through the GitHub link.</p><p>The process of handling video streaming is also straightforward and consists of the following six steps. Repeat steps 2 to 6 until the video stream ends. Happy coding!1. Open image or stream<br>2. Capture Frame<br>3. Resize Frame<br>4. Perform inference via inference package (HTTP Post)<br>5. Process JSON output and draw boxes<br>6. Display Frame</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://blog.cmwang.net/content/images/max/800/1-x4vvphqmyj3japb3hrf5eq.gif" class="kg-image" alt="Deploying models through Nilvana Vision Inference Runtime" loading="lazy" width="800" height="419" srcset="https://blog.cmwang.net/content/images/size/w600/max/800/1-x4vvphqmyj3japb3hrf5eq.gif 600w, https://blog.cmwang.net/content/images/max/800/1-x4vvphqmyj3japb3hrf5eq.gif 800w" sizes="(min-width: 720px) 720px"><figcaption>Video by cottonbro from Pexels: <a href="https://www.pexels.com/video/woman-art-iphone-smartphone-3960181/?ref=localhost" data-href="https://www.pexels.com/video/woman-art-iphone-smartphone-3960181/" class="markup--anchor markup--figure-anchor" rel="nofollow noopener" target="_blank">https://www.pexels.com/video/woman-art-iphone-smartphone-3960181/</a></figcaption></figure><!--kg-card-begin: html--><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://github.com/nilvana-taka/nvir-mask-demo?ref=localhost"><div class="kg-bookmark-content"><div class="kg-bookmark-title">GitHub - nilvana-ai/nvir-mask-demo: This is a demo repo to demonstrate nilvana vision inference&#x2026;</div><div class="kg-bookmark-description">This is a demo repo to demonstrate nilvana vision inference runtime. </div><div class="kg-bookmark-metadata"></div></div><div class="kg-bookmark-thumbnail"><img src="https://blog.cmwang.net/content/images/fit/c/160/160/0-4rrzs0rllf0exjnd.png" alt="Deploying models through Nilvana Vision Inference Runtime"></div></a></figure><!--kg-card-end: html--><!--kg-card-begin: html--><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://nilvana.ai/?ref=localhost"><div class="kg-bookmark-content"><div class="kg-bookmark-title">Hello nilvana&#x2122;, Hello Tomorrow</div><div class="kg-bookmark-description">AI Workstation, Jetson Nano, Xavier, Nvidia, Edge Computing, IoT, Edge Computing, &#x4EBA;&#x5DE5;&#x667A;&#x6167;&#xFF0C;&#x5DE5;&#x4F5C;&#x7AD9;&#xFF0C;&#x7269;&#x806F;&#x7DB2;&#xFF0C;&#x908A;&#x7DE3;&#x904B;&#x7B97;&#xFF0C;&#x908A;&#x7DE3;&#x88DD;&#x7F6E;&#x3002;</div><div class="kg-bookmark-metadata"></div></div><div class="kg-bookmark-thumbnail"><img src="https://blog.cmwang.net/content/images/fit/c/160/160/0-gegawc126g4zni92.jpg" alt="Deploying models through Nilvana Vision Inference Runtime"></div></a></figure><!--kg-card-end: html-->]]></content:encoded></item></channel></rss>