<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Execute YOLOv7 model on iOS devices | Liberation Notes</title><meta name=keywords content="CoreML,iOS,Computer Vision,Object Detection,Yolov7"><meta name=description content="Real-time object detection with CoreML is trickier than you think."><meta name=author content="Taka"><link rel=canonical href=https://blog.cmwang.net/en/posts/2022/08/execute-yolov7-model-on-ios-devices/><link crossorigin=anonymous href=/assets/css/stylesheet.8b888087e4751d8a4f97764ced43219c7e3aa9c508bdcb5bf3fc5315f984cbbc.css integrity="sha256-i4iAh+R1HYpPl3ZM7UMhnH46qcUIvctb8/xTFfmEy7w=" rel="preload stylesheet" as=style><script defer crossorigin=anonymous src=/assets/js/highlight.f413e19d0714851f6474e7ee9632408e58ac146fbdbe62747134bea2fa3415e0.js integrity="sha256-9BPhnQcUhR9kdOfuljJAjlisFG+9vmJ0cTS+ovo0FeA=" onload=hljs.initHighlightingOnLoad()></script>
<link rel=icon href=https://blog.cmwang.net/assets/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://blog.cmwang.net/assets/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://blog.cmwang.net/assets/favicon-32x32.png><link rel=apple-touch-icon href=https://blog.cmwang.net/assets/apple-touch-icon.png><link rel=mask-icon href=https://blog.cmwang.net/assets/favicon.ico><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://blog.cmwang.net/en/posts/2022/08/execute-yolov7-model-on-ios-devices/><link rel=alternate hreflang=zh href=https://blog.cmwang.net/zh/posts/2022/08/execute-yolov7-model-on-ios-devices/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--hljs-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><script async src="https://www.googletagmanager.com/gtag/js?id=G-G9WYYRMSWH"></script>
<script>window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-G9WYYRMSWH")</script><script async src="https://www.googletagmanager.com/gtag/js?id=G-G9WYYRMSWH"></script>
<script>var doNotTrack=!1;if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-G9WYYRMSWH",{anonymize_ip:!1})}</script><meta property="og:title" content="Execute YOLOv7 model on iOS devices"><meta property="og:description" content="Real-time object detection with CoreML is trickier than you think."><meta property="og:type" content="article"><meta property="og:url" content="https://blog.cmwang.net/en/posts/2022/08/execute-yolov7-model-on-ios-devices/"><meta property="og:image" content="https://blog.cmwang.net/47"><meta property="article:section" content="posts"><meta property="article:published_time" content="2022-08-06T11:04:15+00:00"><meta property="article:modified_time" content="2023-10-09T21:22:40+08:00"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://blog.cmwang.net/47"><meta name=twitter:title content="Execute YOLOv7 model on iOS devices"><meta name=twitter:description content="Real-time object detection with CoreML is trickier than you think."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://blog.cmwang.net/en/posts/"},{"@type":"ListItem","position":2,"name":"Execute YOLOv7 model on iOS devices","item":"https://blog.cmwang.net/en/posts/2022/08/execute-yolov7-model-on-ios-devices/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Execute YOLOv7 model on iOS devices","name":"Execute YOLOv7 model on iOS devices","description":"Real-time object detection with CoreML is trickier than you think.","keywords":["CoreML","iOS","Computer Vision","Object Detection","Yolov7"],"articleBody":" Real-time object detection with CoreML is trickier than you think.\nUsually, you have two choices to build a machine learning app for your mobile device, inference can happen either directly on-device or on cloud-based servers. It all depends on your usage scenario, there is no one-size fit all solution. In this article, we will only focus on on-device inference.\nAt WWDC 2017 Apple released first Core ML. Core ML is Appleâ€™s machine learning framework for doing on-device inference. Core ML is not the only way to do on-device inference, there are tens of libraries and frameworks that are compatible with iOS, but thatâ€™s beyond the scope of this article. From the YOLOv7 official repository, we can get the export script to convert trained PyTorch model to Core ML format effortlessly. However, keep one thing in mind, YOLOv7 is a popular open source project, new changes and updates are added very quickly. Iâ€™m also very glad to send a PR to improve the export script last night due to this writing ðŸ˜ƒ.After you got the exported Core ML models, no kidding, you have tons of things in your todo list. Matthijs Hollemans has already written an insightful article in his blog, be sure to checkout and support his efforts! Here is my short list:\nConfigure your Core ML model in a particular way. You can either append NMS to your model or write a lot of additional Swift code. IMHO, this is the most difficult part if you know nothing about the object detection model. Specify camera resolution, donâ€™t simply select the highest resolution available if your app doesnâ€™t require it. Resize or crop your input image to fit network input dimension, it depends on your application. Feed modified images to your model in a correct orientation. Fix Visionâ€™s weird orin. Convert bounding boxes coordinate system for display. This is also a trickier part, you need some iOS development experiences and a pencil for calculation ðŸ˜Ž. According to Hollemansâ€™s article, there are at least 5 different coordinate systems you need to take care, not to mention how to handle real-time capturing correctly and efficiently is also non-trivial. You can follow these two articles to learn how to create a custom camera view.\nApple Developer Documentation | Recognizing Objects in Live Capture\nCreating a Custom Camera View | CodePath iOS Cliffnotes At the latest WWDC 2022, Apple introduced even more performance tools to its CoreML toolchain, now you can check your modelâ€™s metadata via performance reports and Core ML Instrument without writing any code. You can also use computeUnits = .cpuAndNeuralEngine if you donâ€™t want to use the GPU but always force the model to run on the CPU and ANE if available.\nPrefer CPU and ANE instead of GPU.\nYou can learn more about ANE from the following repository, thank you again Hollemans.\nGitHub - hollance/neural-engine: Everything we actually know about the Apple Neural Engine (ANE)\nHere are snapshots from my modelâ€™s performance reports.\nYou can evaluate your model via drag-and-drop image files.\nThere is no significant inference speed differences among quantization models, but the model size only about half the size. Itâ€™s a good thing for your mobile applications. No inference speed improved. (Left is FP32, right is FP16)\nHalf the size of the FP32 model.\nFinally, you have a working YOLOv7 Core ML model on the iOS devices, be careful of the heatðŸ”¥. Happy coding!\nYolov7-tiny on iPad Mini 6. Copyrights of BBIBBI Dance Practice belongs Kakao Entertainment.\nReferences Recognizing Objects in Live Capture How to display Vision bounding boxes Creating a Custom Camera View The Neural Engineâ€Šâ€”â€Šwhat do we know about it? WongKinYiu/yolov7 WWDC2022â€Šâ€”â€ŠOptimize your Core ML usage MobileNetV2 + SSDLite with Core ML yolov5â€Šâ€”â€ŠCoreML Tools ","wordCount":"695","inLanguage":"en","datePublished":"2022-08-06T11:04:15.84Z","dateModified":"2023-10-09T21:22:40+08:00","author":{"@type":"Person","name":"Taka"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://blog.cmwang.net/en/posts/2022/08/execute-yolov7-model-on-ios-devices/"},"publisher":{"@type":"Organization","name":"Liberation Notes","logo":{"@type":"ImageObject","url":"https://blog.cmwang.net/assets/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://blog.cmwang.net/en/ accesskey=h title="Liberation Notes (Alt + H)">Liberation Notes</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button><ul class=lang-switch><li>|</li><li><a href=https://blog.cmwang.net/zh/ title=ä¸­æ–‡ aria-label=ä¸­æ–‡>ä¸­æ–‡</a></li></ul></div></div><ul id=menu><li><a href=https://blog.cmwang.net/en/archives/ title=Archives><span>Archives</span></a></li><li><a href=https://blog.cmwang.net/en/categories/ title=Categories><span>Categories</span></a></li><li><a href=https://blog.cmwang.net/en/tags/ title=Tags><span>Tags</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><h1 class=post-title>Execute YOLOv7 model on iOS devices</h1><div class=post-meta><span title='2022-08-06 11:04:15.84 +0000 UTC'>2022-08-06</span>&nbsp;Â·&nbsp;695 words&nbsp;Â·&nbsp;Taka&nbsp;|&nbsp;Translations:<ul class=i18n_list><li><a href=https://blog.cmwang.net/zh/posts/2022/08/execute-yolov7-model-on-ios-devices/>ä¸­æ–‡</a></li></ul></div></header><div class=post-content><p><img loading=lazy src=images/1.jpeg alt=image>
Real-time object detection with CoreML is trickier than you think.</p><p>Usually, you have two choices to build a machine learning app for your mobile device, <strong>inference</strong> can happen either directly on-device or on cloud-based servers. It all depends on your usage scenario, there is no one-size fit all solution. In this article, we will only focus on on-device inference.</p><p>At WWDC 2017 Apple released first Core ML. Core ML is Appleâ€™s machine learning framework for doing on-device inference. Core ML is not the only way to do on-device inference, there are tens of libraries and frameworks that are compatible with iOS, but thatâ€™s beyond the scope of this article. From the <a href=https://github.com/WongKinYiu/yolov7>YOLOv7 official repository</a>, we can get the <a href=https://github.com/WongKinYiu/yolov7/blob/main/export.py>export script</a> to convert trained PyTorch model to Core ML format effortlessly. However, keep one thing in mind, YOLOv7 is a popular open source project, new changes and updates are added very quickly. Iâ€™m also very glad to send a <a href=https://github.com/WongKinYiu/yolov7/pull/434/commits>PR</a> to improve the export script last night due to this writing ðŸ˜ƒ.After you got the exported Core ML models, no kidding, you have tons of things in your todo list. <a href=https://twitter.com/mhollemans>Matthijs Hollemans</a> has already written an insightful <a href=https://machinethink.net/blog/bounding-boxes/>article</a> in his blog, be sure to checkout and <a href=https://leanpub.com/coreml-survival-guide>support his efforts</a>! Here is my short list:</p><ul><li>Configure your Core ML model in a particular way. You can either append NMS to your model or write a lot of additional Swift code. IMHO, this is the most difficult part if you know nothing about the object detection model.</li><li>Specify camera resolution, donâ€™t simply select the highest resolution available if your app doesnâ€™t require it.</li><li>Resize or crop your input image to fit network input dimension, it depends on your application.</li><li>Feed modified images to your model in a correct orientation.</li><li>Fix Visionâ€™s weird orin.</li><li>Convert bounding boxes coordinate system for display. This is also a trickier part, you need some iOS development experiences and a pencil for calculation ðŸ˜Ž.</li></ul><p>According to Hollemansâ€™s article, there are at least 5 different coordinate systems you need to take care, not to mention how to handle real-time capturing correctly and efficiently is also non-trivial. You can follow these two articles to learn how to create a custom camera view.</p><p><a href=https://developer.apple.com/documentation/vision/recognizing_objects_in_live_capture>Apple Developer Documentation | Recognizing Objects in Live Capture</a></p><p><a href=https://guides.codepath.com/ios/Creating-a-Custom-Camera-View>Creating a Custom Camera View | CodePath iOS Cliffnotes</a>
At the latest <a href=https://developer.apple.com/videos/play/wwdc2022/10027/>WWDC 2022</a>, Apple introduced even more performance tools to its CoreML toolchain, now you can check your modelâ€™s metadata via <strong>performance reports</strong> and <strong>Core ML Instrument</strong> without writing any code. You can also use <code>computeUnits = .cpuAndNeuralEngine</code> if you donâ€™t want to use the GPU but always force the model to run on the CPU and ANE if available.</p><p><img loading=lazy src=images/2.png#layoutTextWidth alt=image>
Prefer CPU and ANE instead of GPU.</p><p>You can learn more about ANE from the following repository, thank you again Hollemans.</p><p><a href=https://github.com/hollance/neural-engine>GitHub - hollance/neural-engine: Everything we actually know about the Apple Neural Engine (ANE)</a></p><p>Here are snapshots from my modelâ€™s performance reports.</p><p><img loading=lazy src=images/3.png#layoutTextWidth alt=image>
You can evaluate your model via drag-and-drop image files.</p><p>There is no significant inference speed differences among quantization models, but the model size only about half the size. Itâ€™s a good thing for your mobile applications.
<img loading=lazy src=images/4.png#layoutTextWidth alt=image></p><p><img loading=lazy src=images/5.png#layoutTextWidth alt=image>
No inference speed improved. (Left is FP32, right is FP16)</p><p><img loading=lazy src=images/6.png#layoutTextWidth alt=image></p><p><img loading=lazy src=images/7.png#layoutTextWidth alt=image>
Half the size of the FP32 model.</p><p>Finally, you have a working YOLOv7 Core ML model on the iOS devices, be careful of the heatðŸ”¥. Happy coding!</p><p><img loading=lazy src=images/8.gif#layoutTextWidth alt=image>
<strong>Yolov7-tiny on iPad Mini 6.</strong> Copyrights of <a href=https://youtu.be/hngml3y2Rq8>BBIBBI Dance Practice</a> belongs Kakao Entertainment.</p><h3 id=references>References<a hidden class=anchor aria-hidden=true href=#references>#</a></h3><ul><li><a href=https://developer.apple.com/documentation/vision/recognizing_objects_in_live_capture>Recognizing Objects in Live Capture</a></li><li><a href=https://machinethink.net/blog/bounding-boxes/>How to display Vision bounding boxes</a></li><li><a href=https://guides.codepath.com/ios/Creating-a-Custom-Camera-View>Creating a Custom Camera View</a></li><li><a href=https://github.com/hollance/neural-engine>The Neural Engineâ€Šâ€”â€Šwhat do we know about it?</a></li><li><a href=https://github.com/WongKinYiu/yolov7>WongKinYiu/yolov7</a></li><li><a href=https://developer.apple.com/videos/play/wwdc2022/10027/>WWDC2022â€Šâ€”â€ŠOptimize your Core ML usage</a></li><li><a href=https://machinethink.net/blog/mobilenet-ssdlite-coreml/>MobileNetV2 + SSDLite with Core ML</a></li><li><a href=https://github.com/dbsystel/yolov5-coreml-tools>yolov5â€Šâ€”â€ŠCoreML Tools</a></li></ul></div><footer class=post-footer><ul class=post-tags><li><a href=https://blog.cmwang.net/en/tags/coreml/>CoreML</a></li><li><a href=https://blog.cmwang.net/en/tags/ios/>iOS</a></li><li><a href=https://blog.cmwang.net/en/tags/computer-vision/>Computer Vision</a></li><li><a href=https://blog.cmwang.net/en/tags/object-detection/>Object Detection</a></li></ul><nav class=paginav><a class=prev href=https://blog.cmwang.net/en/posts/2022/12/configuring-usb-fs-for-usb3-vision-camera/><span class=title>Â« Prev</span><br><span>Configuring USB-FS for USB3 Vision Camera</span></a>
<a class=next href=https://blog.cmwang.net/en/posts/2022/07/performance-benchmarking-of-yolov7-tensorrt/><span class=title>Next Â»</span><br><span>Performance Benchmarking of YOLOv7 TensorRT</span></a></nav><div class=share-buttons><a target=_blank rel="noopener noreferrer" aria-label="share Execute YOLOv7 model on iOS devices on twitter" href="https://twitter.com/intent/tweet/?text=Execute%20YOLOv7%20model%20on%20iOS%20devices&amp;url=https%3a%2f%2fblog.cmwang.net%2fen%2fposts%2f2022%2f08%2fexecute-yolov7-model-on-ios-devices%2f&amp;hashtags=CoreML%2ciOS%2cComputerVision%2cObjectDetection"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM195.519 424.544c135.939.0 210.268-112.643 210.268-210.268.0-3.218.0-6.437-.153-9.502 14.406-10.421 26.973-23.448 36.935-38.314-13.18 5.824-27.433 9.809-42.452 11.648 15.326-9.196 26.973-23.602 32.49-40.92-14.252 8.429-30.038 14.56-46.896 17.931-13.487-14.406-32.644-23.295-53.946-23.295-40.767.0-73.87 33.104-73.87 73.87.0 5.824.613 11.494 1.992 16.858-61.456-3.065-115.862-32.49-152.337-77.241-6.284 10.881-9.962 23.601-9.962 37.088.0 25.594 13.027 48.276 32.95 61.456-12.107-.307-23.448-3.678-33.41-9.196v.92c0 35.862 25.441 65.594 59.311 72.49-6.13 1.686-12.72 2.606-19.464 2.606-4.751.0-9.348-.46-13.946-1.38 9.349 29.426 36.628 50.728 68.965 51.341-25.287 19.771-57.164 31.571-91.8 31.571-5.977.0-11.801-.306-17.625-1.073 32.337 21.15 71.264 33.41 112.95 33.41z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Execute YOLOv7 model on iOS devices on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2fblog.cmwang.net%2fen%2fposts%2f2022%2f08%2fexecute-yolov7-model-on-ios-devices%2f&amp;title=Execute%20YOLOv7%20model%20on%20iOS%20devices&amp;summary=Execute%20YOLOv7%20model%20on%20iOS%20devices&amp;source=https%3a%2f%2fblog.cmwang.net%2fen%2fposts%2f2022%2f08%2fexecute-yolov7-model-on-ios-devices%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Execute YOLOv7 model on iOS devices on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fblog.cmwang.net%2fen%2fposts%2f2022%2f08%2fexecute-yolov7-model-on-ios-devices%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Execute YOLOv7 model on iOS devices on telegram" href="https://telegram.me/share/url?text=Execute%20YOLOv7%20model%20on%20iOS%20devices&amp;url=https%3a%2f%2fblog.cmwang.net%2fen%2fposts%2f2022%2f08%2fexecute-yolov7-model-on-ios-devices%2f"><svg viewBox="2 2 28 28" height="30" width="30" fill="currentcolor"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg></a></div></footer><section class="article discussion"><script defer>let discussionSection=document.querySelector("section.article.discussion"),logoSwitches=document.querySelector(".logo-switches"),debounceTimer;function loadComments(e,t){let n=document.createElement("script");n.src="https://giscus.app/client.js",n.setAttribute("data-repo","taka-wang/taka-wang.github.io"),n.setAttribute("data-repo-id","R_kgDOKc6Suw"),n.setAttribute("data-category","Announcements"),n.setAttribute("data-category-id","DIC_kwDOKc6Su84CaTJ_"),n.setAttribute("data-mapping","pathname"),n.setAttribute("data-strict","0"),n.setAttribute("data-reactions-enabled","1"),n.setAttribute("data-emit-metadata","0"),n.setAttribute("data-input-position","bottom"),n.setAttribute("data-theme",e),n.setAttribute("data-lang",t),n.setAttribute("data-loading","lazy"),n.setAttribute("crossorigin","anonymous"),n.setAttribute("async",""),discussionSection.innerHTML="",discussionSection.appendChild(n)}let currentHugoTheme=window.localStorage.getItem("pref-theme")||"light",currentLang=(document.documentElement.lang||"en").toLowerCase();loadComments(currentHugoTheme=="light"?"light":"dark",currentLang==="zh"?"zh-TW":"en"),logoSwitches.onclick=()=>{clearTimeout(debounceTimer),debounceTimer=setTimeout(()=>{let e=window.localStorage.getItem("pref-theme")||"light",t=(document.documentElement.lang||"en").toLowerCase();(e!==currentHugoTheme||t!==currentLang)&&(currentHugoTheme=e,currentLang=t,loadComments(e=="light"?"light":"dark",t==="zh"?"zh-TW":"en"))},200)}</script></section></article></main><footer class=footer><span>&copy; 2023 <a href=https://blog.cmwang.net/en/>Liberation Notes</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg></a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>