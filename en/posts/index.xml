<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>Posts on Liberation Notes</title>
    <link>https://blog.cmwang.net/en/posts/</link>
    <description>Recent content in Posts on Liberation Notes</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    <lastBuildDate>Sun, 16 Apr 2023 15:16:37 +0000</lastBuildDate><atom:link href="https://blog.cmwang.net/en/posts/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Make Crispy Plum</title>
      <link>https://blog.cmwang.net/en/posts/2023/04/make-crispy-plum/</link>
      <pubDate>Sun, 16 Apr 2023 15:16:37 +0000</pubDate>
      
      <guid>https://blog.cmwang.net/en/posts/2023/04/make-crispy-plum/</guid>
      <description>Making preserved plums is a straightforward technique that doesn’t call for specialized equipment</description>
      <content:encoded><![CDATA[<p><img loading="lazy" src="1.jpeg" alt="image"  />

green plums</p>
<p>The time leading up to and following the Tomb Sweeping Festival has become a significant rite for my family in recent years. We purchase green plums in advance from local farmers in Xinyi Township and use them to create preserved plums. However, due to insufficient rainfall this year, the fruit was slightly less than satisfactory compared to previous years.</p>
<p>Making preserved plums is a straightforward technique that doesn’t call for specialized equipment; rather, it just takes some time and perseverance. One kilogram of green plums, 100 grams of coarse salt, and 600 grams of sugar make up the recipe, which is quite straightforward. Please carry out the actions listed below:</p>
<p><img loading="lazy" src="2.jpeg" alt="image"  />

Rub plums with salt</p>
<ul>
<li>Wash the plums and remove the stems one by one.</li>
<li>Blanch the plums by rubbing them with coarse salt for at least 15 minutes until the surface becomes moist and changes color.</li>
<li>Tap the plums with a hammer one by one until they crack slightly, but do not smash them.</li>
<li>Soak the plums in saltwater for 8 hours, making sure that the water covers them completely.</li>
<li>Pour out the syrup and replace it with fresh syrup every 8 hours.</li>
<li>Repeat step 5 for 3 days until the plums are fully infused with the sugar syrup.</li>
<li>Drain the plums of any remaining bitter syrup, replace it with the final batch of sugar syrup, and let it sit for another 3 days to complete the process.</li>
</ul>
<p>I am grateful to my family for bringing a sense of ceremony into our daily lives. This ritual makes me appreciate the beauty of life.</p>
<p><img loading="lazy" src="3.jpeg" alt="image"  />

Crispy Plum</p>
]]></content:encoded>
    </item>
    
    <item>
      <title>You Will Have a Good Day</title>
      <link>https://blog.cmwang.net/en/posts/2023/04/you-will-have-a-good-day/</link>
      <pubDate>Thu, 06 Apr 2023 12:29:10 +0000</pubDate>
      
      <guid>https://blog.cmwang.net/en/posts/2023/04/you-will-have-a-good-day/</guid>
      <description>Today you will have a good day. Jesus spoke to them at once, “Don’t be afraid,” he said. “Take courage! I am here!” (Mark 6:50)</description>
      <content:encoded><![CDATA[<p><img loading="lazy" src="1.jpeg" alt="image"  />
</p>
<p>Today you will have a good day. Jesus spoke to them at once, “Don’t be afraid,” he said. “Take courage! I am here!” (Mark 6:50)</p>
<p><img loading="lazy" src="2.png" alt="image"  />
</p>
]]></content:encoded>
    </item>
    
    <item>
      <title>Reflections on Five Years in the Company</title>
      <link>https://blog.cmwang.net/en/posts/2023/03/reflections-on-five-years-in-the-company/</link>
      <pubDate>Mon, 27 Mar 2023 14:46:01 +0000</pubDate>
      
      <guid>https://blog.cmwang.net/en/posts/2023/03/reflections-on-five-years-in-the-company/</guid>
      <description>As I reflect on my five years in this company, I realize how much has happened and how much I have grown</description>
      <content:encoded><![CDATA[<p><img loading="lazy" src="1.jpeg" alt="image"  />
</p>
<p>As I reflect on my five years in this company, I realize how much has happened and how much I have grown. From having a supervisor to being a lone ranger, to forming a small team and eventually founding the Nilvana, time has flown by quickly, and I am left wondering where tomorrow will take me.</p>
<p>It seems like just yesterday that I started my journey in this company. I remember the excitement of meeting new colleagues, learning new skills, and finding my place in the organization. Over time, I became more confident and began taking on more responsibilities. I was proud of my accomplishments and the impact that I was making.</p>
<p>However, the road was not always easy. There were times when I faced challenges that made me question my abilities and my decisions. I learned that setbacks and failures are a part of the journey, and they can be a source of growth and resilience.</p>
<p>One of the most significant moments in my journey was the creation of the Nilvana brand. It was a leap of faith, but I knew that I had a vision that could change the game. With the help of a few dedicated team members, we worked tirelessly to bring the brand to life. Seeing it take off and gain recognition was one of the proudest moments of my career.</p>
<p>Looking back on my journey, I realize that time truly flies. It seems like just yesterday that I started, but five years have gone by in a blink of an eye. As I prepare to take on new challenges, I am grateful for the experiences and the lessons that I have learned.</p>

<div style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;">
  <iframe src="https://www.youtube-nocookie.com/embed/1t8kAbUg4t4" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;" allowfullscreen title="YouTube Video"></iframe>
</div>

]]></content:encoded>
    </item>
    
    <item>
      <title>Configuring USB-FS for USB3 Vision Camera</title>
      <link>https://blog.cmwang.net/en/posts/2022/12/configuring-usb-fs-for-usb3-vision-camera/</link>
      <pubDate>Thu, 29 Dec 2022 13:14:24 +0000</pubDate>
      
      <guid>https://blog.cmwang.net/en/posts/2022/12/configuring-usb-fs-for-usb3-vision-camera/</guid>
      <description>This post explains how to increase the buffer memory for USB-FS devices on Linux systems.</description>
      <content:encoded><![CDATA[<p><img loading="lazy" src="images/1.jpeg" alt="image"  />
</p>
<p>This post explains how to increase the buffer memory for USB-FS devices on Linux systems in order to make full use of the imaging hardware’s capabilities. By default, USB-FS on Linux systems only allows 16 MB of buffer memory for all USB devices, which may not be sufficient for high-resolution cameras or multiple-camera set ups, resulting in image acquisition issues. To configure USB-FS and increase the buffer memory limit, the following steps should be taken:</p>
<p>Note that GRUB is for desktop PC architecture. ARM embedded systems use a different bootloader, as GRUB requires a system with a BIOS, and embedded systems do not have one.</p>
<ol>
<li>Create the file <code>/etc/rc.local</code> with the command <code>sudo touch /etc/rc.local</code>. This will create the file, allowing it to be edited.</li>
<li>Change the permissions of the file with the command <code>sudo chmod 744 /etc/rc.local</code>. This will ensure that the file has the correct permissions to be edited.</li>
<li>Change the buffer memory limit with the command <code>echo 1000 &amp;gt; /sys/module/usbcore/parameters/usbfs_memory_mb</code>. This command will set the memory limit to 1000 MB, which should be enough to prevent image acquisition issues.`</li>
</ol>
<h2 id="etcrclocal-file-contents-example">/etc/rc.local file contents example</h2>


<div class="terminal space shadow">
    <div class="top">
        <div class="btns">
            <span class="circle red"></span>
            <span class="circle yellow"></span>
            <span class="circle green"></span>
        </div>
        <div class="title">
            /etc/rc.local
        </div>
    </div>
    <div class="terminalbody"><div class="highlight"><pre tabindex="0" class="chroma"><code class="language-sh" data-lang="sh"><span class="line"><span class="cl"><span class="cp">#!/bin/sh -e
</span></span></span><span class="line"><span class="cl"><span class="cp"></span>
</span></span><span class="line"><span class="cl"><span class="nb">echo</span> <span class="m">1000</span> &gt; /sys/module/usbcore/parameters/usbfs_memory_mb
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="nb">exit</span>
</span></span><span class="line"><span class="cl"><span class="m">0</span>
</span></span></code></pre></div></div>
</div>
<br />

<p>After changing the memory limit, it is important to confirm the changes have been made correctly. This can be done by running the command:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-sh" data-lang="sh"><span class="line"><span class="cl">cat /sys/module/usbcore/parameters/usbfs_memory_mb
</span></span></code></pre></div><p>which will display the current memory limit. If the limit is still 16 MB, then the changes will need to be made again. Additionally, further information about USB-FS on Linux can be found in the following sources:</p>
<ul>
<li><a href="https://www.flir.asia/support-center/iis/machine-vision/application-note/understanding-usbfs-on-linux/">Understanding USBFS on Linux</a></li>
<li><a href="https://importgeek.wordpress.com/2017/02/26/increase-usbfs-memory-limit-in-ubuntu/">Increase USBFS Memory Limit in Ubuntu</a></li>
<li><a href="https://forums.developer.nvidia.com/t/change-kernel-cmdline-by-edit-etc-default-grub-failed/159831/2">Change Kernel Cmdline by Edit /etc/default/grub Failed</a></li>
</ul>
]]></content:encoded>
    </item>
    
    <item>
      <title>Execute YOLOv7 model on iOS devices</title>
      <link>https://blog.cmwang.net/en/posts/2022/08/execute-yolov7-model-on-ios-devices/</link>
      <pubDate>Sat, 06 Aug 2022 11:04:15 +0000</pubDate>
      
      <guid>https://blog.cmwang.net/en/posts/2022/08/execute-yolov7-model-on-ios-devices/</guid>
      <description>Real-time object detection with CoreML is trickier than you think.</description>
      <content:encoded><![CDATA[<p><img loading="lazy" src="images/1.jpeg" alt="image"  />

Real-time object detection with CoreML is trickier than you think.</p>
<p>Usually, you have two choices to build a machine learning app for your mobile device, <strong>inference</strong> can happen either directly on-device or on cloud-based servers. It all depends on your usage scenario, there is no one-size fit all solution. In this article, we will only focus on on-device inference.</p>
<p>At WWDC 2017 Apple released first Core ML. Core ML is Apple’s machine learning framework for doing on-device inference. Core ML is not the only way to do on-device inference, there are tens of libraries and frameworks that are compatible with iOS, but that’s beyond the scope of this article. From the <a href="https://github.com/WongKinYiu/yolov7">YOLOv7 official repository</a>, we can get the <a href="https://github.com/WongKinYiu/yolov7/blob/main/export.py">export script</a> to convert trained PyTorch model to Core ML format effortlessly. However, keep one thing in mind, YOLOv7 is a popular open source project, new changes and updates are added very quickly. I’m also very glad to send a <a href="https://github.com/WongKinYiu/yolov7/pull/434/commits">PR</a> to improve the export script last night due to this writing 😃.After you got the exported Core ML models, no kidding, you have tons of things in your todo list. <a href="https://twitter.com/mhollemans">Matthijs Hollemans</a> has already written an insightful <a href="https://machinethink.net/blog/bounding-boxes/">article</a> in his blog, be sure to checkout and <a href="https://leanpub.com/coreml-survival-guide">support his efforts</a>! Here is my short list:</p>
<ul>
<li>Configure your Core ML model in a particular way. You can either append NMS to your model or write a lot of additional Swift code. IMHO, this is the most difficult part if you know nothing about the object detection model.</li>
<li>Specify camera resolution, don’t simply select the highest resolution available if your app doesn’t require it.</li>
<li>Resize or crop your input image to fit network input dimension, it depends on your application.</li>
<li>Feed modified images to your model in a correct orientation.</li>
<li>Fix Vision’s weird orin.</li>
<li>Convert bounding boxes coordinate system for display. This is also a trickier part, you need some iOS development experiences and a pencil for calculation 😎.</li>
</ul>
<p>According to Hollemans’s article, there are at least 5 different coordinate systems you need to take care, not to mention how to handle real-time capturing correctly and efficiently is also non-trivial. You can follow these two articles to learn how to create a custom camera view.</p>
<p><a href="https://developer.apple.com/documentation/vision/recognizing_objects_in_live_capture">Apple Developer Documentation | Recognizing Objects in Live Capture</a></p>
<p><a href="https://guides.codepath.com/ios/Creating-a-Custom-Camera-View">Creating a Custom Camera View | CodePath iOS Cliffnotes</a>
At the latest <a href="https://developer.apple.com/videos/play/wwdc2022/10027/">WWDC 2022</a>, Apple introduced even more performance tools to its CoreML toolchain, now you can check your model’s metadata via <strong>performance reports</strong> and <strong>Core ML Instrument</strong> without writing any code. You can also use <code>computeUnits = .cpuAndNeuralEngine</code> if you don’t want to use the GPU but always force the model to run on the CPU and ANE if available.</p>
<p><img loading="lazy" src="images/2.png#layoutTextWidth" alt="image"  />

Prefer CPU and ANE instead of GPU.</p>
<p>You can learn more about ANE from the following repository, thank you again Hollemans.</p>
<p><a href="https://github.com/hollance/neural-engine">GitHub - hollance/neural-engine: Everything we actually know about the Apple Neural Engine (ANE)</a></p>
<p>Here are snapshots from my model’s performance reports.</p>
<p><img loading="lazy" src="images/3.png#layoutTextWidth" alt="image"  />

You can evaluate your model via drag-and-drop image files.</p>
<p>There is no significant inference speed differences among quantization models, but the model size only about half the size. It’s a good thing for your mobile applications.
<img loading="lazy" src="images/4.png#layoutTextWidth" alt="image"  />
</p>
<p><img loading="lazy" src="images/5.png#layoutTextWidth" alt="image"  />

No inference speed improved. (Left is FP32, right is FP16)</p>
<p><img loading="lazy" src="images/6.png#layoutTextWidth" alt="image"  />
</p>
<p><img loading="lazy" src="images/7.png#layoutTextWidth" alt="image"  />

Half the size of the FP32 model.</p>
<p>Finally, you have a working YOLOv7 Core ML model on the iOS devices, be careful of the heat🔥. Happy coding!</p>
<p><img loading="lazy" src="images/8.gif#layoutTextWidth" alt="image"  />

<strong>Yolov7-tiny on iPad Mini 6.</strong> Copyrights of <a href="https://youtu.be/hngml3y2Rq8">BBIBBI Dance Practice</a> belongs Kakao Entertainment.</p>
<p>Any opinions expressed are solely my own and do not express the views or opinions of my employer.</p>
<h3 id="references">References</h3>
<ul>
<li><a href="https://developer.apple.com/documentation/vision/recognizing_objects_in_live_capture">Recognizing Objects in Live Capture</a></li>
<li><a href="https://machinethink.net/blog/bounding-boxes/">How to display Vision bounding boxes</a></li>
<li><a href="https://guides.codepath.com/ios/Creating-a-Custom-Camera-View">Creating a Custom Camera View</a></li>
<li><a href="https://github.com/hollance/neural-engine">The Neural Engine — what do we know about it?</a></li>
<li><a href="https://github.com/WongKinYiu/yolov7">WongKinYiu/yolov7</a></li>
<li><a href="https://developer.apple.com/videos/play/wwdc2022/10027/">WWDC2022 — Optimize your Core ML usage</a></li>
<li><a href="https://machinethink.net/blog/mobilenet-ssdlite-coreml/">MobileNetV2 + SSDLite with Core ML</a></li>
<li><a href="https://github.com/dbsystel/yolov5-coreml-tools">yolov5 — CoreML Tools</a></li>
</ul>
]]></content:encoded>
    </item>
    
    <item>
      <title>Design Patterns Explained - CH8</title>
      <link>https://blog.cmwang.net/en/posts/2017/01/design-patterns-explained-ch8/</link>
      <pubDate>Sat, 21 Jan 2017 20:12:53 +0000</pubDate>
      
      <guid>https://blog.cmwang.net/en/posts/2017/01/design-patterns-explained-ch8/</guid>
      <description>&lt;h1 id=&#34;expanding-our-horizons&#34;&gt;Expanding Our Horizons&lt;/h1&gt;
&lt;h2 id=&#34;overview&#34;&gt;Overview&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Object&lt;/li&gt;
&lt;li&gt;Encapsulation&lt;/li&gt;
&lt;li&gt;Inheritance&lt;/li&gt;
&lt;li&gt;Handling variation&lt;/li&gt;
&lt;li&gt;Commonality and variability analysis&lt;/li&gt;
&lt;li&gt;Abstract class and its derived classes&lt;/li&gt;
&lt;/ul&gt;</description>
      <content:encoded><![CDATA[<h1 id="expanding-our-horizons">Expanding Our Horizons</h1>
<h2 id="overview">Overview</h2>
<ul>
<li>Object</li>
<li>Encapsulation</li>
<li>Inheritance</li>
<li>Handling variation</li>
<li>Commonality and variability analysis</li>
<li>Abstract class and its derived classes</li>
</ul>
<h2 id="objects-traditional-view-and-new-view">Objects: Traditional view and new view</h2>
<h3 id="traditional-view">Traditional View</h3>
<ul>
<li>Data with methods - <strong>smart data</strong></li>
<li>too narrow from <strong>implementation</strong> perspective</li>
</ul>
<h3 id="broad-view">Broad View</h3>
<ul>
<li>From <strong>conceptual</strong> perspective</li>
<li>an object is an <em>entity that has responsibilities</em> (責任), these responsibilities define the behavior of the object. Or an <em>entity that has specific behavior</em> (特定行為).</li>
</ul>
<h2 id="focus-on-intentionmotivation-not-implementation">Focus on intention/motivation not implementation</h2>
<p>This view enables us to build software in two steps:</p>
<ol>
<li>Make a preliminary design without worrying about all the details involved.</li>
<li>Implement the design.</li>
</ol>
<p>The reason this works is that we only have to focus on the object’s <strong>public interface</strong> — the communication window through which I ask the object to do something.</p>
<p>Hiding implementations behind interfaces essentially <strong>decouples</strong> them from the using objects.</p>
<hr>
<h2 id="encapsulation-traditional-view-and-new-view">Encapsulation: Traditional view and new view</h2>
<h3 id="traditional-view-1">Traditional View</h3>
<ul>
<li>data hiding</li>
</ul>
<h3 id="broad-view-1">Broad View</h3>
<ul>
<li><strong>any kind of hiding</strong>
<ul>
<li>Implementations (data, methods..)</li>
<li><strong>Drived</strong> classes (<strong>Encapsulation of type</strong> is achieved when there is an <em>abstract class</em> with derivations (or an interface with implementations) that are used <em>polymorphically</em>)</li>
<li>Design details</li>
<li>Instantiation rules (ex. creational patterns)</li>
</ul>
</li>
</ul>
<h2 id="advantage">Advantage</h2>
<p>It gives us a better way to split up (decompose) our programs. The <strong>encapsulating layers</strong> become the <strong>interfaces</strong> we design to. (封裝層成為設計需要遵循的介面)</p>
<p>By encapsulating different kinds of subclasses (<strong>encapsulation of type</strong>), we can add new ones without changing any of the client programs using them. (<em>GoF typically means when they mention encapsulation</em>)</p>
<hr>
<h2 id="inheritance">Inheritance</h2>
<h3 id="traditional-view-2">Traditional View</h3>
<ul>
<li><strong>reuse</strong> of classes</li>
<li>achived by creating classes and then deriving new (<em>spcialized</em>) classes bases on these base (<em>generalized</em>) classes</li>
</ul>
<h3 id="broad-view-2">Broad View</h3>
<ul>
<li>using inheritance for specialization, however
<ul>
<li>can cause weak cohesion</li>
<li>reduces possibility of reuse</li>
<li>does not scale well with variation</li>
</ul>
</li>
<li><em>to classify classes as things that behave the same way</em>. (<strong>placeholder</strong>)</li>
</ul>
<hr>
<h2 id="find-what-is-varying-and-encapsulate-it">Find What Is Varying and Encapsulate It</h2>
<blockquote>
<p>Consider <strong>what</strong> should be variable in your design. This approach is the opposite of focusing on the <strong>cause</strong> of redesign. Instead of considering what might force a change to a design, consider what you want to be <strong><em>able</em></strong> to change without redesign. The focus here is on <strong><em>encapsulating the concept that varies</em></strong>, a theme of many design patterns.
&ndash; GoF, Design Patterns</p>
</blockquote>
<hr>
<h2 id="more-about-gofs-encapsulation">More about GoF&rsquo;s Encapsulation</h2>
<ul>
<li>Design patterns use inheritance to classify variations in behaviors.</li>
<li>Hiding classes with an abstract class or interface — <strong>type encapsulation</strong>.</li>
<li>Containing a reference of this <em>abstract class</em> or <em>interface</em> type (aggregation) <strong>hides</strong> these <em>derived classes</em> that represent <strong>variations</strong> in behavior.</li>
<li>In effect, many design patterns <em>use encapsulation to create layers</em> between objects.</li>
</ul>
<hr>
<h2 id="containing-variation-in-data-vs-containing-variation-in-behavior">Containing variation in data vs containing variation in behavior</h2>
<h3 id="handling-variation-in-data">Handling variation in data</h3>
<ul>
<li>Have a data member that tells me what type of movement my object has.</li>
<li>Have two different types of <strong>Animals</strong> (both derived from the base <strong>Animal</strong> class) — one for walking and one for flying.</li>
</ul>
<h3 id="handling-variation-in-behavior-with-objects">Handling variation in behavior with objects</h3>
<p>Using objects to contain variation in attributes and using objects to contain variation in behavior are very similar. Don&rsquo;t afraid.</p>
<hr>
<h2 id="commonality-and-variability">Commonality and Variability</h2>
<p>Identify <strong>where</strong> things vary (commonality analysis) and then identify <strong>how</strong> they vary (variability analysis).</p>
<p><strong>Commonality analysis</strong> is the search for common elements that helps us understand how family members are the same.</p>
<p><strong>Variability analysis</strong> reveals how family members vary. Variability only makes sense within a given commonality.</p>
<p>Ex. Whiteboard marker, pencil, ballpoint pen</p>
<ul>
<li>Commonality: writing instrument</li>
<li>Variability: material to write, shape..</li>
</ul>
<h2 id="commonality-and-variability-and-abstract-class">Commonality and Variability and Abstract class</h2>
<p><strong>Commonality analysis</strong> seeks structure that is unlikely to change over time, while <strong>variability analysis</strong> captures structure that is likely to change. <strong><em>Variability analysis makes sense only in terms of the context defined by the associated commonality analysis</em></strong>.
In other words, if variations are the specific concrete cases in the domain, commonality defines the concepts in the domain that tie them together. The <strong><em>common concepts</em></strong> will be represented by <strong>abstract classes</strong>. The <strong><em>variations</em></strong> found by variability analysis will be implemented by the <strong>concrete classes</strong>.</p>
<h2 id="relationship-between-commonality-and-variability-perspectives-and-abstract-classes">Relationship between Commonality and Variability, perspectives, and abstract classes</h2>
<p><img loading="lazy" src="relationship.png" alt="relationship"  />
</p>
<hr>
<h2 id="benefits-of-using-abstract-classes-for-specialization">Benefits of using abstract classes for specialization</h2>
<p><img loading="lazy" src="benefits.png" alt="benefits"  />
</p>
<hr>
<h2 id="two-step-procedure-for-design">Two-Step Procedure for Design</h2>
<p>Ask yourself:</p>
<ul>
<li>When defining an <strong>abstract class</strong> (commonality):
<ul>
<li>What <strong>interface</strong> is needed to handle all the responsibilities (core concepts from the conceptual perspective) of this class?</li>
</ul>
</li>
<li>When defining <strong>derived classes</strong>:
<ul>
<li>Given this particular implementation (this <strong>variation</strong>), how can I implement it (variation) with the given specification?</li>
</ul>
</li>
</ul>
<hr>
<h2 id="take-away">Take away</h2>
<p>Think object-oriented in a broad way.</p>
<ul>
<li>Object: an <em>entity that has responsibilities (specific behavior)</em></li>
<li>Encapsulation: <strong>any</strong> kind of <strong>hiding</strong> (instantiation rule, type..)</li>
<li>Inheritance: use for <strong>specialization</strong> and <em>classify classes as things that behave the same way</em>.</li>
</ul>
<p><strong>Find what is varying and encapsulate it</strong> (in behavior).</p>
<p>Commonality, variability and abstract class: use inheritance to classify variations in behaviors.</p>]]></content:encoded>
    </item>
    
  </channel>
</rss>
