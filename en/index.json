[{"content":"MacOS Installing via Homebrew Package Manager is the most convenient method.\nInstall Packages with Homebrew on macOS # install homebrew /bin/bash -c \u0026#34;$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)\u0026#34; # install hugo brew install hugo # verify hugo installation hugo version \u0026gt; hugo v0.119.0-b84644c008e0dc2c4b67bd69cccf87a41a03937e+extended darwin/amd64 BuildDate=2023-09-24T15:20:17Z VendorInfo=brew Windows Open powershell with Task Manager: Press win + x, navigate to Windows PowerShell (Admin), make sure to select Run as administrator.\nCheck the execution policy:\nGet-ExecutionPolicy If it shows Restricted, run the following command:\nSet-ExecutionPolicy AllSigned Install Chocolatey:\nSet-ExecutionPolicy Bypass -Scope Process -Force; iex ((New-Object System.Net.WebClient).DownloadString(\u0026#39;https://chocolatey.org/install.ps1\u0026#39;)) Verify the installation by typing in powershell:\nchoco # If you receive a response, the installation is successful Now, you can install Hugo using Chocolatey! üëáüëáüëá\nInstall Packages with Chocolatey on Windows # install hugo choco install hugo-extended # verify hugo installation hugo version \u0026gt; hugo v0.119.0-b84644c008e0dc2c4b67bd69cccf87a41a03937e... References Install Hugo on macOS Install Hugo on Windows ÊàëÁöÑ Windows Êñ∞Ë£ùÊ©ü Chocolatey ÂÆâË£ùÊ∏ÖÂñÆ ","permalink":"https://blog.cmwang.net/en/posts/2023/10/how-to-install-hugo/","summary":"We will introduce how to install hugo on your system in this post.","title":"How to Install Hugo"},{"content":"Welcome to the world of Hugo, where you can swiftly create your personal website! This guide will walk you through the process of creating a private website on GitHub and deploying it to GitHub Pages.\nWhat\u0026rsquo;s included in this guide .gitignore: Excludes files from version control. .github/workflows/hugo.yml: Uses GitHub Actions to deploy your private Hugo repository to a public GitHub Pages repository. Steps Create a private repository hugo-site on GitHub to manage your website source code, ensuring to include the README.md file.\nCreate a public repository {YOUR_USER_NAME}.github.io on GitHub to upload your static web pages to GitHub Pages.\nClone hugo-site to your local machine:\ngit clone https://github.com/{YOUR_USER_NAME}/hugo-site.git Alternatively, if you are using Git Submodules:\ngit clone --recursive https://github.com/{YOUR_USER_NAME}/hugo-site.git Create a Hugo project in the same directory as hugo-site (not inside hugo-site):\nhugo new site hugo-site --force Add an example theme. Here, we\u0026rsquo;ll use PaperMod as an example:\nAdd theme cd hugo-site git submodule add --depth=1 https://github.com/adityatelange/hugo-PaperMod.git themes/PaperMod echo \u0026#34;theme : \u0026#39;PaperMod\u0026#39;\u0026#34; \u0026gt;\u0026gt; hugo.yaml Copy the .gitignore file into hugo-site.\n[Optional] Set up GitHub Action for automatic deployment:\nmkdir -p .github/workflows/ touch .github/workflows/hugo.yml Copy the contents of hugo.yml into the newly created hugo.yml file. Remember to modify these two parts:\ntoken: ${{ secrets.ACCESS_TOKEN }}: Use a personal access token for this private repository. repository-name: YOUR_USER_NAME}}/YOUR_USER_NAME.github.io: Replace YOUR_USER_NAME with your GitHub username. Test your Hugo website:\nhugo server -D Start writing your own posts:\nhugo new posts/20231006/index.md Begin writing in index.md inside content/posts/20231006, and place images in the same folder.\nCongratulations, you\u0026rsquo;ve configured your private repository! Don\u0026rsquo;t forget to commit your changes to the private repository. Best of luck creating your beautiful website!\nReferences How to setup ACCESS Token Using GitHub Actions to Publish Hugo Site From Private to Public Repo Learn to create a Hugo site in minutes. ","permalink":"https://blog.cmwang.net/en/posts/2023/10/hugo-get-started/","summary":"This guide will walk you through the process of creating a private website on GitHub and deploying it to GitHub Pages","title":"Hugo Deployment Guide"},{"content":"After a few days of experimentation, I have officially relaunched my Blog on Github Pages. I often contemplate which platform to use for recording my thoughts, or what format to become familiar with, but nothing beats the authenticity of continuous documentation.\nHaving a notebook that I can control not only allows me to express my feelings but also enables me to record the bits and pieces of my learning journey. Hugo is truly an amazing tool; the overall experience is much better than it was with Hexo several years ago. In the upcoming posts, I will share some simple records on how to configure it on your own.\n","permalink":"https://blog.cmwang.net/en/posts/2023/10/blog-reboot-2023/","summary":"In 2023, I\u0026rsquo;m going back to the simplicity of static webpages.","title":"Moving Back to Github Pages"},{"content":" Hello World!!\ngit clone https://github.com/{YOUR_USER_NAME}/hugo-site.git Let\u0026rsquo;s Do the Thing!\nimport sys from PyQt5.QtWidgets import QApplication, QLabel, QMainWindow, QVBoxLayout, QWidget class HelloWorldApp(QMainWindow): pass if __name__ == \u0026#39;__main__\u0026#39;: app = QApplication(sys.argv) window = HelloWorldApp() window.show() sys.exit(app.exec_()) Another Test\nSet some variables import sys from PyQt5.QtWidgets import QApplication, QLabel, QMainWindow, QVBoxLayout, QWidget class HelloWorldApp(QMainWindow): pass if __name__ == \u0026#39;__main__\u0026#39;: app = QApplication(sys.argv) window = HelloWorldApp() window.show() sys.exit(app.exec_()) Tip\nThis is a warning notice. Be warned!\n\u003c!DOCTYPE html\u003e ","permalink":"https://blog.cmwang.net/en/posts/2023/10/hugo-test/","summary":"Hello World!!\ngit clone https://github.com/{YOUR_USER_NAME}/hugo-site.git Let\u0026rsquo;s Do the Thing!\nimport sys from PyQt5.QtWidgets import QApplication, QLabel, QMainWindow, QVBoxLayout, QWidget class HelloWorldApp(QMainWindow): pass if __name__ == \u0026#39;__main__\u0026#39;: app = QApplication(sys.argv) window = HelloWorldApp() window.show() sys.exit(app.exec_()) Another Test\nSet some variables import sys from PyQt5.QtWidgets import QApplication, QLabel, QMainWindow, QVBoxLayout, QWidget class HelloWorldApp(QMainWindow): pass if __name__ == \u0026#39;__main__\u0026#39;: app = QApplication(sys.argv) window = HelloWorldApp() window.show() sys.exit(app.exec_()) Tip\nThis is a warning notice.","title":"Hugo Test"},{"content":" green plums\nThe time leading up to and following the Tomb Sweeping Festival has become a significant rite for my family in recent years. We purchase green plums in advance from local farmers in Xinyi Township and use them to create preserved plums. However, due to insufficient rainfall this year, the fruit was slightly less than satisfactory compared to previous years.\nMaking preserved plums is a straightforward technique that doesn‚Äôt call for specialized equipment; rather, it just takes some time and perseverance. One kilogram of green plums, 100 grams of coarse salt, and 600 grams of sugar make up the recipe, which is quite straightforward. Please carry out the actions listed below:\nRub plums with salt\nWash the plums and remove the stems one by one. Blanch the plums by rubbing them with coarse salt for at least 15 minutes until the surface becomes moist and changes color. Tap the plums with a hammer one by one until they crack slightly, but do not smash them. Soak the plums in saltwater for 8 hours, making sure that the water covers them completely. Pour out the syrup and replace it with fresh syrup every 8 hours. Repeat step 5 for 3 days until the plums are fully infused with the sugar syrup. Drain the plums of any remaining bitter syrup, replace it with the final batch of sugar syrup, and let it sit for another 3 days to complete the process. I am grateful to my family for bringing a sense of ceremony into our daily lives. This ritual makes me appreciate the beauty of life.\nCrispy Plum\n","permalink":"https://blog.cmwang.net/en/posts/2023/04/make-crispy-plum/","summary":"Making preserved plums is a straightforward technique that doesn‚Äôt call for specialized equipment","title":"Make Crispy Plum"},{"content":" Today you will have a good day. Jesus spoke to them at once, ‚ÄúDon‚Äôt be afraid,‚Äù he said. ‚ÄúTake courage! I am here!‚Äù (Mark 6:50)\n","permalink":"https://blog.cmwang.net/en/posts/2023/04/you-will-have-a-good-day/","summary":"Today you will have a good day. Jesus spoke to them at once, ‚ÄúDon‚Äôt be afraid,‚Äù he said. ‚ÄúTake courage! I am here!‚Äù (Mark 6:50)","title":"You Will Have a Good Day"},{"content":" As I reflect on my five years in this company, I realize how much has happened and how much I have grown. From having a supervisor to being a lone ranger, to forming a small team and eventually founding the Nilvana, time has flown by quickly, and I am left wondering where tomorrow will take me.\nIt seems like just yesterday that I started my journey in this company. I remember the excitement of meeting new colleagues, learning new skills, and finding my place in the organization. Over time, I became more confident and began taking on more responsibilities. I was proud of my accomplishments and the impact that I was making.\nHowever, the road was not always easy. There were times when I faced challenges that made me question my abilities and my decisions. I learned that setbacks and failures are a part of the journey, and they can be a source of growth and resilience.\nOne of the most significant moments in my journey was the creation of the Nilvana brand. It was a leap of faith, but I knew that I had a vision that could change the game. With the help of a few dedicated team members, we worked tirelessly to bring the brand to life. Seeing it take off and gain recognition was one of the proudest moments of my career.\nLooking back on my journey, I realize that time truly flies. It seems like just yesterday that I started, but five years have gone by in a blink of an eye. As I prepare to take on new challenges, I am grateful for the experiences and the lessons that I have learned.\n","permalink":"https://blog.cmwang.net/en/posts/2023/03/reflections-on-five-years-in-the-company/","summary":"As I reflect on my five years in this company, I realize how much has happened and how much I have grown","title":"Reflections on Five Years in the Company"},{"content":" This post explains how to increase the buffer memory for USB-FS devices on Linux systems in order to make full use of the imaging hardware‚Äôs capabilities. By default, USB-FS on Linux systems only allows 16 MB of buffer memory for all USB devices, which may not be sufficient for high-resolution cameras or multiple-camera set ups, resulting in image acquisition issues. To configure USB-FS and increase the buffer memory limit, the following steps should be taken:\nNote that GRUB is for desktop PC architecture. ARM embedded systems use a different bootloader, as GRUB requires a system with a BIOS, and embedded systems do not have one.\nCreate the file /etc/rc.local with the command sudo touch /etc/rc.local. This will create the file, allowing it to be edited. Change the permissions of the file with the command sudo chmod 744 /etc/rc.local. This will ensure that the file has the correct permissions to be edited. Change the buffer memory limit with the command echo 1000 \u0026amp;gt; /sys/module/usbcore/parameters/usbfs_memory_mb. This command will set the memory limit to 1000 MB, which should be enough to prevent image acquisition issues.` /etc/rc.local file contents example /etc/rc.local #!/bin/sh -e echo 1000 \u0026gt; /sys/module/usbcore/parameters/usbfs_memory_mb exit 0 After changing the memory limit, it is important to confirm the changes have been made correctly. This can be done by running the command:\ncat /sys/module/usbcore/parameters/usbfs_memory_mb which will display the current memory limit. If the limit is still 16 MB, then the changes will need to be made again. Additionally, further information about USB-FS on Linux can be found in the following sources:\nUnderstanding USBFS on Linux Increase USBFS Memory Limit in Ubuntu Change Kernel Cmdline by Edit /etc/default/grub Failed ","permalink":"https://blog.cmwang.net/en/posts/2022/12/configuring-usb-fs-for-usb3-vision-camera/","summary":"This post explains how to increase the buffer memory for USB-FS devices on Linux systems.","title":"Configuring USB-FS for USB3 Vision Camera"},{"content":" Real-time object detection with CoreML is trickier than you think.\nUsually, you have two choices to build a machine learning app for your mobile device, inference can happen either directly on-device or on cloud-based servers. It all depends on your usage scenario, there is no one-size fit all solution. In this article, we will only focus on on-device inference.\nAt WWDC 2017 Apple released first Core ML. Core ML is Apple‚Äôs machine learning framework for doing on-device inference. Core ML is not the only way to do on-device inference, there are tens of libraries and frameworks that are compatible with iOS, but that‚Äôs beyond the scope of this article. From the YOLOv7 official repository, we can get the export script to convert trained PyTorch model to Core ML format effortlessly. However, keep one thing in mind, YOLOv7 is a popular open source project, new changes and updates are added very quickly. I‚Äôm also very glad to send a PR to improve the export script last night due to this writing üòÉ.After you got the exported Core ML models, no kidding, you have tons of things in your todo list. Matthijs Hollemans has already written an insightful article in his blog, be sure to checkout and support his efforts! Here is my short list:\nConfigure your Core ML model in a particular way. You can either append NMS to your model or write a lot of additional Swift code. IMHO, this is the most difficult part if you know nothing about the object detection model. Specify camera resolution, don‚Äôt simply select the highest resolution available if your app doesn‚Äôt require it. Resize or crop your input image to fit network input dimension, it depends on your application. Feed modified images to your model in a correct orientation. Fix Vision‚Äôs weird orin. Convert bounding boxes coordinate system for display. This is also a trickier part, you need some iOS development experiences and a pencil for calculation üòé. According to Hollemans‚Äôs article, there are at least 5 different coordinate systems you need to take care, not to mention how to handle real-time capturing correctly and efficiently is also non-trivial. You can follow these two articles to learn how to create a custom camera view.\nApple Developer Documentation | Recognizing Objects in Live Capture\nCreating a Custom Camera View | CodePath iOS Cliffnotes At the latest WWDC 2022, Apple introduced even more performance tools to its CoreML toolchain, now you can check your model‚Äôs metadata via performance reports and Core ML Instrument without writing any code. You can also use computeUnits = .cpuAndNeuralEngine if you don‚Äôt want to use the GPU but always force the model to run on the CPU and ANE if available.\nPrefer CPU and ANE instead of GPU.\nYou can learn more about ANE from the following repository, thank you again Hollemans.\nGitHub - hollance/neural-engine: Everything we actually know about the Apple Neural Engine (ANE)\nHere are snapshots from my model‚Äôs performance reports.\nYou can evaluate your model via drag-and-drop image files.\nThere is no significant inference speed differences among quantization models, but the model size only about half the size. It‚Äôs a good thing for your mobile applications. No inference speed improved. (Left is FP32, right is FP16)\nHalf the size of the FP32 model.\nFinally, you have a working YOLOv7 Core ML model on the iOS devices, be careful of the heatüî•. Happy coding!\nYolov7-tiny on iPad Mini 6. Copyrights of BBIBBI Dance Practice belongs Kakao Entertainment.\nReferences Recognizing Objects in Live Capture How to display Vision bounding boxes Creating a Custom Camera View The Neural Engine‚Ää‚Äî‚Ääwhat do we know about it? WongKinYiu/yolov7 WWDC2022‚Ää‚Äî‚ÄäOptimize your Core ML usage MobileNetV2 + SSDLite with Core ML yolov5‚Ää‚Äî‚ÄäCoreML Tools ","permalink":"https://blog.cmwang.net/en/posts/2022/08/execute-yolov7-model-on-ios-devices/","summary":"Real-time object detection with CoreML is trickier than you think.","title":"Execute YOLOv7 model on iOS devices"},{"content":" YOLOv7 TensorRT Performance Benchmarking.\nObject detection is one of the fundamental problems of computer vision. Instead of region detection and object classification separately in two stage detectors, object classification and bounding-box regression are done directly without using pre-generated region proposals in one stage detectors. YOLO (You Only Look Once) is one of the representative models of one-stage architecture. The YOLO family has continued to evolve since 2016, this summer we‚Äôve got its latest update to version 7.\nGitHub - WongKinYiu/yolov7: Implementation of paper - YOLOv7: Trainable bag-of-freebies sets new‚Ä¶\nIf you are trying to learn how to train your model on a custom dataset from the beginning, there are already many tutorials, notebooks and videos available online. In Nilvana, we really care about its real-world performance on the embedded devices, especially Nvidia Jetson family devices. So we conducted a series performance testing of YOLOv7 variants models on different devices, from cloud GPUs A100 to the latest tiny powerhouse AGX Orin.\nNVIDIA¬Æ Jetson AGX Orin‚Ñ¢ Developer Kit: smallest and most powerful AI edge computer\nThe main reason YOLOv7 is more accurate, compare to other models with similar AP, YOLOv7 has only about half computational cost.‚Ää‚Äî‚ÄäWongKinYiu \u0026gt; Input and Output shape of YOLOv7 (80 class)\nAccording to the results table, Xavier NX can run YOLOv7-tiny model pretty well. AGX Orin can even run YOLOv7x model more than 30 FPS, it‚Äôs amazing!\nEnd-to-End Performance on 1080P video, Batch Size=1\nPerformance Benchmarking Playlist\n","permalink":"https://blog.cmwang.net/en/posts/2022/07/performance-benchmarking-of-yolov7-tensorrt/","summary":"I will conduct a series performance test of YOLOv7 variants models from cloud GPUs to the latest powerhouse AGX Orin in this post.","title":"Performance Benchmarking of YOLOv7 TensorRT"},{"content":"Expanding Our Horizons Overview Object Encapsulation Inheritance Handling variation Commonality and variability analysis Abstract class and its derived classes Objects: Traditional view and new view Traditional View Data with methods - smart data too narrow from implementation perspective Broad View From conceptual perspective an object is an entity that has responsibilities (Ë≤¨‰ªª), these responsibilities define the behavior of the object. Or an entity that has specific behavior (ÁâπÂÆöË°åÁÇ∫). Focus on intention/motivation not implementation This view enables us to build software in two steps:\nMake a preliminary design without worrying about all the details involved. Implement the design. The reason this works is that we only have to focus on the object‚Äôs public interface ‚Äî the communication window through which I ask the object to do something.\nHiding implementations behind interfaces essentially decouples them from the using objects.\nEncapsulation: Traditional view and new view Traditional View data hiding Broad View any kind of hiding Implementations (data, methods..) Drived classes (Encapsulation of type is achieved when there is an abstract class with derivations (or an interface with implementations) that are used polymorphically) Design details Instantiation rules (ex. creational patterns) Advantage It gives us a better way to split up (decompose) our programs. The encapsulating layers become the interfaces we design to. (Â∞ÅË£ùÂ±§ÊàêÁÇ∫Ë®≠Ë®àÈúÄË¶ÅÈÅµÂæ™ÁöÑ‰ªãÈù¢)\nBy encapsulating different kinds of subclasses (encapsulation of type), we can add new ones without changing any of the client programs using them. (GoF typically means when they mention encapsulation)\nInheritance Traditional View reuse of classes achived by creating classes and then deriving new (spcialized) classes bases on these base (generalized) classes Broad View using inheritance for specialization, however can cause weak cohesion reduces possibility of reuse does not scale well with variation to classify classes as things that behave the same way. (placeholder) Find What Is Varying and Encapsulate It Consider what should be variable in your design. This approach is the opposite of focusing on the cause of redesign. Instead of considering what might force a change to a design, consider what you want to be able to change without redesign. The focus here is on encapsulating the concept that varies, a theme of many design patterns. \u0026ndash; GoF, Design Patterns\nMore about GoF\u0026rsquo;s Encapsulation Design Patterns use inheritance to classify variations in behaviors. Hiding classes with an abstract class or interface ‚Äî type encapsulation. Containing a reference of this abstract class or interface type (aggregation) hides these derived classes that represent variations in behavior. In effect, many design patterns use encapsulation to create layers between objects. Containing variation in data vs containing variation in behavior Handling variation in data Have a data member that tells me what type of movement my object has. Have two different types of Animals (both derived from the base Animal class) ‚Äî one for walking and one for flying. Handling variation in behavior with objects Using objects to contain variation in attributes and using objects to contain variation in behavior are very similar. Don\u0026rsquo;t afraid.\nCommonality and Variability Identify where things vary (commonality analysis) and then identify how they vary (variability analysis).\nCommonality analysis is the search for common elements that helps us understand how family members are the same.\nVariability analysis reveals how family members vary. Variability only makes sense within a given commonality.\nEx. Whiteboard marker, pencil, ballpoint pen\nCommonality: writing instrument Variability: material to write, shape.. Commonality and Variability and Abstract class Commonality analysis seeks structure that is unlikely to change over time, while variability analysis captures structure that is likely to change. Variability analysis makes sense only in terms of the context defined by the associated commonality analysis. In other words, if variations are the specific concrete cases in the domain, commonality defines the concepts in the domain that tie them together. The common concepts will be represented by abstract classes. The variations found by variability analysis will be implemented by the concrete classes.\nRelationship between Commonality and Variability, perspectives, and abstract classes Benefits of using abstract classes for specialization Two-Step Procedure for Design Ask yourself:\nWhen defining an abstract class (commonality): What interface is needed to handle all the responsibilities (core concepts from the conceptual perspective) of this class? When defining derived classes: Given this particular implementation (this variation), how can I implement it (variation) with the given specification? Take away Think object-oriented in a broad way.\nObject: an entity that has responsibilities (specific behavior) Encapsulation: any kind of hiding (instantiation rule, type..) Inheritance: use for specialization and classify classes as things that behave the same way. Find what is varying and encapsulate it (in behavior).\nCommonality, variability and abstract class: use inheritance to classify variations in behaviors.\n","permalink":"https://blog.cmwang.net/en/posts/2017/01/design-patterns-explained-ch8/","summary":"\u003ch1 id=\"expanding-our-horizons\"\u003eExpanding Our Horizons\u003c/h1\u003e\n\u003ch2 id=\"overview\"\u003eOverview\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003eObject\u003c/li\u003e\n\u003cli\u003eEncapsulation\u003c/li\u003e\n\u003cli\u003eInheritance\u003c/li\u003e\n\u003cli\u003eHandling variation\u003c/li\u003e\n\u003cli\u003eCommonality and variability analysis\u003c/li\u003e\n\u003cli\u003eAbstract class and its derived classes\u003c/li\u003e\n\u003c/ul\u003e","title":"Design Patterns Explained - CH8"}]