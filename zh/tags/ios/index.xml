<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>ios on Liberation Notes</title>
    <link>https://blog.cmwang.net/zh/tags/ios/</link>
    <description>Recent content in ios on Liberation Notes</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>zh</language>
    <lastBuildDate>Sat, 06 Aug 2022 11:04:15 +0000</lastBuildDate><atom:link href="https://blog.cmwang.net/zh/tags/ios/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Execute YOLOv7 model on iOS devices</title>
      <link>https://blog.cmwang.net/zh/posts/2022/08/execute-yolov7-model-on-ios-devices/</link>
      <pubDate>Sat, 06 Aug 2022 11:04:15 +0000</pubDate>
      
      <guid>https://blog.cmwang.net/zh/posts/2022/08/execute-yolov7-model-on-ios-devices/</guid>
      <description>Real-time object detection with CoreML is trickier than you think.</description>
      <content:encoded><![CDATA[<p><img loading="lazy" src="images/1.jpeg" alt="image"  />

Real-time object detection with CoreML is trickier than you think.</p>
<p>Usually, you have two choices to build a machine learning app for your mobile device, <strong>inference</strong> can happen either directly on-device or on cloud-based servers. It all depends on your usage scenario, there is no one-size fit all solution. In this article, we will only focus on on-device inference.</p>
<p>At WWDC 2017 Apple released first Core ML. Core ML is Appleâ€™s machine learning framework for doing on-device inference. Core ML is not the only way to do on-device inference, there are tens of libraries and frameworks that are compatible with iOS, but thatâ€™s beyond the scope of this article. From the <a href="https://github.com/WongKinYiu/yolov7">YOLOv7 official repository</a>, we can get the <a href="https://github.com/WongKinYiu/yolov7/blob/main/export.py">export script</a> to convert trained PyTorch model to Core ML format effortlessly. However, keep one thing in mind, YOLOv7 is a popular open source project, new changes and updates are added very quickly. Iâ€™m also very glad to send a <a href="https://github.com/WongKinYiu/yolov7/pull/434/commits">PR</a> to improve the export script last night due to this writing ğŸ˜ƒ.After you got the exported Core ML models, no kidding, you have tons of things in your todo list. <a href="https://twitter.com/mhollemans">Matthijs Hollemans</a> has already written an insightful <a href="https://machinethink.net/blog/bounding-boxes/">article</a> in his blog, be sure to checkout and <a href="https://leanpub.com/coreml-survival-guide">support his efforts</a>! Here is my short list:</p>
<ul>
<li>Configure your Core ML model in a particular way. You can either append NMS to your model or write a lot of additional Swift code. IMHO, this is the most difficult part if you know nothing about the object detection model.</li>
<li>Specify camera resolution, donâ€™t simply select the highest resolution available if your app doesnâ€™t require it.</li>
<li>Resize or crop your input image to fit network input dimension, it depends on your application.</li>
<li>Feed modified images to your model in a correct orientation.</li>
<li>Fix Visionâ€™s weird orin.</li>
<li>Convert bounding boxes coordinate system for display. This is also a trickier part, you need some iOS development experiences and a pencil for calculation ğŸ˜.</li>
</ul>
<p>According to Hollemansâ€™s article, there are at least 5 different coordinate systems you need to take care, not to mention how to handle real-time capturing correctly and efficiently is also non-trivial. You can follow these two articles to learn how to create a custom camera view.</p>
<p><a href="https://developer.apple.com/documentation/vision/recognizing_objects_in_live_capture">Apple Developer Documentation | Recognizing Objects in Live Capture</a></p>
<p><a href="https://guides.codepath.com/ios/Creating-a-Custom-Camera-View">Creating a Custom Camera View | CodePath iOS Cliffnotes</a>
At the latest <a href="https://developer.apple.com/videos/play/wwdc2022/10027/">WWDC 2022</a>, Apple introduced even more performance tools to its CoreML toolchain, now you can check your modelâ€™s metadata via <strong>performance reports</strong> and <strong>Core ML Instrument</strong> without writing any code. You can also use <code>computeUnits = .cpuAndNeuralEngine</code> if you donâ€™t want to use the GPU but always force the model to run on the CPU and ANE if available.</p>
<p><img loading="lazy" src="images/2.png#layoutTextWidth" alt="image"  />

Prefer CPU and ANE instead of GPU.</p>
<p>You can learn more about ANE from the following repository, thank you again Hollemans.</p>
<p><a href="https://github.com/hollance/neural-engine">GitHub - hollance/neural-engine: Everything we actually know about the Apple Neural Engine (ANE)</a></p>
<p>Here are snapshots from my modelâ€™s performance reports.</p>
<p><img loading="lazy" src="images/3.png#layoutTextWidth" alt="image"  />

You can evaluate your model via drag-and-drop image files.</p>
<p>There is no significant inference speed differences among quantization models, but the model size only about half the size. Itâ€™s a good thing for your mobile applications.
<img loading="lazy" src="images/4.png#layoutTextWidth" alt="image"  />
</p>
<p><img loading="lazy" src="images/5.png#layoutTextWidth" alt="image"  />

No inference speed improved. (Left is FP32, right is FP16)</p>
<p><img loading="lazy" src="images/6.png#layoutTextWidth" alt="image"  />
</p>
<p><img loading="lazy" src="images/7.png#layoutTextWidth" alt="image"  />

Half the size of the FP32 model.</p>
<p>Finally, you have a working YOLOv7 Core ML model on the iOS devices, be careful of the heatğŸ”¥. Happy coding!</p>
<p><img loading="lazy" src="images/8.gif#layoutTextWidth" alt="image"  />

<strong>Yolov7-tiny on iPad Mini 6.</strong> Copyrights of <a href="https://youtu.be/hngml3y2Rq8">BBIBBI Dance Practice</a> belongs Kakao Entertainment.</p>
<h3 id="references">References</h3>
<ul>
<li><a href="https://developer.apple.com/documentation/vision/recognizing_objects_in_live_capture">Recognizing Objects in Live Capture</a></li>
<li><a href="https://machinethink.net/blog/bounding-boxes/">How to display Vision bounding boxes</a></li>
<li><a href="https://guides.codepath.com/ios/Creating-a-Custom-Camera-View">Creating a Custom Camera View</a></li>
<li><a href="https://github.com/hollance/neural-engine">The Neural Engineâ€Šâ€”â€Šwhat do we know about it?</a></li>
<li><a href="https://github.com/WongKinYiu/yolov7">WongKinYiu/yolov7</a></li>
<li><a href="https://developer.apple.com/videos/play/wwdc2022/10027/">WWDC2022â€Šâ€”â€ŠOptimize your Core ML usage</a></li>
<li><a href="https://machinethink.net/blog/mobilenet-ssdlite-coreml/">MobileNetV2 + SSDLite with Core ML</a></li>
<li><a href="https://github.com/dbsystel/yolov5-coreml-tools">yolov5â€Šâ€”â€ŠCoreML Tools</a></li>
</ul>
]]></content:encoded>
    </item>
    
    <item>
      <title>æ¨¡æ“¬å¥§é‹äº”ç’°ç„¡äººæ©Ÿç‡ˆå…‰ç§€</title>
      <link>https://blog.cmwang.net/zh/posts/2022/03/simulated-olympic-rings-drone-light-show/</link>
      <pubDate>Sat, 19 Mar 2022 12:37:27 +0000</pubDate>
      
      <guid>https://blog.cmwang.net/zh/posts/2022/03/simulated-olympic-rings-drone-light-show/</guid>
      <description>ç·´ç¿’ CALayer &amp;amp; Gesture Recognizer</description>
      <content:encoded><![CDATA[<p><strong>å­¸ç¿’ç›®æ¨™ï¼š</strong></p>
<ul>
<li>Gesture Recognizer</li>
<li>CALayer &amp; UIBezierPath</li>
<li>Class &amp; Function</li>
</ul>
<p><strong>ç·´ç¿’æˆæœï¼š</strong></p>
<p><img loading="lazy" src="images/1.gif" alt="image"  />
</p>
<p><strong>åŸå§‹ç¢¼ï¼š</strong></p>
<p><a href="https://github.com/takawang/olympic-drone">GitHub - takawang/olympic-drone</a></p>
<p><strong>ç·£èµ·ï¼š</strong></p>
<p>é€™æ¬¡çš„ç·´ç¿’ï¼Œä¸»è¦æ˜¯çœ‹åˆ°ä¸‹é¢é€™ç¯‡æ–‡ç« çš„å•Ÿç™¼ï¼Œä¸éæˆ‘ç°¡åŒ–äº†è§¸æ¨¡æ•£é–‹èˆ‡åŠ é€Ÿåº¦é€™å€‹éƒ¨åˆ†ï¼Œä¸»è¦æ˜¯ç·´ç¿’ swift å¯¦ä½œã€‚</p>
<p><a href="https://velog.io/@heekang/Vanilla-JS-%ED%8F%89%EC%B0%BD%EC%98%AC%EB%A6%BC%ED%94%BD-%EB%93%9C%EB%A1%A0%EC%87%BC-%EB%A7%8C%EB%93%A4%EA%B8%B0">[Vanilla JS] í‰ì°½ì˜¬ë¦¼í”½ ë“œë¡ ì‡¼ ë§Œë“¤ê¸°</a></p>
<p><strong>èªªæ˜ï¼š</strong></p>
<p>å¯¦ä½œçš„æ§‹æƒ³é‚„ç®—ç›´è¦ºï¼Œé™¤äº†<strong>é•·å£“é€£çºŒè§¸ç™¼</strong>éœ€è¦ç ”ç©¶ä¸€ä¸‹ã€‚</p>
<ol>
<li>ä½¿ç”¨ javascript çš„ <strong>canvas getImageData()</strong> å°‡ SVG å­˜æˆ json æª” (å¯åƒè€ƒä¸Šæ–‡é€£çµä¸­çš„ <strong>getDotPos</strong> æ–¹æ³•ã€‚</li>
<li>åœ¨ swift ä¸­ï¼Œé€éè®€å– json æª”ï¼Œå­˜æˆåº§æ¨™é»çš„é¡åˆ¥é™£åˆ—ï¼Œé€™å…©æ­¥é©Ÿä¹Ÿå¯ä»¥ç”¨ä¾†ç•«å…¶ä»–åœ–æ¡ˆï¼Œä¸ç”¨æ‰‹å·¥ä¸€ç›´æé»ã€‚</li>
<li>è¨­è¨ˆ Drone é¡åˆ¥ï¼Œè®“ Drone <strong>åˆå§‹ä½ç½®</strong>æ˜¯éš¨æ©Ÿçš„ï¼Œ<strong>ç›®çš„ä½ç½®</strong>æ˜¯å‰›å‰›è¼‰å…¥çš„åœ–å½¢åº§æ¨™ã€‚</li>
<li>å°‡å…©åº§æ¨™åˆ†æˆ 20 ä»½ï¼ŒæŒçºŒæŒ‰å£“é€£çºŒè§¸ç™¼æ™‚ï¼Œä¸€æ¬¡ç§»å‹•ä¸€ä»½ã€‚</li>
<li>ç•¶åˆ°é”ç›®çš„åº§æ¨™æ™‚ï¼Œæ”¹è®Š Drone çš„é¡è‰²ã€‚</li>
</ol>
]]></content:encoded>
    </item>
    
    <item>
      <title>iPadä¸Šçš„å½±ç‰‡æ’­æ”¾å™¨</title>
      <link>https://blog.cmwang.net/zh/posts/2022/03/practice-avplayer-view-controller/</link>
      <pubDate>Wed, 16 Mar 2022 10:32:00 +0000</pubDate>
      
      <guid>https://blog.cmwang.net/zh/posts/2022/03/practice-avplayer-view-controller/</guid>
      <description>ç·´ç¿’ä½¿ç”¨ AVPlayerViewController è£½ä½œå‹•æ…‹å­—å¹•å½±ç‰‡</description>
      <content:encoded><![CDATA[<p><strong>å­¸ç¿’ç›®æ¨™ï¼š</strong></p>
<ul>
<li>AVPlayer</li>
<li>NotificationCenter</li>
<li>Timer</li>
<li>Swift Optional</li>
<li>Function</li>
<li>Switch Case</li>
</ul>
<p><strong>ç·´ç¿’æˆæœï¼š</strong></p>
<p><strong>åŸå§‹ç¢¼ï¼š</strong></p>
<p><a href="https://github.com/takawang/letter-song">GitHub - takawang/letter-song</a></p>
<p><strong>æ­Œè©ï¼š</strong>
<code>ä»Šæ™š æˆ‘æƒ³å°‡é‚£å¤©çš„è¢ç« é€åˆ°ä½ çš„çª—å‰ å«æ„æ˜¯&amp;#34;æˆ‘æ„›ä½ &amp;#34; æˆ‘æƒ³èµ·æˆ‘å€‘çš„åˆå» ä¸ç®¡ä½•æ™‚ åªè¦é–‰ä¸Šé›™çœ¼ å°±èƒ½å¥”å‘é‚£æœ€é™é çš„åœ°æ–¹ å°±åƒæˆ‘åœ¨è¢«æµªæ¿¤æ¹§ä¾†çš„æ²™ä¸Šå¯«ä¸‹çš„å­—è·¡èˆ¬ ä½ æ„Ÿè¦ºä¹Ÿæœƒåƒä»–å€‘é‚£æ¨£å¾æ­¤æ¶ˆå¤± ç¸½æ˜¯æƒ³å¿µ é›–ç„¶ç„¡æ³•å°‡æˆ‘å¿ƒè£¡æ‰€æœ‰çš„è©±èªï¼Œèªªçµ¦ä½ è½ ä½†é‚£è£¡é ­ï¼Œå…¨éƒ½æ˜¯&amp;#34;æˆ‘æ„›ä½ &amp;#34; æˆ‘ä½•å¾·ä½•èƒ½ èƒ½æ“æœ‰åç‚ºä½ çš„é€™ä»½å¹¸é‹ è‹¥æˆ‘å€‘ç¾åœ¨èƒ½æœ›è‘—å°æ–¹ è©²æœ‰å¤šç¾å¥½ å°±åƒæˆ‘åœ¨è¢«æµªæ¿¤æ¹§ä¾†çš„æ²™ä¸Šå¯«ä¸‹çš„å­—è·¡èˆ¬ ä½ æ„Ÿè¦ºä¹Ÿæœƒåƒä»–å€‘é‚£æ¨£å¾æ­¤æ¶ˆå¤± é‚„æ˜¯æƒ³å¿µ é›–ç„¶ç„¡æ³•å°‡æˆ‘å¯«åœ¨æ—¥è¨˜ä¸Šçš„ä¸€å­—ä¸€å¥ éƒ½å‘Šè¨´ä½  ä½†å­—å­—å¥å¥éƒ½ä»£è¡¨è‘—æˆ‘æ„›ä½  ä»Šæ™š æˆ‘æƒ³å°‡é‚£å¤©çš„è¢ç« é€åˆ°ä½ çš„çª—å‰ å¸Œæœ›ä»Šå¤œçš„ä½ æœ‰å€‹ç¾å¥½çš„å¤¢</code></p>
<p><strong>å½±ç‰‡ä¾†æºï¼š</strong><a href="https://www.youtube.com/watch?v=BzYnNdJhZQw">https://www.youtube.com/watch?v=BzYnNdJhZQw</a></p>
<p><strong>æ­Œè©ä¾†æºï¼š</strong><a href="https://tinyurl.com/2p9c5d8a">https://tinyurl.com/2p9c5d8a</a></p>
<p><strong>Storyboard:</strong></p>
<p><img loading="lazy" src="images/1.png" alt="image"  />

<strong>éç¨‹ï¼š</strong></p>
<p>æœ¬æ¬¡çµåˆå¹¾å€‹é …ç›®ï¼Œç·´ç¿’ SDK ä½¿ç”¨èˆ‡ç†Ÿæ‚‰ Swift èªæ³•ï¼Œåªç”¨ function print ç„¡æ³•æ»¿è¶³è‡ªå·±çš„æœŸå¾…ï¼Œæ‰€ä»¥ä¸Šç¶²æŸ¥äº†ä¸€äº›è³‡æ–™ä¸¦æ­é…ä¸Šèª²å…§å®¹è£½ä½œï¼Œå°‡ AVPlayerViewController åŠ é€² UIView æ‡‰è©²æ˜¯æœ€å›°é›£çš„éƒ¨åˆ†ï¼ŒåŸæœ¬è€ƒæ…®ä½¿ç”¨ Container View å»åšï¼Œä½†é‚„ä¸çŸ¥é“æ€éº¼æ§åˆ¶å¤šå€‹ Controllerï¼Œä¸¦ä¸”è·Ÿ Storyboard ä¸²æ¥èµ·ä¾†ã€‚</p>
<p>ä¸»è¦åœ¨ <strong>iPad Pro 12.9</strong> èˆ‡ <strong>iPad Mini 6</strong> å¯¦æ©Ÿæ¸¬è©¦éï¼ŒiPhone ç‰ˆé¢æœƒçœ‹ä¸åˆ°æ­Œè©ã€‚</p>
<ol>
<li>æ–°å¢ UIViewï¼Œå–å¾— IBOutlet -&gt; Line 12</li>
<li>å°‡ AVPlayerViewController åŠ é€² UIView -&gt; Line 50</li>
<li>ç”¨ IBAction æ§åˆ¶ å½±ç‰‡æ’­æ”¾ä¸¦é–‹å§‹è¨ˆæ™‚ -&gt; Line59</li>
<li>è¨»å†Š View ç”Ÿå‘½é€±æœŸèˆ‡å½±ç‰‡æ’­æ”¾çµæŸçš„é€šçŸ¥ -&gt;Line 20~28</li>
</ol>
<p><strong>å…è²¬è²æ˜ï¼š</strong></p>
<p>å½±ç‰‡ä½¿ç”¨åªç‚ºäº†å­¸ç¿’ç›®çš„ï¼Œæ²’æœ‰ç‡Ÿåˆ©è¡Œç‚ºï¼Œç‰ˆæ¬Šå±¬æ–¼åŸå‰µä½œæ–¹ <a href="https://www.youtube.com/channel/UCweOkPb1wVVH0Q0Tlj4a5Pw">1theK</a>.</p>
]]></content:encoded>
    </item>
    
    <item>
      <title>ä»¿è£½ iPad ä¸Šçš„ Apple Music</title>
      <link>https://blog.cmwang.net/zh/posts/2022/03/replicate-apple-music-ipad-os-app/</link>
      <pubDate>Sun, 13 Mar 2022 11:35:11 +0000</pubDate>
      
      <guid>https://blog.cmwang.net/zh/posts/2022/03/replicate-apple-music-ipad-os-app/</guid>
      <description>ç·´ç¿’ä½¿ç”¨ Scroll View &amp;amp; Table View ä»¿è£½ iPad ä¸Šçš„ Apple Music App</description>
      <content:encoded><![CDATA[<p><strong>å­¸ç¿’ç›®æ¨™ï¼š</strong></p>
<ul>
<li>ç·´ç¿’ Scroll View</li>
<li>ç·´ç¿’ Navigation Controller</li>
<li>ç·´ç¿’ Table View Controller &amp; Static Cells</li>
<li>ç·´ç¿’ Label &amp; Button æ¨£å¼</li>
<li>ç·´ç¿’ segue &amp; SF Symbols</li>
</ul>
<p><strong>ç·´ç¿’æˆæœï¼š</strong></p>
<p><img loading="lazy" src="images/1.gif" alt="ç·´ç¿’æˆæœéŒ„å½±æˆªåœ–"  />
</p>
<p><a href="https://github.com/takawang/fake-music">GitHub - takawang/fake-music</a></p>
<p><strong>éç¨‹ï¼š</strong></p>
<p>é€éç›´æ¥æ“·å– iPad Pro 12.9 å¯¦æ©Ÿç•«é¢ï¼Œåœ¨ story board ä¸Šä»¿è£½ Apple Music ä¸Šçš„ IU å°ˆè¼¯é é¢ï¼Œæ›´é€²ä¸€æ­¥ç†Ÿæ‚‰äº† Navigation Controller çš„ä½¿ç”¨ï¼Œå› ç‚ºé‚„ä¸æœƒè£½ä½œç¬¦åˆå„ç¨®æ©Ÿå‹çš„ç‰ˆé¢ï¼Œæ‰€ä»¥å°±ç›´æ¥ hardcode åªæ”¯æ´ 12.9 æ°´å¹³çš„ç‰ˆé¢ï¼Œæ˜¯æœ‰é»å‘†ç‰ˆä½†ä¹Ÿæ”¶ç©«ä¸å°‘ã€‚</p>
<p><img loading="lazy" src="images/2.png" alt="image"  />

é™å®šåªæ”¯æ´ iPad èˆ‡ Landscape</p>
<p><img loading="lazy" src="images/3.png" alt="image"  />

ä¿®æ”¹ Product Name æ”¹è®Š APP åœ¨æ¡Œé¢çš„åç¨±</p>
<p>éœ€è¦æ³¨æ„çš„åœ°æ–¹æ˜¯ï¼Œåœ¨ story board ä¸Šçš„æ˜¯ pointï¼Œä½†è¢å¹•æˆªåœ–æ˜¯ pixelï¼Œå› ç‚ºæ²’æœ‰ zeplin ä¹‹é¡çš„è¨­è¨ˆç¨¿ï¼Œæ‰€ä»¥èŠ±æœ€å¤šæ™‚é–“é‚„æ˜¯åœ¨æŠ“å–èˆ‡å°é½Šåº§æ¨™ï¼Œä½ç½®ä¸æ˜¯ pixel perfectï¼Œä½†ç·´ç¿’çš„ç›®çš„æ‡‰è©²æ˜¯é”åˆ°äº†ã€‚</p>
<p><img loading="lazy" src="images/4.png" alt="image"  />

Point èˆ‡ pixel æ˜¯ 2x scale çš„é—œä¿‚</p>
<p><strong>æœ€è¿‘æ’­æ”¾</strong>çš„é»é¸ï¼Œåƒè€ƒä¸‹é¢æ–‡ç« ï¼ŒåŸæœ¬ä½¿ç”¨ <strong>tap gesture recognizer</strong>ï¼Œä¸éå¤§æ¦‚å› ç‚ºæˆ‘å°‡ navigation controller æ”¾éŒ¯åœ°æ–¹ï¼Œç¸½æ˜¯çœ‹ä¸åˆ° back é¸å–®ï¼Œæœ€å¾Œé¸æ“‡<strong>é€æ˜æŒ‰éˆ•ç–Šåœ¨ä¸Šæ–¹</strong>çš„åšæ³•ã€‚</p>
<p><a href="https://medium.com/%E5%BD%BC%E5%BE%97%E6%BD%98%E7%9A%84-swift-ios-app-%E9%96%8B%E7%99%BC%E5%95%8F%E9%A1%8C%E8%A7%A3%E7%AD%94%E9%9B%86/%E5%88%A9%E7%94%A8-segue-%E5%AF%A6%E7%8F%BE%E5%9C%96%E7%89%87%E9%BB%9E%E9%81%B8%E7%9A%84%E6%8F%9B%E9%A0%81%E5%8A%9F%E8%83%BD-84f6991d74d4">åˆ©ç”¨ segue å¯¦ç¾åœ–ç‰‡é»é¸çš„æ›é åŠŸèƒ½</a></p>
<p><img loading="lazy" src="images/5.png" alt="image"  />

Navigation Controller éœ€è¦å¾ç¬¬ä¸€å€‹ç•«é¢é•·å‡ºä¾†ï¼Œè€Œä¸æ˜¯ç¬¬äºŒé </p>
<p>ä¸»ç•«é¢æœ‰å…©å€‹ scoll viewï¼Œåƒè€ƒé€£çµæä¾›çš„æŠ€å·§å°±å¯ä»¥æ–¹ä¾¿è£½ä½œï¼Œè¨˜å¾—å°‡<strong>æ°´å¹³èˆ‡å‚ç›´çš„æ²è»¸æŒ‡ç¤º</strong>é—œé–‰ï¼Œé€™æ¨£ç•«é¢æœƒæ¯”è¼ƒæ“¬çœŸã€‚</p>
<p><a href="https://medium.com/%E5%BD%BC%E5%BE%97%E6%BD%98%E7%9A%84-swift-ios-app-%E9%96%8B%E7%99%BC%E5%95%8F%E9%A1%8C%E8%A7%A3%E7%AD%94%E9%9B%86/%E5%9C%A8-storyboard-%E8%A8%AD%E5%AE%9A-content-size-%E5%AF%A6%E7%8F%BE%E6%B0%B4%E5%B9%B3%E6%8D%B2%E5%8B%95%E7%9A%84-scroll-view-2710fa247293">è¨­å®š content sizeï¼Œå¯¦ç¾æ°´å¹³æ²å‹•ï¼Œä¸Šä¸‹æ²å‹•å’Œåˆ†é çš„ scroll view</a></p>
<p><img loading="lazy" src="images/6.png" alt="image"  />

å–æ¶ˆå‹¾é¸æ°´å¹³èˆ‡å‚ç›´ indicatorï¼Œä½¿å¾— scroll view æ²å‹•æ™‚ä¸æœƒé¡¯ç¤ºå·è»¸</p>
<p>Table View çš„ç¬¬ä¸€å€‹ Cell èˆ‡æœ€å¾Œå…©å€‹ Cell ç‚ºäº†ç‰ˆé¢èª¿æ•´ï¼Œä¸­é–“çš„æ­Œæ›²åˆ—è¡¨åªéœ€è¦é€éä¿®æ”¹ row æ•¸å°±å¯ä»¥è¤‡è£½ï¼Œå†ä¿®æ”¹å°é¢ç…§èˆ‡ label å³å¯ã€‚</p>
]]></content:encoded>
    </item>
    
  </channel>
</rss>
