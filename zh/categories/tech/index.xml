<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>Tech on Liberation Notes</title>
    <link>https://blog.cmwang.net/zh/categories/tech/</link>
    <description>Recent content in Tech on Liberation Notes</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>zh</language>
    <lastBuildDate>Tue, 10 Oct 2023 16:33:39 +0800</lastBuildDate><atom:link href="https://blog.cmwang.net/zh/categories/tech/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>æ¬å› Github Pages</title>
      <link>https://blog.cmwang.net/zh/posts/2023/10/blog-reboot-2023/</link>
      <pubDate>Tue, 10 Oct 2023 16:33:39 +0800</pubDate>
      
      <guid>https://blog.cmwang.net/zh/posts/2023/10/blog-reboot-2023/</guid>
      <description>2023 é‡æ–°å›åˆ°éœæ…‹ç¶²é çš„æ‡·æŠ±ï¼ŒHugo è®“äººæ„Ÿè¦ºæ„‰å¿«</description>
      <content:encoded><![CDATA[<p>ç¶“éå¹¾å¤©çš„å¯¦é©—ï¼Œæ­£å¼å°‡ <a href="https://medium.com/@takawang">Blog</a> é‡å•Ÿåœ¨ <a href="https://pages.github.com/">Github Pages</a>ï¼Œç¸½æ˜¯æ€è€ƒè¦åœ¨ä»€éº¼å¹³å°ç´€éŒ„æˆ–è€…è©²ç†Ÿæ‚‰æ€æ¨£çš„æ ¼å¼ï¼Œéƒ½æ¯”ä¸ä¸ŠæŒçºŒè¨˜éŒ„çš„å¯¦åœ¨ã€‚</p>
<p>æœ‰å€‹å¯ä»¥æŒæ¡åœ¨è‡ªå·±æ‰‹ä¸­çš„ç­†è¨˜æœ¬ï¼Œé™¤äº†å¯ä»¥æŠ’ç™¼å¿ƒæƒ…ä¹Ÿå¯ä»¥è¨˜éŒ„å­¸ç¿’çš„é»æ»´ã€‚<a href="https://gohugo.io/">Hugo</a> çœŸçš„æ˜¯ä¸€å€‹å¾ˆæ£’çš„å·¥å…·ï¼Œæ•´é«”é«”é©—æ¯”èµ·å¤šå¹´å‰çš„ <a href="https://hexo.io/index.html">Hexo</a> å¥½å¾ˆå¤šï¼Œæ¥ä¸‹ä¾†æˆ‘æœƒåˆ†äº«å¹¾ç¯‡å¦‚ä½•è‡ªå·±å‹•æ‰‹é…ç½®çš„ç°¡å–®ç´€éŒ„ã€‚</p>
]]></content:encoded>
    </item>
    
    <item>
      <title>é…ç½®USB-FSä»¥é©ç”¨æ–¼USB3 Visionç›¸æ©Ÿ</title>
      <link>https://blog.cmwang.net/zh/posts/2022/12/configuring-usb-fs-for-usb3-vision-camera/</link>
      <pubDate>Thu, 29 Dec 2022 13:14:24 +0000</pubDate>
      
      <guid>https://blog.cmwang.net/zh/posts/2022/12/configuring-usb-fs-for-usb3-vision-camera/</guid>
      <description>é€™ç¯‡æ–‡ç« åªæœ‰è‹±æ–‡ç‰ˆçš„</description>
      <content:encoded><![CDATA[<p><img loading="lazy" src="images/1.jpeg" alt="image"  />
</p>
<p>This post explains how to increase the buffer memory for USB-FS devices on Linux systems in order to make full use of the imaging hardwareâ€™s capabilities. By default, USB-FS on Linux systems only allows 16 MB of buffer memory for all USB devices, which may not be sufficient for high-resolution cameras or multiple-camera set ups, resulting in image acquisition issues. To configure USB-FS and increase the buffer memory limit, the following steps should be taken:</p>
<p>Note that GRUB is for desktop PC architecture. ARM embedded systems use a different bootloader, as GRUB requires a system with a BIOS, and embedded systems do not have one.</p>
<ol>
<li>Create the file <code>/etc/rc.local</code> with the command <code>sudo touch /etc/rc.local</code>. This will create the file, allowing it to be edited.</li>
<li>Change the permissions of the file with the command <code>sudo chmod 744 /etc/rc.local</code>. This will ensure that the file has the correct permissions to be edited.</li>
<li>Change the buffer memory limit with the command <code>echo 1000 &amp;gt; /sys/module/usbcore/parameters/usbfs_memory_mb</code>. This command will set the memory limit to 1000 MB, which should be enough to prevent image acquisition issues.`</li>
</ol>
<h2 id="etcrclocal-file-contents-example">/etc/rc.local file contents example</h2>


<div class="terminal space shadow">
    <div class="top">
        <div class="btns">
            <span class="circle red"></span>
            <span class="circle yellow"></span>
            <span class="circle green"></span>
        </div>
        <div class="title">
            /etc/rc.local
        </div>
    </div>
    <div class="terminalbody"><div class="highlight"><pre tabindex="0" class="chroma"><code class="language-sh" data-lang="sh"><span class="line"><span class="cl"><span class="cp">#!/bin/sh -e
</span></span></span><span class="line"><span class="cl"><span class="cp"></span>
</span></span><span class="line"><span class="cl"><span class="nb">echo</span> <span class="m">1000</span> &gt; /sys/module/usbcore/parameters/usbfs_memory_mb
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="nb">exit</span>
</span></span><span class="line"><span class="cl"><span class="m">0</span>
</span></span></code></pre></div></div>
</div>
<br />

<p>After changing the memory limit, it is important to confirm the changes have been made correctly. This can be done by running the command:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-sh" data-lang="sh"><span class="line"><span class="cl">cat /sys/module/usbcore/parameters/usbfs_memory_mb
</span></span></code></pre></div><p>which will display the current memory limit. If the limit is still 16 MB, then the changes will need to be made again. Additionally, further information about USB-FS on Linux can be found in the following sources:</p>
<ul>
<li><a href="https://www.flir.asia/support-center/iis/machine-vision/application-note/understanding-usbfs-on-linux/">Understanding USBFS on Linux</a></li>
<li><a href="https://importgeek.wordpress.com/2017/02/26/increase-usbfs-memory-limit-in-ubuntu/">Increase USBFS Memory Limit in Ubuntu</a></li>
<li><a href="https://forums.developer.nvidia.com/t/change-kernel-cmdline-by-edit-etc-default-grub-failed/159831/2">Change Kernel Cmdline by Edit /etc/default/grub Failed</a></li>
</ul>
]]></content:encoded>
    </item>
    
    <item>
      <title>é€é CoreML åœ¨ iOS ä¸Šå¯¦ç¾å³æ™‚çš„ç‰©é«”åµæ¸¬</title>
      <link>https://blog.cmwang.net/zh/posts/2022/08/execute-yolov7-model-on-ios-devices/</link>
      <pubDate>Sat, 06 Aug 2022 11:04:15 +0000</pubDate>
      
      <guid>https://blog.cmwang.net/zh/posts/2022/08/execute-yolov7-model-on-ios-devices/</guid>
      <description>é€™ç¯‡æ–‡ç« åªæœ‰è‹±æ–‡ç‰ˆçš„</description>
      <content:encoded><![CDATA[<p><img loading="lazy" src="images/1.jpeg" alt="image"  />

Real-time object detection with CoreML is trickier than you think.</p>
<p>Usually, you have two choices to build a machine learning app for your mobile device, <strong>inference</strong> can happen either directly on-device or on cloud-based servers. It all depends on your usage scenario, there is no one-size fit all solution. In this article, we will only focus on on-device inference.</p>
<p>At WWDC 2017 Apple released first Core ML. Core ML is Appleâ€™s machine learning framework for doing on-device inference. Core ML is not the only way to do on-device inference, there are tens of libraries and frameworks that are compatible with iOS, but thatâ€™s beyond the scope of this article. From the <a href="https://github.com/WongKinYiu/yolov7">YOLOv7 official repository</a>, we can get the <a href="https://github.com/WongKinYiu/yolov7/blob/main/export.py">export script</a> to convert trained PyTorch model to Core ML format effortlessly. However, keep one thing in mind, YOLOv7 is a popular open source project, new changes and updates are added very quickly. Iâ€™m also very glad to send a <a href="https://github.com/WongKinYiu/yolov7/pull/434/commits">PR</a> to improve the export script last night due to this writing ğŸ˜ƒ.After you got the exported Core ML models, no kidding, you have tons of things in your todo list. <a href="https://twitter.com/mhollemans">Matthijs Hollemans</a> has already written an insightful <a href="https://machinethink.net/blog/bounding-boxes/">article</a> in his blog, be sure to checkout and <a href="https://leanpub.com/coreml-survival-guide">support his efforts</a>! Here is my short list:</p>
<ul>
<li>Configure your Core ML model in a particular way. You can either append NMS to your model or write a lot of additional Swift code. IMHO, this is the most difficult part if you know nothing about the object detection model.</li>
<li>Specify camera resolution, donâ€™t simply select the highest resolution available if your app doesnâ€™t require it.</li>
<li>Resize or crop your input image to fit network input dimension, it depends on your application.</li>
<li>Feed modified images to your model in a correct orientation.</li>
<li>Fix Visionâ€™s weird orin.</li>
<li>Convert bounding boxes coordinate system for display. This is also a trickier part, you need some iOS development experiences and a pencil for calculation ğŸ˜.</li>
</ul>
<p>According to Hollemansâ€™s article, there are at least 5 different coordinate systems you need to take care, not to mention how to handle real-time capturing correctly and efficiently is also non-trivial. You can follow these two articles to learn how to create a custom camera view.</p>
<p><a href="https://developer.apple.com/documentation/vision/recognizing_objects_in_live_capture">Apple Developer Documentation | Recognizing Objects in Live Capture</a></p>
<p><a href="https://guides.codepath.com/ios/Creating-a-Custom-Camera-View">Creating a Custom Camera View | CodePath iOS Cliffnotes</a>
At the latest <a href="https://developer.apple.com/videos/play/wwdc2022/10027/">WWDC 2022</a>, Apple introduced even more performance tools to its CoreML toolchain, now you can check your modelâ€™s metadata via <strong>performance reports</strong> and <strong>Core ML Instrument</strong> without writing any code. You can also use <code>computeUnits = .cpuAndNeuralEngine</code> if you donâ€™t want to use the GPU but always force the model to run on the CPU and ANE if available.</p>
<p><img loading="lazy" src="images/2.png#layoutTextWidth" alt="image"  />

Prefer CPU and ANE instead of GPU.</p>
<p>You can learn more about ANE from the following repository, thank you again Hollemans.</p>
<p><a href="https://github.com/hollance/neural-engine">GitHub - hollance/neural-engine: Everything we actually know about the Apple Neural Engine (ANE)</a></p>
<p>Here are snapshots from my modelâ€™s performance reports.</p>
<p><img loading="lazy" src="images/3.png#layoutTextWidth" alt="image"  />

You can evaluate your model via drag-and-drop image files.</p>
<p>There is no significant inference speed differences among quantization models, but the model size only about half the size. Itâ€™s a good thing for your mobile applications.
<img loading="lazy" src="images/4.png#layoutTextWidth" alt="image"  />
</p>
<p><img loading="lazy" src="images/5.png#layoutTextWidth" alt="image"  />

No inference speed improved. (Left is FP32, right is FP16)</p>
<p><img loading="lazy" src="images/6.png#layoutTextWidth" alt="image"  />
</p>
<p><img loading="lazy" src="images/7.png#layoutTextWidth" alt="image"  />

Half the size of the FP32 model.</p>
<p>Finally, you have a working YOLOv7 Core ML model on the iOS devices, be careful of the heatğŸ”¥. Happy coding!</p>
<p><img loading="lazy" src="images/8.gif#layoutTextWidth" alt="image"  />

<strong>Yolov7-tiny on iPad Mini 6.</strong> Copyrights of <a href="https://youtu.be/hngml3y2Rq8">BBIBBI Dance Practice</a> belongs Kakao Entertainment.</p>
<h3 id="references">References</h3>
<ul>
<li><a href="https://developer.apple.com/documentation/vision/recognizing_objects_in_live_capture">Recognizing Objects in Live Capture</a></li>
<li><a href="https://machinethink.net/blog/bounding-boxes/">How to display Vision bounding boxes</a></li>
<li><a href="https://guides.codepath.com/ios/Creating-a-Custom-Camera-View">Creating a Custom Camera View</a></li>
<li><a href="https://github.com/hollance/neural-engine">The Neural Engineâ€Šâ€”â€Šwhat do we know about it?</a></li>
<li><a href="https://github.com/WongKinYiu/yolov7">WongKinYiu/yolov7</a></li>
<li><a href="https://developer.apple.com/videos/play/wwdc2022/10027/">WWDC2022â€Šâ€”â€ŠOptimize your Core ML usage</a></li>
<li><a href="https://machinethink.net/blog/mobilenet-ssdlite-coreml/">MobileNetV2 + SSDLite with Core ML</a></li>
<li><a href="https://github.com/dbsystel/yolov5-coreml-tools">yolov5â€Šâ€”â€ŠCoreML Tools</a></li>
</ul>
]]></content:encoded>
    </item>
    
    <item>
      <title>YOLOv7 TensorRT çš„æ€§èƒ½æ¸¬è©¦</title>
      <link>https://blog.cmwang.net/zh/posts/2022/07/performance-benchmarking-of-yolov7-tensorrt/</link>
      <pubDate>Mon, 25 Jul 2022 10:30:00 +0000</pubDate>
      
      <guid>https://blog.cmwang.net/zh/posts/2022/07/performance-benchmarking-of-yolov7-tensorrt/</guid>
      <description>é€™ç¯‡æ–‡ç« åªæœ‰è‹±æ–‡ç‰ˆçš„</description>
      <content:encoded><![CDATA[<p><img loading="lazy" src="images/1.gif#layoutTextWidth" alt="image"  />

YOLOv7 TensorRT Performance Benchmarking.</p>
<p>Object detection is one of the fundamental problems of computer vision. Instead of region detection and object classification separately in two stage detectors, object classification and bounding-box regression are done directly without using pre-generated region proposals in one stage detectors. YOLO (You Only Look Once) is one of the representative models of one-stage architecture. The YOLO family has continued to evolve since 2016, this summer weâ€™ve got its latest update to version 7.</p>
<p><a href="https://github.com/WongKinYiu/yolov7">GitHub - WongKinYiu/yolov7: Implementation of paper - YOLOv7: Trainable bag-of-freebies sets newâ€¦</a></p>
<p>If you are trying to learn how to train your model on a custom dataset from the beginning, there are already many tutorials, notebooks and videos available online. In <a href="https://nilvana.ai/">Nilvana</a>, we really care about its real-world performance on the embedded devices, especially Nvidia Jetson family devices. So we conducted a series performance testing of YOLOv7 variants models on different devices, from cloud GPUs A100 to the latest tiny powerhouse <a href="https://developer.nvidia.com/embedded/jetson-orin">AGX Orin</a>.</p>
<p><a href="https://www.seeedstudio.com/NVIDIA-Jetson-AGX-Orin-Developer-Kit-p-5314.html">NVIDIAÂ® Jetson AGX Orinâ„¢ Developer Kit: smallest and most powerful AI edge computer</a></p>
<blockquote>
<p>The main reason YOLOv7 is more accurate, compare to other models with similar AP, YOLOv7 has only about half computational cost.â€Šâ€”â€Š<a href="https://github.com/WongKinYiu/yolov7/issues/207#issuecomment-1186934294">WongKinYiu</a> &gt; <img loading="lazy" src="images/2.png#layoutTextWidth" alt="image"  />

Input and Output shape of YOLOv7 (80 class)</p>
</blockquote>
<p>According to the results table, Xavier NX can run YOLOv7-tiny model pretty well. AGX Orin can even run YOLOv7x model more than 30 FPS, itâ€™s amazing!</p>
<p><img loading="lazy" src="images/3.png" alt="image"  />

<em>End-to-End Performance on 1080P video, Batch Size=1</em></p>
<p>
<div style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;">
  <iframe src="https://www.youtube-nocookie.com/embed/OypXod2zaoQ" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;" allowfullscreen title="YouTube Video"></iframe>
</div>

<em>Performance Benchmarking Playlist</em></p>
]]></content:encoded>
    </item>
    
    <item>
      <title>æ¨¡æ“¬å¥§é‹äº”ç’°ç„¡äººæ©Ÿç‡ˆå…‰ç§€</title>
      <link>https://blog.cmwang.net/zh/posts/2022/03/simulated-olympic-rings-drone-light-show/</link>
      <pubDate>Sat, 19 Mar 2022 12:37:27 +0000</pubDate>
      
      <guid>https://blog.cmwang.net/zh/posts/2022/03/simulated-olympic-rings-drone-light-show/</guid>
      <description>ç·´ç¿’ CALayer &amp;amp; Gesture Recognizer@</description>
      <content:encoded><![CDATA[<p><strong>å­¸ç¿’ç›®æ¨™ï¼š</strong></p>
<ul>
<li>Gesture Recognizer</li>
<li>CALayer &amp; UIBezierPath</li>
<li>Class &amp; Function</li>
</ul>
<p><strong>ç·´ç¿’æˆæœï¼š</strong></p>
<p><img loading="lazy" src="images/1.gif" alt="image"  />
</p>
<p><strong>åŸå§‹ç¢¼ï¼š</strong></p>
<p><a href="https://github.com/takawang/olympic-drone">GitHub - takawang/olympic-drone</a></p>
<p><strong>ç·£èµ·ï¼š</strong></p>
<p>é€™æ¬¡çš„ç·´ç¿’ï¼Œä¸»è¦æ˜¯çœ‹åˆ°ä¸‹é¢é€™ç¯‡æ–‡ç« çš„å•Ÿç™¼ï¼Œä¸éæˆ‘ç°¡åŒ–äº†è§¸æ¨¡æ•£é–‹èˆ‡åŠ é€Ÿåº¦é€™å€‹éƒ¨åˆ†ï¼Œä¸»è¦æ˜¯ç·´ç¿’ swift å¯¦ä½œã€‚</p>
<p><a href="https://velog.io/@heekang/Vanilla-JS-%ED%8F%89%EC%B0%BD%EC%98%AC%EB%A6%BC%ED%94%BD-%EB%93%9C%EB%A1%A0%EC%87%BC-%EB%A7%8C%EB%93%A4%EA%B8%B0">[Vanilla JS] í‰ì°½ì˜¬ë¦¼í”½ ë“œë¡ ì‡¼ ë§Œë“¤ê¸°</a></p>
<p><strong>èªªæ˜ï¼š</strong></p>
<p>å¯¦ä½œçš„æ§‹æƒ³é‚„ç®—ç›´è¦ºï¼Œé™¤äº†<strong>é•·å£“é€£çºŒè§¸ç™¼</strong>éœ€è¦ç ”ç©¶ä¸€ä¸‹ã€‚</p>
<ol>
<li>ä½¿ç”¨ javascript çš„ <strong>canvas getImageData()</strong> å°‡ SVG å­˜æˆ json æª” (å¯åƒè€ƒä¸Šæ–‡é€£çµä¸­çš„ <strong>getDotPos</strong> æ–¹æ³•ã€‚</li>
<li>åœ¨ swift ä¸­ï¼Œé€éè®€å– json æª”ï¼Œå­˜æˆåº§æ¨™é»çš„é¡åˆ¥é™£åˆ—ï¼Œé€™å…©æ­¥é©Ÿä¹Ÿå¯ä»¥ç”¨ä¾†ç•«å…¶ä»–åœ–æ¡ˆï¼Œä¸ç”¨æ‰‹å·¥ä¸€ç›´æé»ã€‚</li>
<li>è¨­è¨ˆ Drone é¡åˆ¥ï¼Œè®“ Drone <strong>åˆå§‹ä½ç½®</strong>æ˜¯éš¨æ©Ÿçš„ï¼Œ<strong>ç›®çš„ä½ç½®</strong>æ˜¯å‰›å‰›è¼‰å…¥çš„åœ–å½¢åº§æ¨™ã€‚</li>
<li>å°‡å…©åº§æ¨™åˆ†æˆ 20 ä»½ï¼ŒæŒçºŒæŒ‰å£“é€£çºŒè§¸ç™¼æ™‚ï¼Œä¸€æ¬¡ç§»å‹•ä¸€ä»½ã€‚</li>
<li>ç•¶åˆ°é”ç›®çš„åº§æ¨™æ™‚ï¼Œæ”¹è®Š Drone çš„é¡è‰²ã€‚</li>
</ol>
]]></content:encoded>
    </item>
    
    <item>
      <title>iPadä¸Šçš„å½±ç‰‡æ’­æ”¾å™¨</title>
      <link>https://blog.cmwang.net/zh/posts/2022/03/practice-avplayer-view-controller/</link>
      <pubDate>Wed, 16 Mar 2022 10:32:00 +0000</pubDate>
      
      <guid>https://blog.cmwang.net/zh/posts/2022/03/practice-avplayer-view-controller/</guid>
      <description>ç·´ç¿’ä½¿ç”¨ AVPlayerViewController è£½ä½œå‹•æ…‹å­—å¹•å½±ç‰‡</description>
      <content:encoded><![CDATA[<p><strong>å­¸ç¿’ç›®æ¨™ï¼š</strong></p>
<ul>
<li>AVPlayer</li>
<li>NotificationCenter</li>
<li>Timer</li>
<li>Swift Optional</li>
<li>Function</li>
<li>Switch Case</li>
</ul>
<p><strong>ç·´ç¿’æˆæœï¼š</strong></p>
<p><strong>åŸå§‹ç¢¼ï¼š</strong></p>
<p><a href="https://github.com/takawang/letter-song">GitHub - takawang/letter-song</a></p>
<p><strong>æ­Œè©ï¼š</strong>
<code>ä»Šæ™š æˆ‘æƒ³å°‡é‚£å¤©çš„è¢ç« é€åˆ°ä½ çš„çª—å‰ å«æ„æ˜¯&amp;#34;æˆ‘æ„›ä½ &amp;#34; æˆ‘æƒ³èµ·æˆ‘å€‘çš„åˆå» ä¸ç®¡ä½•æ™‚ åªè¦é–‰ä¸Šé›™çœ¼ å°±èƒ½å¥”å‘é‚£æœ€é™é çš„åœ°æ–¹ å°±åƒæˆ‘åœ¨è¢«æµªæ¿¤æ¹§ä¾†çš„æ²™ä¸Šå¯«ä¸‹çš„å­—è·¡èˆ¬ ä½ æ„Ÿè¦ºä¹Ÿæœƒåƒä»–å€‘é‚£æ¨£å¾æ­¤æ¶ˆå¤± ç¸½æ˜¯æƒ³å¿µ é›–ç„¶ç„¡æ³•å°‡æˆ‘å¿ƒè£¡æ‰€æœ‰çš„è©±èªï¼Œèªªçµ¦ä½ è½ ä½†é‚£è£¡é ­ï¼Œå…¨éƒ½æ˜¯&amp;#34;æˆ‘æ„›ä½ &amp;#34; æˆ‘ä½•å¾·ä½•èƒ½ èƒ½æ“æœ‰åç‚ºä½ çš„é€™ä»½å¹¸é‹ è‹¥æˆ‘å€‘ç¾åœ¨èƒ½æœ›è‘—å°æ–¹ è©²æœ‰å¤šç¾å¥½ å°±åƒæˆ‘åœ¨è¢«æµªæ¿¤æ¹§ä¾†çš„æ²™ä¸Šå¯«ä¸‹çš„å­—è·¡èˆ¬ ä½ æ„Ÿè¦ºä¹Ÿæœƒåƒä»–å€‘é‚£æ¨£å¾æ­¤æ¶ˆå¤± é‚„æ˜¯æƒ³å¿µ é›–ç„¶ç„¡æ³•å°‡æˆ‘å¯«åœ¨æ—¥è¨˜ä¸Šçš„ä¸€å­—ä¸€å¥ éƒ½å‘Šè¨´ä½  ä½†å­—å­—å¥å¥éƒ½ä»£è¡¨è‘—æˆ‘æ„›ä½  ä»Šæ™š æˆ‘æƒ³å°‡é‚£å¤©çš„è¢ç« é€åˆ°ä½ çš„çª—å‰ å¸Œæœ›ä»Šå¤œçš„ä½ æœ‰å€‹ç¾å¥½çš„å¤¢</code></p>
<p><strong>å½±ç‰‡ä¾†æºï¼š</strong><a href="https://www.youtube.com/watch?v=BzYnNdJhZQw">https://www.youtube.com/watch?v=BzYnNdJhZQw</a></p>
<p><strong>æ­Œè©ä¾†æºï¼š</strong><a href="https://tinyurl.com/2p9c5d8a">https://tinyurl.com/2p9c5d8a</a></p>
<p><strong>Storyboard:</strong></p>
<p><img loading="lazy" src="images/1.png" alt="image"  />

<strong>éç¨‹ï¼š</strong></p>
<p>æœ¬æ¬¡çµåˆå¹¾å€‹é …ç›®ï¼Œç·´ç¿’ SDK ä½¿ç”¨èˆ‡ç†Ÿæ‚‰ Swift èªæ³•ï¼Œåªç”¨ function print ç„¡æ³•æ»¿è¶³è‡ªå·±çš„æœŸå¾…ï¼Œæ‰€ä»¥ä¸Šç¶²æŸ¥äº†ä¸€äº›è³‡æ–™ä¸¦æ­é…ä¸Šèª²å…§å®¹è£½ä½œï¼Œå°‡ AVPlayerViewController åŠ é€² UIView æ‡‰è©²æ˜¯æœ€å›°é›£çš„éƒ¨åˆ†ï¼ŒåŸæœ¬è€ƒæ…®ä½¿ç”¨ Container View å»åšï¼Œä½†é‚„ä¸çŸ¥é“æ€éº¼æ§åˆ¶å¤šå€‹ Controllerï¼Œä¸¦ä¸”è·Ÿ Storyboard ä¸²æ¥èµ·ä¾†ã€‚</p>
<p>ä¸»è¦åœ¨ <strong>iPad Pro 12.9</strong> èˆ‡ <strong>iPad Mini 6</strong> å¯¦æ©Ÿæ¸¬è©¦éï¼ŒiPhone ç‰ˆé¢æœƒçœ‹ä¸åˆ°æ­Œè©ã€‚</p>
<ol>
<li>æ–°å¢ UIViewï¼Œå–å¾— IBOutlet -&gt; Line 12</li>
<li>å°‡ AVPlayerViewController åŠ é€² UIView -&gt; Line 50</li>
<li>ç”¨ IBAction æ§åˆ¶ å½±ç‰‡æ’­æ”¾ä¸¦é–‹å§‹è¨ˆæ™‚ -&gt; Line59</li>
<li>è¨»å†Š View ç”Ÿå‘½é€±æœŸèˆ‡å½±ç‰‡æ’­æ”¾çµæŸçš„é€šçŸ¥ -&gt;Line 20~28</li>
</ol>
<p><strong>å…è²¬è²æ˜ï¼š</strong></p>
<p>å½±ç‰‡ä½¿ç”¨åªç‚ºäº†å­¸ç¿’ç›®çš„ï¼Œæ²’æœ‰ç‡Ÿåˆ©è¡Œç‚ºï¼Œç‰ˆæ¬Šå±¬æ–¼åŸå‰µä½œæ–¹ <a href="https://www.youtube.com/channel/UCweOkPb1wVVH0Q0Tlj4a5Pw">1theK</a>.</p>
]]></content:encoded>
    </item>
    
    <item>
      <title>ä»¿è£½ iPad ä¸Šçš„ Apple Music</title>
      <link>https://blog.cmwang.net/zh/posts/2022/03/replicate-apple-music-ipad-os-app/</link>
      <pubDate>Sun, 13 Mar 2022 11:35:11 +0000</pubDate>
      
      <guid>https://blog.cmwang.net/zh/posts/2022/03/replicate-apple-music-ipad-os-app/</guid>
      <description>ç·´ç¿’ä½¿ç”¨ Scroll View &amp;amp; Table View ä»¿è£½ iPad ä¸Šçš„ Apple Music App</description>
      <content:encoded><![CDATA[<p><strong>å­¸ç¿’ç›®æ¨™ï¼š</strong></p>
<ul>
<li>ç·´ç¿’ Scroll View</li>
<li>ç·´ç¿’ Navigation Controller</li>
<li>ç·´ç¿’ Table View Controller &amp; Static Cells</li>
<li>ç·´ç¿’ Label &amp; Button æ¨£å¼</li>
<li>ç·´ç¿’ segue &amp; SF Symbols</li>
</ul>
<p><strong>ç·´ç¿’æˆæœï¼š</strong></p>
<p><img loading="lazy" src="images/1.gif" alt="ç·´ç¿’æˆæœéŒ„å½±æˆªåœ–"  />
</p>
<p><a href="https://github.com/takawang/fake-music">GitHub - takawang/fake-music</a></p>
<p><strong>éç¨‹ï¼š</strong></p>
<p>é€éç›´æ¥æ“·å– iPad Pro 12.9 å¯¦æ©Ÿç•«é¢ï¼Œåœ¨ story board ä¸Šä»¿è£½ Apple Music ä¸Šçš„ IU å°ˆè¼¯é é¢ï¼Œæ›´é€²ä¸€æ­¥ç†Ÿæ‚‰äº† Navigation Controller çš„ä½¿ç”¨ï¼Œå› ç‚ºé‚„ä¸æœƒè£½ä½œç¬¦åˆå„ç¨®æ©Ÿå‹çš„ç‰ˆé¢ï¼Œæ‰€ä»¥å°±ç›´æ¥ hardcode åªæ”¯æ´ 12.9 æ°´å¹³çš„ç‰ˆé¢ï¼Œæ˜¯æœ‰é»å‘†ç‰ˆä½†ä¹Ÿæ”¶ç©«ä¸å°‘ã€‚</p>
<p><img loading="lazy" src="images/2.png" alt="image"  />

é™å®šåªæ”¯æ´ iPad èˆ‡ Landscape</p>
<p><img loading="lazy" src="images/3.png" alt="image"  />

ä¿®æ”¹ Product Name æ”¹è®Š APP åœ¨æ¡Œé¢çš„åç¨±</p>
<p>éœ€è¦æ³¨æ„çš„åœ°æ–¹æ˜¯ï¼Œåœ¨ story board ä¸Šçš„æ˜¯ pointï¼Œä½†è¢å¹•æˆªåœ–æ˜¯ pixelï¼Œå› ç‚ºæ²’æœ‰ zeplin ä¹‹é¡çš„è¨­è¨ˆç¨¿ï¼Œæ‰€ä»¥èŠ±æœ€å¤šæ™‚é–“é‚„æ˜¯åœ¨æŠ“å–èˆ‡å°é½Šåº§æ¨™ï¼Œä½ç½®ä¸æ˜¯ pixel perfectï¼Œä½†ç·´ç¿’çš„ç›®çš„æ‡‰è©²æ˜¯é”åˆ°äº†ã€‚</p>
<p><img loading="lazy" src="images/4.png" alt="image"  />

Point èˆ‡ pixel æ˜¯ 2x scale çš„é—œä¿‚</p>
<p><strong>æœ€è¿‘æ’­æ”¾</strong>çš„é»é¸ï¼Œåƒè€ƒä¸‹é¢æ–‡ç« ï¼ŒåŸæœ¬ä½¿ç”¨ <strong>tap gesture recognizer</strong>ï¼Œä¸éå¤§æ¦‚å› ç‚ºæˆ‘å°‡ navigation controller æ”¾éŒ¯åœ°æ–¹ï¼Œç¸½æ˜¯çœ‹ä¸åˆ° back é¸å–®ï¼Œæœ€å¾Œé¸æ“‡<strong>é€æ˜æŒ‰éˆ•ç–Šåœ¨ä¸Šæ–¹</strong>çš„åšæ³•ã€‚</p>
<p><a href="https://medium.com/%E5%BD%BC%E5%BE%97%E6%BD%98%E7%9A%84-swift-ios-app-%E9%96%8B%E7%99%BC%E5%95%8F%E9%A1%8C%E8%A7%A3%E7%AD%94%E9%9B%86/%E5%88%A9%E7%94%A8-segue-%E5%AF%A6%E7%8F%BE%E5%9C%96%E7%89%87%E9%BB%9E%E9%81%B8%E7%9A%84%E6%8F%9B%E9%A0%81%E5%8A%9F%E8%83%BD-84f6991d74d4">åˆ©ç”¨ segue å¯¦ç¾åœ–ç‰‡é»é¸çš„æ›é åŠŸèƒ½</a></p>
<p><img loading="lazy" src="images/5.png" alt="image"  />

Navigation Controller éœ€è¦å¾ç¬¬ä¸€å€‹ç•«é¢é•·å‡ºä¾†ï¼Œè€Œä¸æ˜¯ç¬¬äºŒé </p>
<p>ä¸»ç•«é¢æœ‰å…©å€‹ scoll viewï¼Œåƒè€ƒé€£çµæä¾›çš„æŠ€å·§å°±å¯ä»¥æ–¹ä¾¿è£½ä½œï¼Œè¨˜å¾—å°‡<strong>æ°´å¹³èˆ‡å‚ç›´çš„æ²è»¸æŒ‡ç¤º</strong>é—œé–‰ï¼Œé€™æ¨£ç•«é¢æœƒæ¯”è¼ƒæ“¬çœŸã€‚</p>
<p><a href="https://medium.com/%E5%BD%BC%E5%BE%97%E6%BD%98%E7%9A%84-swift-ios-app-%E9%96%8B%E7%99%BC%E5%95%8F%E9%A1%8C%E8%A7%A3%E7%AD%94%E9%9B%86/%E5%9C%A8-storyboard-%E8%A8%AD%E5%AE%9A-content-size-%E5%AF%A6%E7%8F%BE%E6%B0%B4%E5%B9%B3%E6%8D%B2%E5%8B%95%E7%9A%84-scroll-view-2710fa247293">è¨­å®š content sizeï¼Œå¯¦ç¾æ°´å¹³æ²å‹•ï¼Œä¸Šä¸‹æ²å‹•å’Œåˆ†é çš„ scroll view</a></p>
<p><img loading="lazy" src="images/6.png" alt="image"  />

å–æ¶ˆå‹¾é¸æ°´å¹³èˆ‡å‚ç›´ indicatorï¼Œä½¿å¾— scroll view æ²å‹•æ™‚ä¸æœƒé¡¯ç¤ºå·è»¸</p>
<p>Table View çš„ç¬¬ä¸€å€‹ Cell èˆ‡æœ€å¾Œå…©å€‹ Cell ç‚ºäº†ç‰ˆé¢èª¿æ•´ï¼Œä¸­é–“çš„æ­Œæ›²åˆ—è¡¨åªéœ€è¦é€éä¿®æ”¹ row æ•¸å°±å¯ä»¥è¤‡è£½ï¼Œå†ä¿®æ”¹å°é¢ç…§èˆ‡ label å³å¯ã€‚</p>
]]></content:encoded>
    </item>
    
  </channel>
</rss>
